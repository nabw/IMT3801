\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amsthm, amssymb, mathpazo, isomath, mathtools}
\usepackage{subcaption,graphicx}
\usepackage{fullpage}
\usepackage{booktabs}
\usepackage{hyperref}
%%%% Remove some annoying hyperref warnings. %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% See https://tex.stackexchange.com/questions/10555/hyperref-warning-token-not-allowed-in-a-pdf-string
\makeatletter
\pdfstringdefDisableCommands{\let\HyPsd@CatcodeWarning\@gobble}
\makeatother
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{mathtools}
\usepackage{todonotes}
\usepackage{cancel}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\newcommand{\RightComment}[1]{\hfill \(\triangleright\) \textit{#1}}
\newcommand{\example}[1]{\todo[inline,color=orange!30!white]{\textbf{Example:} #1}}

\title{Applications of Functional Analysis and PDEs}
\author{Nicol\'as A Barnafi}
%\date{}

\renewcommand{\vec}{\vectorsym}
\newcommand{\mat}{\matrixsym}
\newcommand{\ten}{\tensorsym}
\DeclareMathOperator{\grad}{\nabla}
\DeclareMathOperator{\dive}{\text{div}}
\DeclareMathOperator{\curl}{\text{curl}}
\newcommand{\tr}{\text{tr}\;}
\newtheorem{remark}{Remark}
\newtheorem{definition}{Definition}
\newtheorem{note}{Note}
\newcommand{\R}{\mathbb{R}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\T}{\mathcal{T}}
\renewcommand{\P}{\mathcal{P}}

\newcommand{\tin}{\text{in }}
\newcommand{\ton}{\text{on }}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

\usepackage{listings}
\usepackage{xcolor}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{mystyle}{
  backgroundcolor=\color{backcolour}, commentstyle=\color{codegreen},
  keywordstyle=\color{magenta},
  numberstyle=\tiny\color{codegray},
  stringstyle=\color{codepurple},
  basicstyle=\ttfamily\footnotesize,
  breakatwhitespace=false,         
%   breaklines=false,                     
  captionpos=b,                    
  keepspaces=true,                 
  numbers=none,                    
  numbersep=5pt,                  
  showspaces=false,                
  showstringspaces=false,
  showtabs=false,                  
  tabsize=2,
%   frameround=tttn,
  framerule=1.5pt,
  rulecolor=\color{red!60!black}
}
\lstset{style=mystyle}

\begin{document}

\maketitle
\newpage
\tableofcontents
\newpage

\section*{Context}

These notes exist as backup material for a course on some deeper topics in Math Eng at Pontificia Universidad Cat√≥lica de Chile, the 2nd semester of 2024. The idea is to provide mathematical tools for students that give them the ability to assess the difficulty of mathematical problems, mainly within the world of Partial Differential Equations (PDEs). The target is ultimately to implement these models, so that all tools are oriented towards having solid foundations that allows one to trust a computational model. Informally speaking, the main mathematical concepts to haunt us during all these notes are: 
    \begin{itemize}
        \item Existence and uniqueness: It is a natural baseline in the mathematician's world to try to solve only problems that \emph{have} a solution. Otherwise, things might be as pointless as developing an iterative method for finding real numbers such that $x^2 = -1$. Uniqueness is a further luxury, but sometimes two different methods give two different solutions, and having only those things at hand can make it difficult to distinguish whether that is a bug or a feature of the model. There exist some root-isolation methods that allow to find solutions of a problem that are \emph{different} from a given one. This is out of the scope of this course. 
        \item Stability: The intuitive idea behind this is that small perturbations in the data give rise to small changes in the solution. This typically looks like 
            $$ \| u\|_X \leq \| f\|_{X'}, $$
        where $u$ is the solution of a problem that depends on $f$, and $X$ is some functional (hopefully Hilbert) space. More rigorously, this means that the solution map $f \mapsto u(f)$ is bounded, or continuous in the linear case. Stability also sometimes refers to time dynamics and the fact that a discrete solution stays \emph{within a certain distance} of the real solution throughout a simulation. In the continuous setting, it might also mean that there are no finite-time singularities. In general, stability is not a well defined term, but still a widely understood one to anyone who has struggled to get a code to run correctly, and a highly desired property. 
    \end{itemize}
All other properties (or at least most of them anyway) are ways to guarantee that a problem enjoys one of these nice properties. There are ways to handle problems that do not have those properties, but they are almost always extremely problem dependent, and the person studying such problems should dive deep into the sectorial knowledge to see how certain communities deal with such issues. This is an aspect that mathematically oriented people almost always disregard, which has some severe mathematical (and social) consequences. In fact, some extremely classical models in engineering are still far from understood mathematically, such as the Navier-Stokes equations. This has not prevented the CFD community from solving these models with extreme efficiency, and from further leveraging them for industrial applications which, unsurprisingly, work fantastically. Discovering the amazing ways in which mathematically obvlivious communities solve mathematically hard problems is, and will probably be for very long, a beautiful opportunity for collaboration.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Finite differences}
This section has been heavily extracted from some notes from the WIAS in Berlin. They didn't have any authorship, so I don't really know who to thank for them.  Probably the simplest (and oldest) idea for approximating equations stems from the use of Taylor's theorem:

\begin{theorem}\label{thm:taylor}
    Consider a function $f$ in $C^n((a,b), \R)$, i.e. $n$ times differentiable whose derivatives are well-defined in $a$. Then, there is some $\xi$ in $(a,b)$ such that
    $$ f(b) = \sum_{k=0}^{n-1}\frac{f^{(k)}(a)}{k!}(b-a)^k + \frac{f^{(n)}(\xi)}{n!}(b-a)^n. $$
\end{theorem}
Naturally, this is the mean value theorem when $n=1$. From now on we consider our setting to be in $\R$. For any continuous function $f:(a,b)\to \R$, we can define its associated \emph{grid function} given by its evaluation in a finite set of points. We sometimes refer to this as \emph{collocation}. Set $a<b$, and define the points $a=x_1<\hdots<x_N=b$ with $h_i=x_{i}-x_{i-1}$, such that we have $N$ points, and define the grid function as the vector
    $$ R_hf = (f_1, \hdots, f_N) \coloneqq (f(x_1), \hdots, f(x_N)) \in \R^N,$$
    where we have defined the $R_h$ operator such that $R_hf:\R^N \to \R^N$ and $R_h:C((a,b),\R)\to [\R^N\to\R^N]$.  We can then define the following difference operators: 
    \begin{itemize}
        \item Forward difference: 
            $$ D^+f(x_i) \coloneqq \frac{f_{i+1} - f_i}{h_{i+1}} $$
        \item Backward difference: 
            $$ D^-f(x_i) \coloneqq \frac{f_{i} - f_{i-1}}{h_{i}} $$
        \item Centered difference:
            $$ Df(x_i) \coloneqq \frac{f_{i+1} - f_{i-1}}{h_{i+1} - h_{i-1}} $$

    \end{itemize}
A nice simple exercise is to extend these ideas to second order derivatives, which all stem from Taylor's formula. The application we will be interested is in that of approximating the action of a differential operator using these formulas. For this, we will focus on the forward difference formula. At each grid point we will have
$$ D^+f(x_i) = \frac{f_{i+1} - f_i}{h_{i+1}} = \frac 1{h_{i+1}} [-1\quad 1]\begin{bmatrix}f_i \\ f_{i+1}\end{bmatrix}, $$
so that defining the matrix $\mat D_h^+$ in $\R^{N\times N}$ as
$$ [\mat D_h^+]_{ij} = \begin{cases} -1/h_{i+1} & j=i \\ 1/h_i  & j=i+1 \\ 0 & \text{elsewhere} \end{cases} $$
we obtain an operator which, given a grid function, it yields its discrete derivative. Naturally, care has to be taken at the last point, where we can simply use instead a backwards difference. 

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
        \begin{loglogaxis}[ 
            xlabel={$h$}, 
            ylabel={$\|y' - Dy\|_{\infty}$}, 
            legend style={at={(1.05,0.5)}, anchor=west, column sep=1ex, legend columns=1, align=left}, 
            grid=major,
            width=10cm, height=6cm, % Adjust the size if needed
            ]
            \addplot[
                blue, 
                line width=1pt
            ] table [x=h, y=fd, col sep=comma] {convergence_rates_FD.csv};
            
            \addplot[
                orange, 
                dashed, 
                line width=1pt
            ] table [x=h, y=bd, col sep=comma] {convergence_rates_FD.csv};
            
            \addplot[
                green, 
                line width=1pt
            ] table [x=h, y=cd, col sep=comma] {convergence_rates_FD.csv};

            % Legend
            \legend{forward, backward, centered}
        \end{loglogaxis}
    \end{tikzpicture}
    \caption{Convergence of forward, centered and backward difference schemes for calculating the first derivative of $u(x)=\sin(2\pi x)$. Note that the centered difference error starts increasing for $h\lesssim 10^{-6}$.}
    \label{fig:fd_convergence}
\end{figure}

Finite difference approximations can be easily seen to be convergent for sufficiently smooth functions. To see this, consider Taylor's theorem to obtain for $h>0$:
    $$ f(x+ h) = f(x) + f'(x)h + O(h^2), $$
from which we readily obtain that 
    $$ | D^+f(x) - f'(x) | = O(h), $$
    so that forward differences and also backward differences (straightforward calculation) converge pointwise linearly. However, it is noteworthy that although centered differences also start converging linearly, they become unstable for very small $h$, see  Figure \ref{fig:fd_convergence}.

\subsection{Second order derivatives}
The standard way of approximating second order derivatives is that of using a finite difference approximation twice. It is given by the following: 
$$ \begin{aligned}
    f''(x) &\approx D^+(f')(x) \\
           &=\frac{f'(x+h) - f'(x)}{h} \\
           &\approx\frac{\frac{f(x+h) - f(x)}{h} - \frac{f(x) - f(x-h)}{h}}{h} \\
           &= \frac{f(x+h) - 2f(x) + f(x-h)}{h^2}.
    \end{aligned}
$$
We note that the intermediate steps show that these computations can be represented as the matrix product $\mat D^+ \mat D^-$, where the order of differentiation can be inverted and thus equivalently $\mat D^- \mat D^+$. 
\subsection{Order of convergence}
The order of convergence of the previously computed schemes can be derived from Taylor's theorem. For example, we previously showed that backward/forward differences converge linearly (pointwise), and indeed we can do the same for centered differences. For this, consider the following third order expansions:
    $$
    \begin{aligned}
        f(x+h) &= f(x) + hf'(x) + \frac {h^2} 2 f''(x) + \mathcal O(h^3) \\
        f(x-h) &= f(x) - hf'(x) + \frac {h^2} 2 f''(x) + \mathcal O(h^3),
    \end{aligned}
    $$
and by simply adding both equations it can be shown that the finite difference formula computed converges linearly.
\subsection{The empirical convergence rate}
One issue that is always overviewed is the computation of the \emph{observed} convergence rate, so we will write it here once and for all so that it is never an issue again. Assume that some error $\vec e_h$ converges to zero in some $X$ norm with rate $k$:
    $$ \|\vec e_h\|_X \leq C h^k.$$
    Then, given a sequence of mesh sizes $h$ and errors as $(h_j, [\vec e_h]_j)$, we would like to estimate $k$ to validate we really obtained the computed rate $k$. For this, we take the logarithm of the error expression:
    $$ \log \|[\vec e_h]_j\|_X \leq \log C + k \log h_j, $$
and note that we obtain roughly a line equation if we assume an equality. Because of this, we can estimate now estimate the slope of our line equation using two consecutive points as
$$ k \approx \frac{\log \|[\vec e_h]_{j+1}\|_X - \log \|[\vec e_h]_j\|_X}{ \log h_{j+1} - \log h_j}, $$
which yields the classical expression one gets everywhere: 
    $$ k \approx  \frac{\log \left(\|[\vec e_h]_{j+1}\|_X / \|[\vec e_h]_j\|_X\right)}{ \log \left(h_{j+1} / h_j\right)}. $$

    This is also referred to sometimes as \emph{numerical convergence rate}. 

\subsection{Method of manufactured solutions}
A typical problem is that one wants to estimate the convergence of some discrete solution
    $$ \mat L_h \vec u_h = 0, $$
which approximates a differential equation
    $$ \mathcal L u = 0 $$
    in which one typically does not know the analytical solution. This method allows to compute a modified problem in which we know the analytic solution. Indeed, consider an arbitrary analytic function (that we know) $u_{ex}$. The main idea here is that we can define a right hand side 
    $$ f_{ex} \coloneqq \mathcal L u_{ex}, $$
such that the solution of the modified problem 
    $$ \mathcal L u = f \quad ( = \mathcal L u_{ex}) $$
    is, if uniqueness holds, $u = u_{ex}$. To obtain the discrete problem, we consider the associated grid function $R_h u_{ex}$ and set the discrete right hand side $\vec f_h = R_h \mathcal L u_{ex}$ such that the solution of the discrete problem 
    $$ \mat L_h \vec u_h = \vec f_h \quad (=R_h \mathcal L u_{ex}) $$
converges to the continuous solution $u_{ex}$ by construction. 

\subsection{Convergence Analysis}
Using these ideas, we can now proceed to propose a mechanisms for discretizing differential operators. A central question regards the capability of approximation that such an objects has, which finds its answer in the Lax equivalence theorem. We will first require other definitions. 

\begin{definition}[Consistency]
    Consider some differential operator $\mathcal A$. A discrete operator $\mat A_h$ in $\R^{N\times N}$ is said to be \emph{consistent} with $\mathcal A$ of order $k$ if for all sufficiently smooth functions it holds that
    $$ \max_{i\in\{1,\hdots,N\}}|\mathcal Af(x_i) - [\mat A_hR_hf]_i| =: \|R_h \mathcal Af - \mat A_hR_h f\|_{\infty,h} \leq C h^k, $$
    where $\|\cdot \|_{\infty,h}$ is the induced infinity norm in $\R^N$ and $C$ is a positive constant independent of $h$. This is sometimes stated more weakly by saying that the norm is $\mathcal O(h^k)$. 
\end{definition}

\example{The difference operators are all consistent with the first order derivative, which can be seen immediately from Taylor's theorem.}
\begin{definition}[Stability]
    A finite difference operator $\mat A_h$ is said to be stable (in the discrete maximum norm) if there is a stability constant $C>0$ that does not depend on the grid size such that
        $$ \| \vec v_h \|_{\infty,h} \leq C \|\mat A_h \vec v_h \|_{\infty,h}$$
        for all grid functions $\vec v_h$. 
\end{definition}
We finally consider an infinite dimensional problem given by
    $$ \mathcal A u = f $$
and its finite difference approximation as
    $$ \mat A_h \vec u_h = \vec f_h, $$
    which will allow us to define convergence. 

\begin{definition}[Convergence]
    The finite differences scheme is said to be convergent of order $k$ in the discrete maximum norm if the discrete solution and the continuous one satisfy that 
    $$ \| \vec u_h - R_hu \|_{\infty,h} \leq C h^k. $$
This is sometimes stated more weakly by saying simply that $\| \vec u_h - R_hu \|_{\infty,h}$ is $\mathcal O(h^k)$. 
\end{definition}

\begin{theorem}[Lax equivalence theorem]
    Let $ \mat A_h \vec u_h = \vec f_h$ be the finite difference discretization for the problem $\mathcal A u = f$ as defined above. If $A_h$ is consistent, then stability and convergence are equivalent. 
    \begin{proof}
        The stability to convergence part is taken directly from the aforementionted notes. The convergence to stability is instead partly inspired by the notes by Long Chen on abstract convergence analysis \cite{chenLFDM}.
        
        \paragraph{Stability $\implies$ convergence:}

        We proceed directly through inequalities:
        \begin{align*}
            \| \vec u_h - R_h u\|_{\infty,h} &\leq C\|\mat A_h(\vec u_h - R_hu) \|_{\infty,h} && \text{(Stability)} \\
                                             &=C \| f_h - \mat A_hR_h u \|_{\infty,h} && \\
                                             &=C \| R_h f - \mat A_hR_h u\|_{\infty,h} &&\\
                                             &=C \|R_h(\mathcal A u) - \mat A_hR_h u\|_{\infty,h}&& \\
                                             &\leq Ch^k && \text{(Consistency)}.
        \end{align*}

        \paragraph{Convergence $\implies$ stability:}

        We note that by taking inverses, we can write stability as 
        $$ \| \mat A_h^{-1}\vec f_h \|_{\infty, h} \leq C \|\vec f_h\|_{\infty, h}, $$
        which is simply the continuity of the inverse map $\mat A_h^{-1}$. Consider an arbitrary grid vector $\vec f_h$, and we denote the associated problem solution as $\vec u_h$, where $\mat A_h \vec u_h = \vec f_h$. Using this, we have that
        $$ 
        \begin{aligned} 
            \|\mat A_h^{-1}\vec f_h\|_{\infty,h} &= \| \vec u_h \|_{\infty, h} \leq \| \vec u_h - R_h u \|_{\infty, h} + \| R_h u \|_{\infty, h} && \text{(Convergence)}\\
                                                 &\leq C h^k + \| R_h \mathcal A^{-1} f \|_{\infty, h}. 
        \end{aligned}
        $$
        To bound this last term, we note that $R_h$ yields pointwise evaluations, so for any continuous function $\eta$ it will hold that
        $$ \| R_h \eta \|_{\infty,h} \leq \| \eta \|_0, $$
        where $\| \eta \|_0$ is the supremum norm. Finally, we use that $\mathcal A$ is a continuous bijection, and thus from the open mapping theorem it holds that $\|\mathcal A^{-1}\|$ is bounded by some constant $C_2$. Using this fact, the previous estimate becomes 
        $$ \| \mat A_h^{-1} \vec f_h \|_{\infty,h} \leq Ch^k + \| \mathcal A f \|_0 \leq C h^k + C_2 \| \vec f_h \|_0<\infty, $$
        which in particular shows that $ \| \mat A_h^{-1} \vec f_h \|$ is bounded for all $\vec f_h$. The uniform boundedness principle them yields that the operator $\mat A_h^{-1}$ is bounded, which concludes the proof.
    \end{proof}
\end{theorem}

\subsection{Stability of the discrete Laplacian in 1D}
Let us now study the stability of the discretized Laplacian operator in 1D, $-\Delta u = -u''$, over the interval $[a,b]$ with $a<b$ and homogeneous boundary conditions $u_h(x_1) = u_h(x_N) = 0$. Consider $\Omega_h = \{x_i\}_{i=1}^N$ a grid of $[a,b]$ with $N$ equispaced elements, where $x_1=a$ and $x_{N-1}=b$, and mesh size $h=(b-a)/N$. Recall that the numerical second derivative can be approximated by the three-point approximation derived above, which for a given interior point is given by
$$-u_h''(x_i) = -\left(\frac{u_h(x_{i-1}) - 2u_h(x_i) + u_h(x_{i+1})}{h^2}\right) = \frac{1}{h^2}(-u_h(x_{i-1}) + 2u_h(x_i) - u_h(x_{i+1})).$$
We note that in our discretized system we do not need to solve for $u_h(x_1)=u_h(x_N)=0$ because of the boundary condition. This implies that the derivative at $x_2$ and $x_{N-1}$ is
\begin{align*}
    -u_h''(x_2) &= \frac{1}{h^2}(\cancel{-u_h(x_{1})} + 2u_h(x_2) - u_h(x_{3})) = \frac{1}{h^2}(2u_h(x_2) - u_h(x_{3}))\\
    -u_h''(x_{N-1}) &= \frac{1}{h^2}(-u_h(x_{N-2}) + 2u_h(x_{N-1}) - \cancel{u_h(x_{N})}) = \frac{1}{h^2}(-u_h(x_{N-2}) + 2u_h(x_{N-1})),\\
\end{align*}
and thus the finite difference operator associated with $-\Delta$ is
$$A_h = \frac{1}{h^2} \begin{bmatrix} 2 & -1 & 0 & \cdots & 0 \\ -1 & 2 & -1 & \cdots & 0 \\ 0 & -1 & 2 & \cdots & 0 \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & 0 & \cdots & 2\end{bmatrix}.$$
where we have $-\Delta u \approx A_h u_h$. To show stability, we will use that it actually satisfies a discrete maximum principle. 
\begin{theorem}[Discrete maximum principle (DMP)]
    Let $A_h$ be the finite difference operator defined above. Then, for any grid function $u_h$ such that $A_h u_h \geq 0$, it holds that
    $$ \max_{x_i\in \Omega_h} u_h \leq \max_{x_i\in \Gamma_h} u_h, $$
    where $x_i$ is an interior node of $\Omega_h$ and the equality holds if and only if $u_h$ is constant.
    \begin{proof}
        We proceed by contradiction, so we assume that $u_h(x_i) \overset{(*)}{=} \max_{x\in \Omega_h} u_h > \max_{x\in \Gamma_h} u_h$, where this maximum is achieved at the internal node $x_i\in \Omega_h$. Then, from the centered difference scheme we get
        $$2u_h(x_i) = u_h(x_{i-1}) + u_h(x_{i+1}) - h^2 \underbrace{A_h u_h(x_i)}_{\geq 0} \leq u_h(x_{i-1}) + u_h(x_{i+1}) \overset{(*)}{\leq} 2u_h(x_i),$$
        which implies that $u_h(x_i)$ is constant in the discrete neighborhood of $x_i$. Repeating this argument recursively for all interior points, we conclude that $u_h(x_i)$ is constant in $\Omega_h$, which contradicts the assumption. Hence, the discrete maximum principle holds.
    \end{proof}
\end{theorem}
We can apply this principle to show that $A_h$ is stable. 
\begin{theorem}
    Let $\Omega$ be a domain with boundary $\Gamma$, and $A_h$ the finite difference operator associated to the problem
    $$\begin{aligned}
        -\Delta u &= f & \tin \Omega, \\
        u &= g & \ton \Gamma.
    \end{aligned}
    $$
    Then, if $A_h u_h = f_h$ in $\Omega_h$ and $u_h = g_h$ in $\Gamma_h$, it holds that
    $$\|u\|_{\infty,h} \leq C\left(\|f\|_{\infty, h} + \|g\|_{\infty, h}\right),$$
    where we write $\|g\|_{\infty, h} = \sup_{x_i\in\Gamma_h}|g(x_i)|$. 
    \begin{proof}
        We will prove the theorem over $\R$. Using the comparison function $\phi(x) = \frac{c}{2}x^2$ where $c=\|f\|_{\infty, h}$ and consider a constant $M$ such that $\phi(x) \leq cM$ in $\Omega$. Using that $A_h \phi_h = c$ (inspired by $\phi''(x) = c$), we get
        $$A_h(u_h + \phi_h) = A_h u_h + c = f_h + c \geq 0,$$
        and using the discrete maximum principle (DMP) we obtain
        $$\max_{\Omega_h} u_h \leq \max_{\Omega_h} (u_h + \phi_h) \overset{\text{DMP}}{\leq} \max_{\Gamma_h} (u_h + \phi_h) = \|g_h\|_{\infty, h} + cM = \|g_h\|_{\infty, h} + M \|f\|_{\infty, h},$$
        and repeating the argument for $-u_h$ yields the lower bound for $\|u_h\|_{\infty,h}$. This shows that $A_h$ is stable.
    \end{proof}
\end{theorem}

\subsection{Discretization in 2D and 3D}
In this section we hint on how to address numerically higher dimensional problems, and also how to include time. The stability of such schemes will be studied further ahead with the von Neumann stability analysis. In typical 1D examples, one has a domain $(a,b)$ and an associated grid given by some points $\{x_i\}_i^N$ such that each index $i$ corresponds to an order of the domain, and thus naturally a finite difference operator is going to involve something like $i$ and its neighbors. This scenario would be different if, for example, one had a random ordering of the grid points such as
    $$ x_0 = a,\; x_7 = a + h,\; x_{12} = a + 2h,\; \hdots, x_N = b, $$
so that in such a grid, a forward difference with respect to the point $x_0$ would be given by
    $$ D^+f(x_0) = \frac 1 h \left( f^7 - f^0 \right). $$
Although this is an unnatural way to do things in 1D, it is the natural scenario arising in higher dimensions. For this, and for the sake of simplicity, we will restrict the presentation to meshes given by tensor products of intervals, i.e. $\Omega = (a_x, b_x) \times (a_y, b_y)$ in 2D and $\Omega = (a_x, b_x) \times (a_y, b_y) \times (a_z, b_z)$ in 3D. The main difficulty will be that of creating an ordering of the degrees of freedom.

Let us now consider the 2D case. Consider $\Omega = (a_x, b_x) \times (a_y, b_y)$ and an equispaced discrete grid $\Omega_h$ with $ a_x = x_0, \hdots, x_{N_x-1} = b_x$ and $a_y = y_0, \hdots, b_y = y_{N_y-1}$, where $N_x$ and $N_y$ are number of points in each subdomain. Then, each grid point will be given by     
    $$ \vec X_{ij} \coloneqq (x_i, y_j) \qquad \forall i,j \in \{0,\hdots, N_x-1\}\times \{0,\hdots, N_y-1\}, $$
and analogously we can define a (matrix) grid function associated to some continuous function $f:\Omega\to \R$ as 
$$ \vec f^{ij} \coloneqq f(\vec X_{ij}) = f(x_i, y_j) \in \R^{N_x}\times \R^{N_y} $$
We can flatten 2D matrices to obtain 1D vectors through the definition of an \textit{index map} $I: \{0,\hdots, N_x-1\} \times \{0, \hdots, N_y-1\}\to \{0,\hdots, N_xN_y-1\}$ given by 
    $$ I \coloneqq I(i,j) = i + j N_x.$$
Fortunately, this map can be inverted as
    $$ I \mapsto (i(I), j(I)) \coloneqq (I\mod N_x, \lfloor I/N_x \rfloor), $$
and so we can uniquely define the 1D vector $\vec f^I$ as the vector of all grid points in $\Omega$ as
    $$ \vec f^I \coloneqq \vec f^{i(I)j(I)} = f(\vec X_{i(I)j(I)}) \in \R^{N_xN_y}. $$
For example, if one has a grid with $N_x=3$ and $N_y=2$, then the forward and inverse index maps are as follows:
    \begin{table}[ht!]
        \centering
        \begin{subfigure}{0.45\textwidth}
            \centering
        \begin{tabular}{c c | c}
            \toprule $i$ & $j$ & $I(i,j)$ \\ \midrule
            0 & 0 & 0 \\
            1 & 0 & 1 \\
            2 & 0 & 2 \\
            0 & 1 & 3 \\
            1 & 1 & 4 \\
            2 & 1 & 5 \\ \bottomrule
        \end{tabular}
        \caption{Forward map}
        \end{subfigure}
        \begin{subfigure}{0.45\textwidth}
            \centering
        \begin{tabular}{c | c c}
            \toprule $I$ & $i(I)$ & $j(I)$ \\ \midrule
            0 & 0 & 0 \\
            1 & 1 & 0 \\
            2 & 2 & 0 \\
            3 & 0 & 1 \\
            4 & 1 & 1 \\
            5 & 2 & 1 \\ \bottomrule
        \end{tabular}
        \caption{Backward map}
        \end{subfigure}
        \caption{Forward and inverse index maps for a 2D example.}
    \end{table}

Using these maps, we define a forward difference in $x$ and $y$ directions as $D^+_xf(\vec X)$ and $D^+_yf(\vec X)$ and note that they are given by
    \begin{align*}
        D^+_x f(\vec X_{ij}) = \frac 1{h_x} \left(\vec f^{(i+1)j} - \vec f^{ij}\right)= \frac 1 {h_x} \left( \vec f^{I(i+1,j)} - \vec f^{I(i,j)}\right), \\
        D^+_y f(\vec X_{ij}) = \frac 1{h_y} \left(\vec f^{i(j+1)} - \vec f^{ij}\right) = \frac 1{h_y} \left(\vec f^{I(i,j+1)} - \vec f^{I(i,j)}\right),
    \end{align*}
where we have used a slight abuse of notation by denoting both the 2D vector and 1D vectors as $\vec f^{ij}$ and $\vec f^I$ respectively. Further, the second derivative operators can be defined as 
\begin{align*}
    D_{xx} f(\vec X_{ij}) &= \frac{-1}{h_x^2} \left( -\vec f^{(i+1)j} + 2\vec f^{ij} - \vec f^{(i-1)j}\right) = \frac{-1}{h_x^2} \left( -\vec f^{I(i+1,j)} + 2\vec f^{I(i,j)} - \vec f^{I(i-1,j)}\right) = A_h f(\vec X_{ij}), \\
    D_{yy} f(\vec X_{ij}) &= \frac{-1}{h_y^2} \left( -\vec f^{i(j+1)} + 2\vec f^{ij} - \vec f^{i(j-1)}\right) = \frac{-1}{h_y^2} \left( -\vec f^{I(i,j+1)} + 2\vec f^{I(i,j)} - \vec f^{I(i,j-1)}\right) = f(\vec X_{ij})A_h^\top,
\end{align*}
where $A_h$ is the discrete second derivative matrix. Let us now apply this for the Laplace equation with zero boundary conditions, that is,
$$\begin{aligned}
    -\Delta u &= f & \tin \Omega, \\
    u &= 0 & \ton \partial\Omega.
\end{aligned}$$
Here, the corresponding discrete equation is
\begin{equation*}
    -\Delta u(\vec X_{ij}) = - \frac{\partial^2}{\partial x^2}\vec u(X_{ij}) - \frac{\partial^2}{\partial y^2} u(\vec X_{ij}) \approx A_h u(\vec X_{ij}) + u(\vec X_{ij})A_h^\top  = f(\vec X_{ij}),
\end{equation*}
which is a matrix equation since $u(\vec X_{ij})$ is a grid function. We define a suitable index map $I$, which allows us to write the 1D vectors $u^I$ and $f^I$, and now it remains to define the operator matrix for these 1D vectors. For this, we use the Kronecker product $\otimes$ defined by
$$
\begin{aligned}
    \otimes : \R^{m\times n} \times \R^{p\times q} &\to \R^{mp\times nq}, \\
    (A,B) &\mapsto A\otimes B \coloneqq \begin{bmatrix}
        A_{11}B & A_{12}B & \cdots & A_{1n}B \\
        A_{21}B & A_{22}B & \cdots & A_{2n}B \\
        \vdots & \vdots & \ddots & \vdots \\
        A_{m1}B & A_{m2}B & \cdots & A_{mn}B
    \end{bmatrix}. \\
\end{aligned}
$$
This operation is needed to correctly define the operator matrix. For this, we need a brief lemma to calculate this matrix. 
\begin{lemma}
    Let $A\in R^{m\times n}$, $B\in \R^{p\times q}$ and $X\in \R^{q\times n}$. Denote the vector form of a matrix (in our case, a grid function) $A$ as $\vec A^I$, $A_i\in \R^n$ the rows of $A$ and $X_i\in \R^q$ the columns of $X$. Then, 
    $$(BXA^\top)^I = (A\otimes B)\vec X^I$$.
    \begin{proof}
        We expand from the right hand side:
        \begin{align*}
            (A\otimes B)\vec x^I &= \begin{bmatrix}
                A_{11}B & A_{12}B & \cdots & A_{1n}B \\
                A_{21}B & A_{22}B & \cdots & A_{2n}B \\
                \vdots & \vdots & \ddots & \vdots \\
                A_{m1}B & A_{m2}B & \cdots & A_{mn}B
            \end{bmatrix} \begin{bmatrix}
                X_1^\top \\
                X_2^\top \\
                \vdots \\
                X_n^\top
            \end{bmatrix} \\
            &= \begin{bmatrix}
                \sum_{i=1}^n A_{1i} B X_i\\
                \sum_{i=1}^n A_{2i} B X_i\\
                \vdots \\
                \sum_{i=1}^n A_{mi} B X_i
            \end{bmatrix}\\
            &= \begin{bmatrix}
                BXA_1^\top\\
                BXA_2^\top\\
                \vdots \\
                BXA_m^\top\\
            \end{bmatrix}\\
            &= (BXA^\top)^I.
        \end{align*}
    \end{proof}
\end{lemma}
Now, we take the 1D vector form of the discrete Laplace equation and invoke the above lemma with $A=I$ the identity matrix:
\begin{align*}
    \vec f^I &= (A_h u(\vec X_{ij}) + u(\vec X_{ij})A_h^\top)^I\\
    &= (A_h u(\vec X_{ij}) I)^I + (I u(\vec X_{ij})A_h^\top)^I\\
    &= (I\otimes A_h)\vec u^I + (A_h^\top \otimes I)\vec u^I\\
    &= (A_h\otimes I + I\otimes A_h)\vec u^I \tag{$A_h$ symmetric}.
\end{align*}
Thus, our new operator is now $D^2_h = A_h\otimes I + I\otimes A_h$, and then the discrete Laplace equation is reduced to the linear system $D^2_h \vec u^I = \vec f^I$, where $D^2_h$ is sparse by construction. We can use the same methods as before to prove the stability of the scheme.  

\subsection{Time discretization}
Time discretization is an enormous topic, so here we provide simple strategies and ways to think about them. We will dedicate an entire section to their analysis using the von Neumann stability analysis. To avoid messing with Bochner spaces, we will refrain from considering a precise functional setting, and leave this for further ahead in the notes. Consider a differential operator $\mathcal L$ and its associated discrete matrix $\mat L_h$ which includes the problems boundary conditions. The continuous differential equation, for some initial condition $x(0) = x_0$, is given by 
    $$ \dot x + \mathcal L x = f, $$
    which we can discretize in space to obtain the system of differential equations
    $$ \dot x_h + \mat L_h \vec x_h = \vec f_h. $$
    Instead of considering a Taylor approximation of the time derivative, we will use a quadrature rule to obtain different strategies. For this, integrating in the interval $(t^n, t^{n+1})$ and using the notation $\vec x(t^n) \approx \vec x^{n}$, yields
    $$ \vec x^{n+1} - \vec x^n + \int_{t^n}^{t^{n+1}} \mat L_h \vec x_h(s) \,ds = \int_{t^n}^{t^{n+1}} \vec f_h(s)\,ds. $$

    We present three different schemes depending on three different quadrature rules: 
    \begin{itemize}
        \item Left-sided rule (Explicit):
                $$ \int \mat L_h \vec x_h(s)\,ds \approx \Delta t \mat L_h\vec x_h^n. $$
        \item  Right-sided rule (Implicit): 
            $$ \int \mat L_h \vec x_h(s)\,ds \approx \Delta t\mat L_h\vec x_h^{n+1}. $$
        \item  Trapezoidal rule (Mid-point): 
            $$ \int \mat L_h \vec x_h(s)\,ds \approx \frac{\Delta t}{2}\left(\mat L_h\vec x_h^{n+1}+ \mat L_h \vec x_h^n\right). $$
    \end{itemize}
    The commonly acknowledged relevant points here are: 
    \begin{itemize}
        \item The explicit scheme yields a system that is easy to solve but unstable.
        \item The implicit scheme is harder to solve but more (usually unconditionally) stable.
        \item The mid-point or \emph{Crank-Nicholson} scheme is also stable, with better accuracy but involves more computations and possible numerical saturation at small timesteps.
    \end{itemize}

    A natural generalization of these three schemes is the $\theta$-method, which for a fixed parameter $\theta\in[0,1]$, results in the quadrature rule
    $$\int \mat L_h \vec x_h(s)\,ds \approx \theta\Delta t \mat L_h\vec x_h^{n+1} + (1-\theta)\Delta t \mat L_h \vec x_h^n. $$
    We note that $\theta=1$ corresponds to the explicit scheme, $\theta=0$ to the implicit scheme, and $\theta=1/2$ to the mid-point scheme. 
    
\subsection{Von Neumann stability analysis}
Energy methods for stability analysis can quickly become untractable, so in order to get easier bounds, we can perform this analysis after moving the equation to the frequency domain. This is the so-called \emph{von Neumann stability analysis}, which is a very powerful tool for analyzing the stability of finite difference schemes. The idea is to assume that the solution can be expressed as a Fourier series, and then analyze the growth of the Fourier modes over time.

For this analysis, we will use the Fourier transform, which is comfortable for this task as it yields simple eigenfunctions of the derivatives. 

\begin{definition}[Fourier transform]
    Let $v\in L^2(\Omega)$. Then, its Fourier transform $\mathcal{F}(v) := \hat{v}$ is defined as
    $$ \mathcal{F}(v)(\xi) := \hat{v}(\xi) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty} v(x)e^{-i\xi x}dx, $$
    where $\xi$ is the frequency variable and $i=\sqrt{-1}$. This transform satisfies the Parseval identity:
    $$ \| v\|_{L^2} = \| \hat{v}\|_{L^2}, $$
    and the inverse transform is defined as
    $$ v(x) = \mathcal{F}^{-1}(\hat{v})(x) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty} \hat{v}(\xi)e^{i\xi x}d\xi. $$
    The most important identity we will use is the Fourier transform of the derivative, which is given by
    $$ \mathcal{F}(v')(x) = i\xi \hat{v}(\xi). $$
    This definition is mostly relevant for differential operators over $\R$, so it will only give guidelines for actual problems.
\end{definition}
Let us consider a basis function $e^{i\xi x}$ for a given wave number $\xi$, and for some finite difference scheme, its grid function
$$W_j = e^{i\xi (jh)}.$$
This function will be an eigenfunction of all translation-invariant finite difference operators. Set $x_j = jh$, then
$$D^+ W(x_j) = \frac{1}{h}\left(e^{i\xi (j+1)h} - e^{i\xi j h}\right) = \frac{1}{h}\left(e^{i\xi h} - 1\right)e^{i\xi j h} = \frac{e^{i\xi h} - 1}{h}W_j.$$
This helps in decoupling the degrees of freedom, and by the Parseval identity, we can again try to get a bound of the form
$$\|u^{n+1}\| = \|\hat{u}^{n+1}\|\leq |g(\xi)| \|\hat{u}^{n}\| = |g(\xi)| \|u^n\|,$$
where $g(\xi)$ is defined as an amplification factor. This scheme will be stable when $|g(\xi)|\leq 1$.

Let us now illustrate the application on the heat equation. Consider the boundary value problem
$$ \begin{aligned}
    \dot{u}(x,t) - u_{xx}(x,t) &= 0, &\quad (x,t) &\in \Omega \times (0,T), \\
    u(x,0)                  &= u^0(x), &\quad x &\in \Omega, \\
    u(x,t)                  &= f(x,t), &\quad (x,t) &\in \partial\Omega \times (0,T).
\end{aligned} $$
With the finite finite difference scheme we get
$$ \frac{u^{n+1}_j - u^n_j}{\Delta t} + \frac{1}{h^2} (-u^n_{j+1} +2u^n_j - u^n_{j-1}) = 0.$$
Here we set $u_j^n = e^{i\xi (jh)}$, and set the ansatz $u_j^{n+1} = g(\xi) u_j^n = g(\xi) e^{i\xi (jh)}$. Inserting this into the scheme we get
\begin{align*}
g(\xi) e^{i\xi j h} &= e^{i\xi (jh)} - \frac{\Delta t}{h^2}\left(-e^{i\xi (j-1) h} + 2e^{i\xi (jh)} - e^{i\xi (j+1) h}\right)\\
&= \left(1 + \frac{\Delta t}{h^2}\left(e^{-i\xi h} - 2 + e^{i\xi h}\right)\right)e^{i\xi(jh)}.
\end{align*}
We thus have
\begin{align*}
g(\xi) &= 1 + \frac{\Delta t}{h^2}\left(e^{-i\xi h} + e^{i\xi h}- 2 \right) \\
&= 1 + \frac{\Delta t}{h^2}\left(2\cos(\xi h) - 2\right) \tag{$e^{-i\xi h} + e^{i\xi h} = 2\cos(\xi h)$} \\
&= 1 - \frac{2\Delta t}{h^2}\left(1 - \cos(\xi h)\right).
\end{align*}
With this form, we can use the fact that $\cos(\xi h) \in (-1,1)$, which yield the following two bounds:
\begin{align*}
    g(\xi) &\leq 1 + \frac{2\Delta t}{h^2}(1-1) = 1 \\
    g(\xi) &\geq 1 + 2\frac{\Delta t}{h^2}(-1-1)  = 1 - \frac{4\Delta t}{h^2}.
\end{align*}
We note that the first inequality yields $g(\xi)\leq 1$, and from the second inequality we seek $-1\leq g(\xi)$. Bounding by below, we get
\begin{align*}
    1-4\frac{\Delta t}{h^2} \geq -1 &\iff 2\geq 4\frac{\Delta t}{h^2}\\
    &\iff \Delta t \leq \frac{h^2}{2},
\end{align*}
which yields an upper bound for the time step $\Delta t$ to ensure stability of the finite difference scheme. Inequalities of this kind are studied more thoroughly in \cite{LeVeque2007}.

\subsection{Stability in time}
We now look for stability estimates regarding also time. We first study a forward scheme 
$$
\begin{cases}
    \frac{u^{n+1}-u^n}{\Delta t} &= cu^n\\
    u^0&= u_{h,0},
\end{cases}
$$
with $c\in \R$. We note that a result such as $\|u^{n+1}\|\leq c\|u^n\|$ for $c\leq 1$ results in $\|u_h\|_{\infty, h} \leq \|u^0\|_{\infty, h}$. This is indeed a stability result, so it will be common tu study equations in the form $u^{n+1} = g(u^n)$. Given that our theory works for linear operators, we will start by studying a simple first-order equation:
$$
\begin{cases}
    u'+cu &= f\\
    u(0) &= u_0.
\end{cases}
$$
Now we look at the stability of forward and backward Euler and the $\theta$-method. 
\begin{enumerate}
    \item \underline{Explicit}: we have $u^{n+1}-u^n + c\Delta t u^n = \Delta t f^n$. Stability is a property of the differential operator, and thus we analyze it with $f\equiv 0$. We note that
    $$u^{n+1} = (1-c\Delta t)u^n \implies |u^{n+1}|\leq |1-c\Delta t||u^n|,$$
    and thus this scheme is stable if $|1-c\Delta t|< 1$. We note two cases:
    \begin{itemize}
        \item If $c > 0$, then we have $|1-c\Delta t|<1$ if $\Delta t < \frac{2}{c}$. This means that this method is \underline{conditionally stable}. 
        \item If $c< 0$, then $|1-c\Delta t|\geq 1$, which means that the scheme is \underline{unconditionally unstable}.
    \end{itemize} 
    \item \underline{Implicit}: we have $u^{n+1}-u^n + c\Delta tu^{n+1} = \Delta t f^n$. This yields
    $$(1+c\Delta t) u^{n+1} = u^n \implies |u^{n+1}| \leq \left|\frac{1}{1+c\Delta t}\right| |u^{n}|.$$
    \begin{itemize}
        \item If $c > 0$,  we have $1+c\Delta t >1$, which means that the method is \underline{unconditionally stable}. 
        \item If $c < 0$, then $1+c\Delta t <1$, so the scheme is \underline{unconditionally unstable}.
    \end{itemize}     
    It is important to note that if $c<0$, then the solution is just $u(x) = Ce^x$, so a condition such as $|u^{n+1}|<|u^n|$ does not make sense. Thankfully, our analysis reveals that, and we now just assume $c>0$. 
    \item \underline{$\theta$-method}: we have
    $$
    u^{n+1}-u^n + \theta c\Delta t u^{n+1} + (1-\theta)c\Delta t u^n = 0.
    $$
    Rearranging this we get
    $$
    (1+\theta c\Delta t)u^{n+1} = (1- (1-\theta)c\Delta t) u^n \implies |u^{n+1}| \leq \left|\frac{1-(1-\theta)c\Delta t}{1+\theta c\Delta t}\right| |u^n|.
    $$
    We now recognize two cases. 
    \begin{itemize}
        \item If $\Delta t$ is big and such that $1-(1-\theta)c\Delta t < 0$, we have
        \begin{align*}
            -\frac{1-(1-\theta)c\Delta t}{1+\theta c\Delta t} \leq 1 &\iff -1+(1-\theta)c\Delta t \leq 1+\theta c\Delta t\\
            &\iff (1-2\theta) c\Delta t \leq 2\\
            &\iff \left(\frac{1}{2}-\theta\right)c\Delta t\leq 1.
        \end{align*}
        So, if $\theta\geq 1/2$, the method is unconditionally stable, and if $\theta <1/2$, the method is \emph{conditionally stable}. 
        \item If $\Delta t$ is small, such that $1-(1-\theta)c\Delta t < 1 + \theta c \Delta t$, then we get $-c\Delta t < 1$, which is \emph{unconditionally stable}. 
    \end{itemize}
\end{enumerate}

\subsection{Unavoidable nonlinearities}
Sometimes, specially in stationary models, we cannot avoid having to solve a nonlinear problem. Two common ways to obtain an approximate solution to a nonlinear problem are 1) fixed point methods (usually of first order), and 2) Newton-Raphson methods. To analyze them, consider the domain $\Omega=(a,b)$ and the example problem
$$
\begin{cases}
    -u'' + \sin(u) &= 0\\
    u(a) &= u^a\\
    u(b) &= u^b.
\end{cases}
$$
As usual, we set a discrete grid of $N$ equispaced interior points $a=x_1<\cdots<x_N=b$. Thus, at each non-boundary node $x_i$ we have the equation
$$
-\frac{1}{h^2}(u^{i+1}-2u^i+u^{i-1}) + \sin(u^i) = 0.
$$
We write $g(\vec u_h) = \{g(u^i)\}$ for the vector of evaluations of $g$ at the non-boundary nodes. This allows us to write more compactly the root-finding problem
$$F(\vec u_h):= \mat A_h \vec u_h + \sin(\vec u_h) = 0.$$
Let us analyze both methods for solving this problem.
\begin{enumerate}
    \item \underline{Newton-Raphson}: we look for solutions of a linearized problems and iterate. Abstractly, we solve the equation $F(\vec x)=0$ by considering an initial point $\vec x^0$ and then using Taylor's theorem for linearizing $F(\vec x^{k+1})$ (unknown) around the previous iteration result $\vec x^k$ (known):
    $$
    F(\vec x^{k+1}) \approx F(\vec x^k) + \nabla F(\vec x^k)\cdot \underbrace{\vec{\delta x}^{k+1}}_{\vec x^{k+1} - \vec{x}^k}.
    $$
    We can now solve a system for $\vec{\delta x}^{k+1}$ such that $F(\vec x^{k+1}) = 0$, that is,
    $$\underbrace{[\nabla F(\vec x^k)]}_{\text{matrix}} \underbrace{\vec{\delta x}^{k+1}}_{\text{vector}} = \underbrace{-F(\vec x^k)}_{\text{vector}},$$
    along with the update step
    $$\vec x^{k+1} = \vec{x}^k + \vec{\delta x}^{k+1}.$$
    We call such a system a \textit{Newton iteration} or \textit{tangent system}. Also, $\vec{\delta x}^{k}$ must have homogeneous (zero) boundary conditions, so that all $\{\vec x^{k}\}_k$ satisfy the original ones. 
    
    In our problem, the $i$-th component of $F(\vec u_h)$ is given by
    $$
    [F(\vec u_h)]^i = -\frac{1}{h^2}(u^{i-1}-2u^i+u^{i+1}) + \sin(u^i),
    $$
    which we differentiate to get
    $$
    [\nabla F(\vec u_h)]_{ij} = \frac{\partial [F(\vec u_h)]^i}{\partial u^j} = \begin{cases}
        -\frac{1}{h^2} &j\in\{i-1,i+1\}\\
        \frac{2}{h^2} + \cos(u^i) &j=i\\
        0 &\text{elsewhere.}
    \end{cases}
    $$
    We can write more compactly this gradient as
    $$
    \nabla F(\vec u_h) = \mat A_h + \textbf{diag}(\cos(\vec u_h)),
    $$
    which allows us to write the tangent system as
    $$
    \left(\mat A_h + \textbf{diag}(\cos(\vec u^k))\right)\vec{\delta u}^{k+1} = -\left(\mat A_h \vec{u}^k + \textbf{diag}(\sin(\vec u^k))\right).
    $$
    \item \underline{Fixed point}: similarly to the time-dependent case, we can \textit{delay} the evaluation of a nonlinearity. We do this by choosing a starting point $\vec u^0$ and evaluating the nonlinear part in the previous iteration $\vec u^k$. This yields
    $$
    -\nabla \vec u^{k+1} + \sin(\vec u^k) = 0.
    $$
    Note that we can enforce this equality at the discrete level as well, i.e. 
    $$
    -\nabla \vec u_h^{k+1} + \sin(\vec u_h^k) = 0,
    $$
    where $\vec u_h^k$ is the discrete vector $\vec u_h$ at the $k$-th iteration. In such cases, we can sometimes prove the convergence of the scheme. For that, we consider the discrete iterates $\vec u_h^k$ given by
    \begin{align*}
        \mat A_h \vec u_h^{k+1} + \sin(\vec u_h^k) &= 0
        \mat A_h \vec u_h^{k} + \sin(\vec u_h^{k-1}) &= 0,
    \end{align*}
    which subtracted give the equation
    $$
    \mat A_h(\vec u_h^{k+1} - \vec u_h^{k}) = -(\sin(\vec u_h^k) - \sin(\vec u_h^{k-1})).
    $$
    We typically call this equation an \textit{error equation}. Finally, we can use the fact that $\mat A_h$ is stable, so we can bound the error norm as
    \begin{align*}
        \|\vec u_h^{k+1} - \vec u_h^{k}\|_{\infty, h} &\leq C\|\mat A_h (\vec u_h^{k+1} - \vec u_h^{k})\|_{\infty, h} \tag{Stability}\\
        &= C\|-(\sin(\vec u_h^k) - \sin(\vec u_h^{k-1}))\|_{\infty, h}\tag{Error equation}\\
        &\leq C\|\vec u_h^{k+1} - \vec u_h^{k}\|_{\infty, h},\tag{$\sin$ is $1$-Lipschitz}
    \end{align*}
    thus the scheme is convergent if $C<1$. Otherwise, the scheme \underline{can converge}, but we have no theoretical guarantee from this analysis. Still, each fixed point iteration
    $$
    \mat A_h \vec u_h^{k+1} = - \sin(\vec u_h^k)
    $$
    is much simpler than a Newton iteration
    \begin{align*}
        \left(\mat A_h + \textbf{diag}(\cos(\vec u^k))\right)\vec{\delta u}^{k+1} &= -\left(\mat A_h \vec{u}^k + \sin(\vec u^k)\right)\\
        \vec u_h^{k+1} &= \vec{u}^k_h + \vec{\delta u}_h^{k+1}.
    \end{align*}
\end{enumerate}
Both fixed point and Newton-Raphson methods are commonly used strategies, and in general there is no universally better choice. We study nonlinear problems more thoroughly in section \ref{sec:nonlinear}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Functional analysis}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this section we will review some important properties of functional spaces and operators. These things should be deemed as 'review' material. Intrinsically new things will start appearing in Section~\ref{section:beyond-ellipticity}. Most, if not all, results will be coming from the amazing book \emph{Linear and nonlinear functional analysis} by PG Ciarlet.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Weak derivatives and Sobolev spaces}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Banach and Hilbert spaces} Throughout the entire manuscript, we will rely on Banach spaces, Hilbert spaces, and their duals. Despite the existence of a flexible theory of Banach space formulations, we will mostly rely on Hilbert spaces because of their many nice properties. For now, let's simply review some relevant properties: 
    \begin{itemize}
        \item Continuous linear operators acting on Banach spaces have an induced operator norm: if $T: X\mapsto Y$, then 
        $$ \| T\| =  \sup_{x\in X}\frac{|Tx|_Y}{|x|_X}. $$
        We denote the space of continuous linear operators from $X$ to $Y$ as $L(X,Y)$. 
        \item Given a Banach space $X$ and a linear operator $T:X\to Y$, where $Y$ is a normed space, we can define the norm induced by $T$ in the space $W=\text{Dom}(T)\subseteq X$ as
        $$ \|x\|_{W} := \|x\|_X + \|Tx\|_Y. $$
        This will be important when constructing Sobolev spaces and their norms. We can show this is indeed a norm: taking $x,y\in W$ and $\alpha\in \R$ we get
        \begin{itemize}
            \item Triangle inequality:
            \begin{align*}
                \|x+y\|_W &= \|x+y\|_X + \|T(x+y)\|_Y \\
                &\leq \|x\|_X + \|y\|_X + \|Tx\|_Y + \|Ty\|_Y \tag{$\|\cdot\|_X$ and $\|\cdot\|_Y$ are norms}\\
                &= \|x\|_W + \|y\|_W.
            \end{align*}
            \item Homogeneity: since $T$ is linear, we get 
            $$\|\alpha x\|_W = \|\alpha x\|_X + \|T(\alpha x)\|_Y = |\alpha|\|x\|_X + |\alpha|\|Tx\|_Y = |\alpha|\|x\|_W.$$
            \item Positive-definiteness: we note that $0=\|x\|_W\geq \|x\|_X\geq 0$, which implies $x=0$.
        \end{itemize}
        \item Banach spaces are complete metric spaces. For a given Banach space $X$, its (topological) dual is the space $X'$ of functions $X\mapsto \R$, which is always Banach. The norm in $X'$ is referred to as the dual norm, and for $T\in X'$ it is defined as 
            $$\|T\|_{X'} = \sup_{\substack{x\in X\\ \|x\|_X\leq 1}} |T(x)|= \sup_{x\in X}\frac{|T(x)|}{\|x\|_X}.$$
        The action of an element of the dual space is sometimes denoted as $\langle T, x\rangle_{X'\times X}$, so as to resemble the notation of an inner product. In general, one can identify a part of the bidual space $X'' = (X')'$ through the evaluation operator $T_f:X'\mapsto \R$ in $X''$ defined as $T_f(L) = L(f)$. This immersion is not surjective. 
        \item Hilbert spaces are Banach spaces with respect to the distance induced by a dot (inner) product, i.e. a bilinear form $(\cdot, \cdot ): X\times X\mapsto \R $ such that: 
            \begin{itemize}
                \item It is symmetric: $( x,y) = ( y, x)$
                \item It is linear in its first argument: $(\alpha x_1 + \beta x_2, y)=\alpha( x_1, y) + \beta( x_2, y)$
                \item It is positive definite: $(x,x)\geq 0$, where it is 0 only if $x=0$.
            \end{itemize}
        \item Inner products are mostly used as projections. This means that, in the same way that we can orthogonalize a vector $x$ with respect to $y$, we can also do this in the Hilbert space setting analogously as 
            $$ x_\perp \coloneqq x - (x, y)_H y. $$
        It can be quickly verified that the function $x_\perp$ is indeed perpendicular to $y$ in the sense that $(x_\perp, y)_H=0$. 
        \item The inner product yields the fantastic Riesz map, which is actually an isometry. This is given as follows: consider a Hilbert space $H$ with inner product $(\cdot, \cdot)_H$, then a Riesz map is an operator $R_H: H\mapsto H'$ such that for any $x,y$ in $H$ it holds that $\langle R_H(x), y\rangle_{H'\times H} = (x, y)_H$. Notably, $\|R_H(x)\|_{H'} = \| x \|_H$. 
    \end{itemize}
One fundamental aspect of Hilbert spaces is that they provide some intuitive properties related to projections, which we recall through the following results: 
\begin{theorem}[Best approximation]
    Set $U\subset H$ a closed subspace of a Hilbert space $H$ and set $f$ in $H$. Then there exists a unique $g$ in $U$ such that
        $$ \|f - g \|_H = \inf_{u\in U} \| f - u\|_H. $$
\end{theorem}
Using this, we can uniquely define the orthogonal complement of a set $U$: 
    $$ U^\perp \coloneqq \{v\in H: (v, u)_H = 0 \quad\forall u\in U\}. $$
\begin{theorem}
    Set $U$ a closed subspace of a Hilbert space $H$. Then, if $f$ is in $H$, there exists a unique pair $(u,v)$ in $U\times U^\perp$ such that 
        $$f = u + v.$$
\end{theorem}
Some common and/or simple examples: Helmholtz decomposition, zero average functions, zero trace tensors, symmetric tensors. Note that the orthogonal complement is defined with respect to a \emph{given} inner product.

\example{
  Assume we want to orthogonalize with respect to $U=\R$. Then, we have that there is a constant $c$ such that for $f$ in a Hilbert space $H$ we can write
    $$ f = h + c, $$
  with $h\perp U$. Noting that a function $x$ satisfies $x\perp U$ iff $(x,1)_H = 0$, then we can use the previous expression to obtain 
    $$ (f,1)_H = (c,1) = c(1,1)_H, $$ 
  which yields
    $$ c = \frac{(f,1)_H}{(1,1)_H}, $$
  and
    $$h = f - c = f -  \frac{(f,1)_H}{(1,1)_H}f.$$
}

The most important spaces for us will be the Lebesgue spaces $L^p(\Omega;\R^d)$ given by measurable functions $f:\Omega \mapsto \R^d$ such that
    $$ \int_\Omega |f|_{\R^d}^p\,dx < \infty. $$
It will be important to know that if $|\Omega|<\infty$, then these spaces form an ordered inclusion: 
    $$ L^\infty(\Omega) \subset L^p(\Omega) \subset ... \subset L^1(\Omega). $$
A simple way to remember this is to split a function as $f = I_{|f|\leq 1}f + I_{|f|\geq 1}f$ and note that $|x|^p < |x|^{p+\epsilon}$ for $\epsilon > 0$. 

\paragraph{Distributions and derivatives} To formulate differential equations in Banach/Hilbert spaces, it will be important to be able to define derivatives in such spaces. This is done through the language of distributions, invented (discovered) by L Schwartz. For this, we require the notion of 'test functions', i.e. functions on which we can discharge derivatives of abstract objects through integration by parts. Consider then a function $f$ in $C_0^\infty(\R^d)$, the space of infinitely differentiable scalar functions with compact support in $\R^d$, then a distribution is simply an element $T$ in the dual space $(C_0^\infty(\R^d))'$, whose action can be written as $\langle T, f\rangle_{(C_0^\infty)'\times C_0^\infty}$, or sometimes simply as $\langle T, f\rangle$, if it is clear by context. 

The topology we consider in each space is the usual one, meaning the ones induced by the norms. In $\mathcal D(\R^d)\coloneqq C_0^\infty(\R^d)$, the norm is the supremum over all derivatives:
$$ \| f \|_{\mathcal D(\R^d)} \coloneqq \sup_{k} \sup_{|\alpha|} \|D^\alpha f\|_\infty, $$
where $\alpha$ is a multi-index. This means that $\alpha=(\alpha_1, \hdots, \alpha_N)$, and $|\alpha| = \sum_i \alpha_i$. For example, when $k=2$, the possible multiindices in $\R^2$ are $(2,0)$, $(1,1)$, and $(0,2)$, and the differential operator is given by 
$$D^\alpha \coloneqq \frac{\partial^{|\alpha|}}{\partial^{\alpha_1}\hdots \partial^{\alpha_N}}. $$
Using that, the norm of a distribution is simply given by the operator norm. 

The idea is to generalize the notion of action through integration, so that for sufficiently smooth functions $f$, their induced distribution is $Tf$ given by
    $$ \langle Tf, g \rangle = \int_{\R^d} fg\,dx. $$

\example{All classical (or strong) derivatives coincide with the weak derivatives, as seen from the integration by parts formulas. Also, consider the Dirac delta distribution given by
    $$ \langle \delta_x, f\rangle = f(x), $$
sometimes written as $\delta_x(f)$, or also simply as $\int_\Omega \delta_x f\,dx$ (with a \emph{not too mild} abuse of notation).
Then, its derivative is given by 
    $$ \langle \delta', f \rangle = -\langle \delta_x, f'\rangle = - f'(x).$$
}

This integral approach, when thinking about integration by parts formulas, allows us to define distribution derivatives as
    $$ \langle \partial_i T, f\rangle \coloneqq -\langle T, \partial_i f\rangle, $$
as given by integration by parts. This is known as a \emph{weak derivative}. Arbitrary order differential operators can be defined analogously, most importantly $\grad, \dive, \curl$, given by 
    \begin{align*}
        \langle \dive T, f\rangle &\coloneqq -\langle T, \grad f\rangle \\
        \langle \curl T, f\rangle &\coloneqq \langle T, \curl f\rangle.
    \end{align*}

\paragraph{Sobolev spaces} The notion of weak derivatives allows us to define differentiable Hilbert spaces, given by 
    $$ W^{1,p}(\Omega) \coloneqq \{ f\in L^p(\Omega): \grad f \in L^p(\Omega)\}. $$
These are Banach spaces with norm
    $$ \| x \|_{W^{1,p}(\Omega)} \coloneqq \| x \|_{L^p(\Omega)} + \| \grad x \|_{L^p(\Omega)}. $$
This is the graph norm of the $\nabla$ operator, and it can be further seen as the $\ell^1$ norm of the two-dimensional vector $(\|x\|_{L^p(\Omega)}, \|\grad x\|_{L^p(\Omega)})$, and thus all vector norms for such a vector induce equivalent norms for Sobolev spaces. It is very common to use the following notations:   
    \begin{itemize}
        \item $H^1(\Omega) = W^{1,2}(\Omega)$.
        \item $\| x \|_{L^2(\Omega)} = \| x \|_{0,\Omega}$, or even simply $ \| x\|_0$, depending on the laziness of the person writing.
        \item $\| x \|_{H^1(\Omega)} = \|x\|_{1,\Omega}$.
    \end{itemize}
The space $H^1(\Omega)$ is very important, as it is a Hilbert space with inner product
    $$ (x,y)_{1,\Omega} \coloneqq (x,y)_{0,\Omega} + (\grad x, \grad y)_{0,\Omega}. $$
Analogously, we can define the spaces
    $$ H(\dive; \Omega) = \left\{f\in L^2(\Omega): \dive f \in L^2(\Omega)\right\} $$
and    
    $$ H(\curl; \Omega) = \left\{f\in L^2(\Omega): \curl f \in L^2(\Omega)\right\}$$
Their application depends on the context, so we only keep here their definition. They are also Hilbert spaces, with the inner product defined as the one in $H^1$ but with the corresponding differential operators. Note that $H^1$ functions belong to both $H(\dive)$ and $H(\curl)$, but inclusions among them are not clear. We conclude this section with the celebrated Sobolev embedding results: 

\begin{theorem}[Sobolev embeddings (continuous)]
    Consider $\Omega$ a bounded Lipschitz domain in $\R^d$, set $m,j$ two non-negative integers, and $p$ in $[1,\infty]$. Then the following embeddings hold: 
    \begin{enumerate}
        \item If $mp < d$ then for $p \leq q \leq \frac{dp}{d-mp}$:
            $$ W^{j+m, p}(\Omega) \hookrightarrow W^{j,q}(\Omega). $$
        \item If $mp = d$, then for $p \leq q < \infty$:
            $$ W^{j+m, p}(\Omega) \hookrightarrow W^{j,q}(\Omega). $$
        \item If $mp > d \geq (m-1)p$, then 
            $$ W^{j+m,p}(\Omega) \hookrightarrow C^j(\bar\Omega). $$
    \end{enumerate}
\end{theorem}

\begin{theorem}[Sobolev embeddings (compact)]
Consider $\Omega$ a bounded Lipschitz domain in $\R^d$, $m\geq 1$ integer, $j\geq 0$ integer and let $p$ in $[1,\infty)$. Then the following embeddings are compact:
    \begin{enumerate}
        \item If $mp \leq d$  then for $q$ in $[1, \frac{dp}{d - mp})$:
            $$ W^{j+m, p}(\Omega) \hookrightarrow W^{j,q}(\Omega). $$
        \item If $mp > d$, then 
            $$ W^{j+m, p}(\Omega) \hookrightarrow C^j(\bar\Omega). $$
    \end{enumerate}
These inclusions hold also if the arrival domain is an arbitrary subdomain of $\Omega$. 
\end{theorem}
These theorems hold in greater generality, and are typically used together with the weak-compactness of the unit ball to show the existence of certain strongly convergent subsequence. We will see examples of this further ahead. One fundamental consequence is that $H^1$ is compactly embedded in $L^2$.

It is important to note that the embeddings are done through the identity map, and the main difference lies in the norms. In this sense, one immediately sees that, for example, the embedding $I:W^{1,p}(\Omega)\to L^p(\Omega)$ is continuous. In fact, it is defined by $Ix=x$, and we get
$$
\|Ix\|_{L^p(\Omega)} = \|x\|_{L^p(\Omega)} \leq \|x\|_{L^p(\Omega)} + \|\nabla x\|_{L^p(\Omega)} = \|x\|_{W^{1,p}(\Omega)},
$$
which shows that the map $I$ is continuous with norm 
$$\|I\|_{L(W^{1,p}(\Omega), L^p(\Omega))}\leq 1.$$
Also, we can see in $\R$ how this can be non-trivial and unexpected. Consider $f:I\to \R$ with $I=(a,b)$ and $f'\in L^2(I)$. Then, the fundamental theorem of calculus gives
$$f(x+h) = f(x) + \int_x^{x+h}f'(s)\mathrm{d}s,$$
and thus we can bound
$$|f(x+h)-f(x)|\leq \left|\int_x^{x+h}f'(s)\mathrm{d}s\right|\leq \int_x^{x+h}|f'(s)|\mathrm{d}s \overset{\text{C-S}}{\leq} \|f'\|_{L^2((x,x+h))}h \leq \|f'\|_{L^2(I)}h \quad \forall x\in I. $$
This shows that if $f'\in L^2(I)$, then $f$ is uniformly continuous. In particular, we have the injection $H^1(I)\to C^0(\overline{I})$. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Traces}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Traces or trace operators are the ones that restrict a function in $\R^d$ to some set in $\R^{d-1}$, most commonly the boundary of a domain. They are fundamental to adequately define boundary conditions. For the presentation of this section, we follow \cite{gatica2014simple} and \cite{monk2003finite}. Some details about Sobolev spaces are drawn from \cite{adams2003sobolev}. The fundamental difficulty of defining trace operators is that the domain where the boundary condition is defined has measure 0 in the measure of the starting domain, so some regularity of the function is required to guarantee that this operation makes sense. We will not enter the details of how a Lipschitz boundary is defined, see \cite{monk2003finite} for further details.

There are several definitions and constructions here that are needed for everything to make sense. We will follow them in a reasonable order, but this might be a very personal vision, so please read other formulations to have a more well-rounded vision. We will denote with $C_0^\infty(X)$ the space of functions with compact support in $X$, and also with $\D(\bar\Omega)$ the functions in $C_0^\infty(\R^d)$ with support in an open set $U$ such that $\bar\Omega\subset U$. This belongs to a wider set known as the Schwartz class of functions. If the set $X$ is open, we may denote $\D(X)$ as $C_0^\infty(X)$ with a bit of an abuse of notation.

\begin{itemize}
    \item Classic densities: $\D(\bar\Omega)$ is dense in $L^p(\Omega)$ if $\Omega$ is bounded and Lipschitz.
    \item $C^\infty(\bar\Omega)$ is dense in $W^{s,p}(\Omega)$ for $s$ a positive integer and $p\in [1,\infty)$.
    \item For $s$ a positive integer and $p\in (1,\infty)$, we have that there is a continuous linear extension $\Pi: W^{s,p}(\Omega) \to W^{s,p}(\R^d)$ such that $\Pi u|_\Omega = u$ for all $u$ in $W^{s,p}(\Omega)$.
\end{itemize}

Some technicalities arise when $\Omega$ is unbounded. For the sake of this course, all domains are bounded and Lipschitz unless otherwise stated. The following theorem allows us to extend the trace operator, which we initially define as $\gamma_0: \D(\bar\Omega) \to C^\infty(\partial\Omega)$, given by
    $$ \gamma_0 u = u|_{\partial\Omega}.$$
An important property that we will use many times is the \emph{trace inequality}, which states that there exists $C$ positive such that 
    $$ \| \gamma_0 f \|_{0,\partial\Omega} \leq C \| f \|_{1,\Omega} \qquad\forall f \in \mathcal D(\bar\Omega). $$

\begin{theorem}[Trace theorem]\label{thm:trace-theorem}
    Set $\Omega$ a bounded and Lipscthiz domain. Then, considering $1/p < s \leq 1$, there exists a continuous extension of $\gamma_0$ given by $\gamma_0: W^{s,p}(\Omega) \to W^{\frac{s-1}{p}, p}(\partial\Omega)$
    \begin{proof}
        We refer to \cite{adams2003sobolev} for a complete proof. We will simply see how to extend $\gamma_0$ from smooth functions to a linear bounded operator from $H^1(\Omega)$ to $L^2(\partial\Omega)$. This result requires the density of $\mathcal D$ in $H^1$ and the trace inequality. Consider thus a Cauchy sequence $\{\varphi_i\}_i$ in $\mathcal D(\bar\Omega)$ converging to some $v$ in $H^1(\Omega)$. Using linearity and continuity, we get that for some pair of indexes $i,j$: 
        $$ \| \gamma_0 \varphi_i - \gamma_0 \varphi_j \|_{0,\partial\Omega} = \| \gamma_0 (\varphi_i - \varphi_j) \|_{0,\partial\Omega} \leq C \| \varphi_i - \varphi_j \|_{1,\Omega}. $$
        This states that $\{ \gamma_0 \varphi_i \}_i $ is a Cauchy sequence in $L^2$, and thus has a limit $\xi$ in $L^2(\partial\Omega)$. Before setting $\xi$ as our extension, we need to check it is independent of the chosen sequence. This can be simply done by choosing another sequence such that $\{\tilde \varphi_i\}_i$ converges also to $v$. Then, it holds that 
        $$ \| \gamma_0 \tilde \varphi_i - \xi \| \leq \| \gamma_0(\tilde\varphi_i - \varphi_i) + \gamma_0 \varphi_i - \xi \| \leq \| \gamma_0(\tilde\varphi_i - \varphi_i) \| + \| \gamma\varphi_i - \xi \|. $$
        The second term goes to zero as we showed previously. For the first one, we use the trace inequality again to obtain 
        $$ \| \gamma_0(\varphi_i - \tilde\varphi_i) \| \leq C \|\varphi_i - \tilde \varphi_i \|, $$
        which concludes the proof. 
    \end{proof}
\end{theorem}

This trace is sometimes referred to as the \emph{Dirichlet} trace, as it is used to define Dirichlet boundary conditions. We can now define the Sobolev spaces "with boundary conditions", i.e. 
    $$ W_0^{1,p} = \{ u \in L^p(\Omega): \grad u \in [L^p(\Omega)]^d \text{ and } \gamma_0 u = 0 \}. $$
Here, we used the standard notation $[X(\Omega)]^d \coloneqq X(\Omega)\times \hdots \times X(\Omega)$. We thus get the extensively used spaces: 
    $$
    \begin{aligned}
        H_0^1(\Omega) &= W_0^{1,2}(\Omega) \\
        H^{-1}(\Omega) &= [H_0^1(\Omega)]' \\
        H^{1/2}(\partial\Omega) &= W^{1/2,2}(\partial\Omega) \\
        H^{-1/2}(\partial\Omega) &= [H^{1/2}(\partial\Omega)]',
    \end{aligned}
    $$
where the space $H^{1/2}(\partial\Omega)$ is the trace space associated to $H^1(\Omega)$, and the kernel of $\gamma_0$ is given by $H_0^1(\Omega)$. 

\paragraph{The trace spaces} This space has some nice properties, which we detail now. Its norm\footnote{Sobolev spaces of fractional order are a best on their own. The rigorous definition can be given using either Fourier transforms or by using the Slobodeckij seminorm. Both are very cumbersome and seldom used.} is given by 

    $$ \| u \|_{1/2,\partial\Omega} \coloneqq \inf\{\|U\|_{1,\Omega}: U \in H^1(\Omega) \text{ and } u = \gamma_0 U\}, $$
which naturally yields the following continuity estimate for the trace operator: 
    $$ \| \gamma_0 U\|_{1/2,\partial\Omega} \leq \| U \|_{1,\Omega} . $$
The trace space $H^{1/2}$ can be seen as a quotient space derived from $H^1$, so it is also a Hilbert space. A natural question is what the inner product looks like. To do that, we consider for a given $u$ in $H^{1/2}(\partial\Omega)$, an element that yields the norm, i.e. $U$ in $H^1(\Omega)$ such that $\gamma_0 U = u$ and $\| u \|_{1/2,\partial\Omega} = \| U \|_{1,\Omega}$. In such a case, we can consider the following inner product: 
    $$ (v_1, v_2)_{1/2,\partial\Omega} \coloneqq (V_1, V_2)_{1,\Omega}, $$
where $V_i$ are the extension functions. Finally, it will be useful to know that $H_0^1(\Omega)$ (and indeed also $W_0^{s,p}$) can be defined as a closure in terms of the $H^1$ norm: 
    $$ H_0^1(\Omega) \coloneqq \overline{C_0^\infty(\Omega)}^{\|\cdot \|_{1,\Omega}}. $$

\paragraph{Note on integration by parts formulas} These formulas will be important to define the normal and tangential traces. All formulas stem from the divergence theorem: 

\begin{theorem}[Divergence Theorem]
    Consider a bounded Lipschitz domain $\Omega$ in $\R^{d=2,3}$ and consider a vector field $\vec F:\R^d \to \R^d$ in $[C^1(\bar\Omega)]^d$. Then it holds that
        $$ \int_\Omega \dive \vec F\,dx = \int_{\partial\Omega}\vec F\cdot \vec n\,ds,$$
    where $\vec n$ is the outwards normal vector, $dx$ is the volume measure and $ds$ is the surface measure.
\end{theorem}

The relevant formulas are the following: 
    \begin{itemize}
        \item If $\xi$ in $C^1(\bar\Omega)$ and $\vec u$ in $[C^1(\bar\Omega)]^d$:
            $$ \int_\Omega (\dive \vec u) \xi\,dx = -\int_\Omega\vec u\cdot \grad \xi\,dx + \int_{\partial\Omega} \vec u \cdot \vec n \xi\,ds.$$
        \item (Green's [first] identity)\footnote{See \cite{monk2003finite} for the second one. It is useful to derive Boundary Element (BEM) methods.} If $\xi$ in $C^1(\bar\Omega)$ and $p$ in $C^2(\bar\Omega)$:
            $$ -\int_\Omega (\Delta p) \xi\,dx = \int_\Omega\grad p\cdot \grad \xi\,dx - \int_{\partial\Omega}\left(\grad p\cdot \vec n\right)\xi\,ds.$$
        \item Consider $\vec u,\vec \phi$ in $[C^1(\bar\Omega)]^d$: 
            $$ \int_\Omega (\curl \vec u) \cdot \vec\phi\,dx = \int_\Omega \vec u \cdot (\curl \vec \phi)\,dx  + \int_{\partial\Omega}(\vec n\times \vec u)\cdot \vec\phi\,ds.$$
    \end{itemize}

\paragraph{$H(\dive)$ and the normal trace} In this space, we have a first simple density result: 

\begin{theorem} Consider a bounded and Lipschitz domain $\Omega$ in $\R^d$, then $H(\dive;\Omega)$ is the closure of $[C(\bar\Omega)]^d$ in the $H(\dive)$ norm.
    \begin{proof}
        Sketch: The main idea is to show that the orthogonal complement of $[C(\bar\Omega]^d$ in $H(\dive;\Omega)$ is the trivial one. Then, one uses the orthogonality condition 
            $$ (\vec u, \vec \phi) + (\dive \vec u, \dive \vec \phi) = 0$$
        for all $\vec\phi$ in $[C(\Omega)]^d$ implies that $\grad \dive\vec u = \vec u$ is in $L^2$, and thus $\dive \vec u$ is in $H^1$. An adequate extension to all $\R^d$ and a density argument concludes the proof. For details, see \cite[Thm 3.22]{monk2003finite}. 
    \end{proof}
\end{theorem}
The normal trace is simply given for a smooth function as 
    $$ \gamma_N \vec v = \vec v|_{\partial\Omega} \cdot n, $$
where $\vec n $ is the outwards normal vector. This can be extended up to functions in $H(\dive)$ as stated in the following theorem. 

\begin{theorem}[Normal trace]
Consider a bounded Lipschitz domain $\Omega$ in $\R^d$ with outwards unit normal $\vec n$. Then, the mapping $\gamma_N$ can be extended to a continuous linear map $\gamma_N: H(\dive; \Omega) \to H^{-1/2}(\partial\Omega)$, and the following integration by parts formula holds: 
        $$ \langle \gamma_N \vec v, \phi \rangle_{-1/2, 1/2} = (\vec v, \grad \phi) + (\dive \vec v, \phi) \qquad \forall \vec v\in H(\dive;\Omega), \phi \in H^1(\Omega). $$
    \begin{proof}
        First, we note that the integration by parts formula, which holds for $C^\infty$ functions initially, can be extended by density to functions $\phi$ in $H^1(\Omega)$:
        $$ \langle \vec v\cdot \vec n, \phi \rangle = (\vec v, \grad \phi) + (\dive \vec v, \phi) \qquad \forall \vec v\in H(\dive;\Omega), \phi \in H^1(\Omega), \qquad \forall \vec v\in H(\dive;\Omega), \phi \in H^1(\Omega). $$
        Cauchy-Schwartz further yields
            $$ |\langle \vec v\cdot \vec n, \phi\rangle | \leq \| \vec v \|_{\dive} \| \phi \|_1. $$
        In particular, using the surjectivity of the Dirichlet trace, we get that for all $\mu$ in $H^{1/2}(\partial\Omega)$ it also holds that
            $$ |\langle \vec v\cdot \vec n, \mu\rangle | \leq \| \vec v \|_{\dive} \| \mu \|_{1/2}, $$
        which naturally yields
            $$ \| \vec v \cdot \vec n\|_{-1/2} \leq \| \vec v\|_{\dive}. $$
        This all implies that $\gamma_N$ is a bounded linear map from $[C(\bar\Omega)]^d$ to $H^{-1/2}(\partial\Omega)$, meaning that it can be extended by density to be defined in $H(\dive;\Omega)$ as well. We are only missing the surjectivity, for which we consider a generic function $\eta$ in $H^{-1/2}(\partial\Omega)$, and define the following problem: Find $\phi$ in $H^1(\Omega)$ such that
        $$ (\grad \phi, \grad \psi) + (\phi, \psi) = \langle \eta, \gamma_0 \psi \rangle_{-1/2,1/2} \qquad\forall \psi \in H^1(\Omega).$$
        This in particular implies that 
        $$ (\grad \phi, \grad \psi_0) + (\phi, \psi_0) = 0 \qquad\forall \psi_0 \in H_0^1(\Omega),$$
        so that, as in the previous proof, $-\dive \grad \phi = \phi$ distributionally and thus $\vec v \coloneqq \grad \phi$ is the $H(\dive;\Omega)$ function we were looking for.
    \end{proof}
\end{theorem}
Finally, we will use the space of functions with null normal trace, so that we initially define
    $$ H_0(\dive, \Omega) \coloneqq \overline{[C_0^\infty(\Omega)]^d}^{\|\cdot \|_{\dive}}, $$
i.e. the closure in the $\dive$ norm. The following result makes all the trouble worth it. 
\begin{theorem}
    Consider a bounded Lipschitz domain $\Omega$ in $\R^d$. Then: 
        $$ H_0(\dive;\Omega) = \{\vec v\in H(\dive;\Omega): \gamma_N \vec v = 0 \}. $$
    \begin{proof}
        The proof is done through the orthogonal complement. All techniques have been already shown here, so we simply refer the interested reader to \cite[Thm 3.25]{monk2003finite}.
    \end{proof}
\end{theorem}

\paragraph{$H(\curl)$ and the tangential trace} In the proofs involving $H(\dive)$, we heavily used the fact that the $\grad$ and $\dive$ are the transpose of one another. This is not the case for the $\curl$ operator, so the proofs for this case are notoriously more complicated. Instead, we will be happy with simply stating the related results:
    \begin{itemize}
        \item $$ H_0(\curl, \Omega) \coloneqq \overline{[C_0^\infty(\Omega)]^d}^{\|\cdot\|_{\curl}}. $$
        \begin{theorem}
            For a bounded Lipschitz domain $\Omega$, it holds that $[C(\bar\Omega)]^3$ is dense in $H(\curl;\Omega)$.
        \end{theorem}
        \item For a bounded Lipschitz domain $\Omega$ it holds that, if $\vec u$ in $H(\curl;\Omega)$ is such that
            $$ (\curl \vec u, \vec \phi) - ( \vec u, \curl \vec \phi) = 0$$
        for all $\vec \phi$ in $[C^\infty(\bar\Omega)]^3$, then $\vec u$ is actually in $H_0(\curl;\Omega)$.
        \item The trace operators here are two: 
            \begin{align*}
                \gamma_t \vec v &= \vec n \times \vec v, \\
                \gamma_T \vec v &= (\vec n \times \vec v) \times \vec n. 
            \end{align*}
        \item 
        \begin{theorem}
            For a bounded Lipschitz domain $\Omega$ it holds that the extension $\gamma_t: H(\curl;\Omega) \to H^{-1/2}(\partial\Omega)$ is bounded and linear, with the following integration by parts formula: 
                $$ (\curl \vec v, \vec\phi) - (\vec v, \curl\vec \phi) = \langle \gamma_t \vec v, \vec \phi\rangle \qquad\forall \vec v \in H(\curl;\Omega), \vec\phi \in \vec H^1(\Omega). $$
            **Note the bold space, which refers to a vector space. We will use this often. 
        \end{theorem}
        The operator $\gamma_t$ is not surjective simply because it is the limit of tangential vectors which will never have a normal component (at least intuitively). 
        \item Characterizing $\gamma_T$ is out of scope in this course. 
        \item 
        \begin{theorem}
            Consider a bounded Lipschitz domain $\Omega$. Then, 
                $$ H_0(\curl;\Omega) = \{ \vec v \in H(\curl;\Omega): \gamma_t \vec v = \vec 0\}. $$
        \end{theorem}
    \end{itemize}

\example{
One interesting context in which these trace operators show up is when considering normal or tangential boundary conditions. As we will see, one typically requires boundary information on all components of the solution on the boundary, but how this is done is highly context dependent. One nice example are \emph{slip} boundary conditions in fluid dynamics, where only the normal component of the fluid velocity is required to be 0: 
    $$ \vec u \cdot \vec n = 0.$$
This has to be complemented with conditions in the tangential direction. One possibility would be to prescribe some tangential velocity: 
    $$ (\ten I - \vec n \otimes \vec n)\vec u = (\vec n \times \vec u) \times \vec n = \vec v_\tau, $$
but one can also look at the tangential components of the stress tensor, thus generating a "tangential" Neumann boundary condition: 
    $$ (I - \vec n \otimes \vec n)[\sigma(\vec u)\vec n] = \vec g_\tau. $$
}

\subsection{Weak formulations}
A weak formulation refers to an integral form of a PDE, understood distributionally. This is typically a systematic procedure that should not be too difficult, and it helps in revealing what are the adequate boundary conditions for a given problem. The main tool for this will be the integration by parts formulas. Our test problem will be the Laplace problem, given by the $-\Delta$ operator. The minus sign will be better justified in the following section. Consider then the problem of finding $u$ such that 
    $$ -\Delta u = f \qquad \text{ in $\Omega$}. $$
Define an arbitrary smooth function $v$, then integration by parts yields
    $$ - \int_\Omega \Delta u v\,dx = -\int_{\partial\Omega}\gamma_D v \gamma_N \grad u \,ds + \int_\Omega \grad u \cdot \grad v\,dx $$
for all $v$. This function is typically called a \emph{test function}. The surface form suggest the boundary conditions: 
    $$ \int_{\partial\Omega}\underbrace{\gamma_D v}_\text{Dirichlet BC} \underbrace{\gamma_N \grad u}_\text{Neumann BC} \,ds,$$
so that we can have boundary conditions on the function itself  
    $$ u = g, $$
or on its normal derivative
    $$ \grad u \cdot \vec n = h. $$
This can be combined, so that for a given partition of the boundary into two sets $\Gamma_D$ and $\Gamma_N$ such that $\overline{\partial\Omega} = \overline{\Gamma_D}\cup\overline{\Gamma_N}$, one can have a Dirichlet boundary condition on $\Gamma_D$ and a Neumann boundary condition on $\Gamma_N$. For this type of boundary condition, one must define a solution space given by 
    $$ V_g = \{v \in H^1(\Omega): \gamma_D v = g \text{ on $\Gamma_D$}\}, $$
but let us focus first on spaces with null Dirichlet boundary condition ($V_0$). In this case, the boundary conditions will give
    $$ \int_{\partial\Omega}\gamma_Dv \gamma_N \grad u\,ds = \int_{\gamma_N} h \gamma_D v\,ds, $$
and thus the integral form of the equation will be given by
    $$ -\int_\Omega\Delta u v\,dx = -\int_{\Gamma_N}v h\,ds + \int_\Omega \grad u \cdot \grad v\,dx = \int_\Omega f v\,dx, $$
for all smooth $v$. Now, we note that (i) this formulation is well defined for $u,v$ in $H^1$ (and thus can be extended to hold for all $v$ in $H^1$ by density as long as $v$ satisfies the Dirichlet boundary conditions), (ii) the $\gamma_D$ operator has been omitted from the surface integral for convenience, and (iii) that the Dirichlet boundary condition does not appear anywhere in the formulation. This justifies naming Dirichlet boundary conditions \emph{essential}, and Neumann boundary conditions \emph{natural}. The \emph{weak formulation} of the problem thus refers to the following statement: Find $u$ in $V_0$ such that
    $$ (\grad u, \grad v)_{0,\Omega} = \langle f, v\rangle  + \langle h,  v\rangle \qquad \forall v\in V_0, $$
for given functions $f$ in $V_0'$ and $g$ in $(\gamma_D V_0)'$. Note the following: 
    \begin{itemize}
        \item The space of the solution and the test functions is the same. This is not mandatory, but it is common and can be better motivated by interpreting the Laplace problem as the first order equations related to the following minimization problem: 
            $$ \min_{v \in V_0} \int_\Omega |\grad v|^2\,dx . $$
        Then, one simply infers the spaces of each function from the definition of the Gateaux derivative. 
        \item The solution $u$ was formulated in a space without boundary condition. This is important because the regularity theory will depend on the solution space being a Hilbert space, and the space $V_g$ is not even a vector space as it is not closed under addition. This can be solved by defining adequate \emph{lifting} operators, i.e. a function $G$ in $H^1(\Omega)$ such that $\gamma_D G = g$ that allows us to write $u$ in $V_g$ as 
            $$ u = u_0 + G, $$
        where $u_0$ belongs to $V_0$. We can then rewrite the problem in $V_g$ as a problem in $V_0$ (and I encourage the reader to do this procedure at least once in their life). The existence of a lifting function in this case is given by the surjectivity of the Dirichlet trace, but it can be tricky in other contexts. This is also tricky in nonlinear problems, which justifies that nonlinear problems are typically studied with homogeneous boundary conditions.  
        \item We note that the Laplacian is now being interpreted as a \emph{distribution}, and thus the strong problem (including the boundary conditions) yields the definition of the \emph{action} of the distribution. In particular, this means that the action of the distribution naturally changes with the boundary conditions. This observation is fundamental to understand Discontinuous Galerkin methods, or other formulations defined on broken spaces (i.e. spaces that allow for discontinuities). 
    \end{itemize}

To conclude this section, we wanted to establish that if the solution is sufficiently regular, then the weak form is equivalent to the strong form. The main tool for this is the Fundamental Lemma of the Calculus of Variations: 

\begin{lemma}[Fundamental Lemma of Calc. of Var.]\label{lemma:lema-calvar}
    Consider $\Omega\subset \R^d$ bounded and set $f\in L^1(\Omega)$ such that
        $$ \int_\Omega f\varphi\,dx = 0 \qquad \forall \varphi \in C_0^\infty(\Omega). $$
    Then, $f=0$ almost everywhere.
\end{lemma}

Consider now the weak formulation of the Poisson problem: Find $u$ in $V_0$ such that 
    $$ (\grad u, \grad v) = (f, v) - (grad u_g, \grad v) + \langle t, v\rangle_{-1/2, 1/2},$$
    where $f$ and $\vec t$ are integrable. Then, integrating by parts one obtains
    $$ (-\Delta (u+u_g) - f, v)+\langle \grad u \cdot \vec n, v\rangle = \langle t, v\rangle_{-1/2,1/2}. $$
    If we consider test functions $v$ in $C_0^\infty(\Omega)$, then 
    $$ (-\Delta (u+u_g) - f, v) = 0, $$
    and if $u$ and the lifting function $u_g$ are integrable, then Lemma~\ref{lemma:lema-calvar} yields
    $$ -\Delta \tilde u = f, $$
    which is the strong form for the combined solution $\tilde u= u + u_g$. Given this, the weak form now yields
    $$ \langle \grad u \cdot \vec n, v\rangle = \langle t, v\rangle $$
    Using again Lemma~\ref{lemma:lema-calvar} on the subspace topology of $C_0^\infty(\Gamma_N)$, then
    $$ \grad u\cdot \vec n = t $$
    holds strongly. 

\example{
    We encourage the reader to try to compute the weak formulation of the Poisson problem in mixed form. To do this, one must define the auxiliary variable $\vec \sigma \coloneqq \grad u$, so that the strong form of the problem now becomes
        $$
            \begin{aligned}
                -\dive \vec\sigma &= f &&\text{ in $\Omega$}, \\
                \vec \sigma - \grad u &= 0 &&\text{ in $\Omega$}, \\
                \gamma_D u &= g &&\text{ on $\Gamma_D$}, \\
                \gamma_N \vec\sigma &= h &&\text{ on $\Gamma_N$}. 
            \end{aligned}
        $$
    This problem will be studied in detail further ahead. 
}

\subsection{Elliptic problems and Lax-Milgram}
One of the easiest classes of PDEs where we can prove existence and uniqueness of solutions are \textit{elliptic} problems. 
\begin{definition}[Elliptic forms]
    A bilinear form $a(\cdot, \cdot)$ defined on a Hilbert space $X$ is said to be elliptic if there exists a constant $\alpha$ such that
        $$ a(x, x) \geq \alpha \| x \|^2_X \qquad \forall x\in X. $$
\end{definition}
This property is the basis of the Lax-Milgram lemma, which gives sufficient conditions for the existence and uniqueness of solutions. 
\begin{lemma}[Lax-Milgram] Consider a bounded bilinear form $a: H\times H\to \R$ defined on a Hilbert space $H$ that is elliptic with constants $C$ and $\alpha$ respectively, and a linear functional $f$ in $H'$. Then, there exists a unique $u$ in $H$ such that 
    $$ a(u, v) = f(v) \qquad \forall v \in H. $$
This solution is continuous with respect to the data, in the sense that there exists a positive constant $C$ such that 
    $$ \| u\|_H \leq \frac 1 \alpha \| f \|_{H'} .$$
This is typically referred to as the \emph{a priori} estimate. 
\end{lemma}

\subsection{Poincar√© inequalities}
Using all of the previous definitions, we can finally look at actual problems and some first well-posedness results. For all of them, the Poincar√© inequality will be fundamental.

\begin{lemma}[Poincar√© inequality] Let $\Omega\subset \R^d$ be a bounded Lipschitz domain. Then, there exists $C>0$ such that
    \begin{equation*}
        \|u\|_{0,\Omega} \leq C\|\nabla u\|_{0,\Omega} \qquad \forall u\in H^1_0(\Omega).
    \end{equation*}
    \begin{proof}
        It suffices to show the result for functions in $C_0^\infty(\Omega)$, since this space is dense in $H_0^1(\Omega)$. By contradiction, assume that there exists a sequence $(v_n)_n\subset C_0^\infty(\Omega)$ such that $\|v_n\|_{0,\Omega}=1$ and 
        $$\|\nabla v_n\|_{0,\Omega} < \frac{1}{n} \qquad \forall n\in \mathbb{N}.$$
        Then, as $(v_n)_n$ is bounded in the reflexive space $H^1(\Omega)$, there exists a weakly convergent subsequence $(v_{n_k})_k$ to some $v\in H^1(\Omega)$, which converges strongly in $L^2(\Omega)$ by the Rellich-Kondrachov theorem, and in particular, it converges in norm, that is,
        $$\|v\|_{0,\Omega} = \lim_{k\to\infty} \underbrace{\|v_{n_k}\|_{0,\Omega}}_{=1} = 1.$$
        
        Given a vector field $\vec\phi = (\phi_1,\dots,\phi_d)\in (C_0^\infty(\Omega))^d$, we can show that for $w$ in $H_0^1$ the map $\varphi_{\vec{\phi}}:H^1(\Omega)\to\R$ given by $\varphi_{\vec{\phi}}(w) = (w, \nabla\cdot \vec \phi)$ is continuous:
        $$|(w,\nabla\cdot\vec\phi)| = |-(\nabla w, \vec\phi)| \leq \|\nabla w\|_{0,\Omega}\|\vec\phi\|_{0,\Omega} \leq \|w\|_{1,\Omega}\|\vec\phi\|_{0,\Omega},$$
        that is, $\varphi_{\vec{\phi}} \in (H^1(\Omega))'$. We note that for the subsequence $(v_{n_k})_k$ and for any $\vec\phi \in [C_0^\infty(\Omega)]^d$ we have 
        $$|(v_{n_k},\nabla\cdot\vec\phi)| \leq \|\nabla v_{n_k}\|_{0,\Omega}\|\vec\phi\|_{0,\Omega} < \frac{1}{n_k}\|\vec\phi\|_{0,\Omega}\overset{k\to\infty}{\longrightarrow} 0. $$
        Since $v_{n_k} \rightharpoonup v$ weakly in $H^1(\Omega)$, we have that for any $\vec\phi \in (C_0^\infty(\Omega))^d$ it holds that
        \begin{align*}
            (\nabla v,\vec\phi) = -(v,\nabla\cdot\vec\phi) = -\varphi_{\vec{\phi}}(v) = -\lim_{k\to\infty} \varphi_{\vec{\phi}}(v_{n_k}) = -\lim_{k\to\infty} (v_{n_k},\nabla\cdot\vec\phi) = 0,
        \end{align*}
        and thus $\nabla v = 0$. Since $\Omega$ is Lipschitz, it is connected, and by the generalized mean value theorem we can prove that $\nabla v = 0$ implies that $v=c$ almost everywhere for $c\in \mathbb{R}$. Now, consider $\vec\phi$ in $[C^\infty(\overline{\Omega})]^d$ and the functional $\psi_{\vec{\phi}}:H^1(\Omega)\to \mathbb{R}$ such that 
        $$\psi_{\vec{\phi}}(u) = \int_{\partial\Omega} \gamma_D u \vec\phi\cdot\vec n \mathrm{d}S,$$
        which is linear and continuous due to the trace inequality. Weak convergence implies
        $$\psi_{\vec{\phi}}(v) = \lim_{k\to\infty} \psi_{\vec{\phi}}(v_{n_k}) = \lim_{k\to\infty} \int_{\partial\Omega} v_{n_k} \vec\phi\cdot\vec n \mathrm{d}S = 0,$$
        since  $v_{n_k}|_{\partial\Omega} = 0$. Now replacing $v=c$ we get
        $$0 = \psi_{\vec{\phi}}(v) = \int_{\partial\Omega} v\vec\phi \cdot\vec n\mathrm{d}S = c\int_{\partial\Omega} \vec\phi\cdot\vec n \mathrm{d}S \qquad \forall \vec\phi \in [C^\infty(\overline\Omega)]^d,$$
        and thus $c=0$, which is a contradiction since we assumed that $\|v\|_{0,\Omega}=1$. 
    \end{proof}
\end{lemma}

This proof can be modified to accomodate for boundary conditions holding only in some portion of the boundary $|\Gamma_D|>0$. We can further generalize this inequality for $u\in H^1(\Omega)$.

\begin{lemma}[Generalized Poincar√© inequality] Let $\Omega\subset\mathbb{R}^d$ be a bounded Lipschitz domain. Then, there exists $C>0$ such that for a non-empty portion of the boundary $\Gamma \subseteq \partial\Omega$ it holds that
        $$ \| u \|_{0,\Omega} \leq C\left(| u |_{1,\Omega} + \left|\int_\Gamma u\,ds\right| \right) \qquad \forall u \in H^1(\Omega). $$
The result also holds in $L^p$ and $W^{1,p}$. 
\end{lemma}

We provide an incomplete proof to show some of the related techniques used to prove this result. Most developments come from \cite{brenner2008mathematical}.
\begin{proof}
    The main result to be used is the Bramble-Hilbert Lemma, which establishes (among other things) that if $B$ is a sufficiently big ball in $\Omega$ such that $\Omega$ is starred with respect to it, then the average over $B$ given by $\bar u = 1/|B|\int_B u\,dx$ satisfies
    $$ \| u - \bar u \|_{0,\Omega} \leq C| u |_{1,\Omega}, $$
where $C$ is a positive constant and $|\cdot|_{1,\Omega}$ is the $H^1$ semi-norm. The case $B=\Omega$ is known as the Friedrich's inequality. To recover Poincar√©'s inequality, one notes the two following properties: 
    $$ \|v \|_{0,\Omega} \leq \| v - \bar v\|_0 + \| \bar v\|_0 \leq C | v |_{1,\Omega} + \|\bar v\|_0,$$
where the first term was controlled using the Friedrich inequality. The second term is controlled by forcing another triangle inequality: 
    $$ \| \bar v\|^2_0 = \frac{|\Omega|}{|\Gamma|} \int_\Gamma \bar v\,ds \leq \frac{|\Omega|}{|\Gamma|}\left(\int_\Gamma (\bar v - v)\,ds + \left| \int_\Gamma v \,ds \right| \right)$$
and finally by using the trace inequality plus another application of the Friedrich inequality one gets the desired result. 
\end{proof}

For the case of pure Neumann boundary conditions, we will require an additional inequality.

\begin{lemma}[Poincar√©-Wirtinger inequality]
    Consider an open connected bounded domain $\Omega$ as previously. Then, if we define the volumetric average as $\bar u = \int_\Omega u\,dx$, then it holds that
        $$ \| u - \bar u \|_{L^p(\Omega)} \leq C \| \grad u \|_{L^p(\Omega)}. $$
    \begin{proof}
        The proof has been taken from \cite{evans2022partial}. By contradiction, we assume that there exists a sequence of functions $(u_k)_k$ in $W^{1,p}(\Omega)$ such that
            $$ \| u_k - \bar u_k \|_p > k \| \grad u_k \|_p. $$
        We can renormalize the sequence by setting
            $$ v_k = \frac{u_k - \bar u_k}{\|u_k - \bar u_k\|_p}, $$
        so that $ \bar v_k = 0$ and $\| v_k \| = 1$. Our hypothesis yields
        $$ \| \grad v_k \|_p = \| \|u_k - \bar u_k\|_p^{-1} \grad (u_k - \bar u_k) \|_p < \frac 1 k,  $$
        which in particular means that $(v_k)_k$ is a bounded sequence in $W^{1,p}(\Omega)$. This means that $(v_k)_k$ has a weakly convergent sequence in $W^{1,p}(\Omega)$, and as this space is compactly embedded in $L^p(\Omega)$ (Rellich-Kondrachov), then there exists an element $v$ in $L^p(\Omega)$ such that, possibly up to a subsequence, $v_{k_j} \to v$. This implies that $\bar v=0$ and $\|v\|_p=1$. Using the bound on $\grad v_k$, one can also show that for smooth functions $\phi$, 
        $$ \int_\Omega v \partial_i \phi\,dx = \lim_k \int v_k \partial_i \phi\,dx = - \lim_k \int_\Omega \partial_i v_k \phi = 0. $$
        This establishes that $\grad v=0$, which implies that $v$ is constant (it is not trivial to check that null weak derivatives implies being a constant), and the null average condition yields that $v=0$. This contradicts the initial hypothesis. 
    \end{proof}
\end{lemma}

Probably the most important consequence of this is that the semi-norm given by $| x | \coloneqq \|\grad x\|_{0,\Omega}$, sometimes referred to as the $H_0^1$ semi-norm, is equivalent to the $H^1$ norm in the following cases: (i) homogeneous Dirichlet boundary conditions, (ii) homogeneous Neumann boundary conditions, and (iii) mixed homogeneous Dirichlet and Neumann boundary conditions. Verifying this is a simple but fundamental exercise. A fundamental property to be verified is the following: a bilinear form $a(\cdot, \cdot)$ defined on a Hilbert space $X$ is said to be elliptic if there exists a constant $\alpha$ such that
        $$ a(x, x) \geq \alpha \| x \|^2_X \qquad \forall x\in X. $$

Before providing a proof, we note that every continuous bilinear form $a:H\times H\to \R$ induces an operator $A:H\to H'$ given by
    $$ (Au)[v] = a(u,v), $$
which one could also write as $Au = a(u, \cdot)$. Naturally, the bilinear form $a$ is bounded if and only if the operator $A$ is bounded. 


\begin{lemma}[Lax-Milgram] Consider a bounded bilinear form $a: H\times H\to \R$ defined on a Hilbert space $H$ that is elliptic with constants $C$ and $\alpha$ respectively, and a linear functional $f$ in $H'$. Then, there exists a unique $u$ in $H$ such that 
    $$ a(u, v) = f(v) \qquad \forall v \in H. $$
This solution is continuous with respect to the data, in the sense that there exists a positive constant $C$ such that 
    $$ \| u\|_H \leq \frac 1 \alpha \| f \|_{H'} .$$
This is typically referred to as the \emph{a priori} estimate. 
\end{lemma}
\begin{proof}
    It will be seen further ahead that this can be easily proved using the inf-sup condition. Still, we present a more elementary proof that uses only the properties of the bilinear form and a fixed point argument. Consider $\rho>0$ and the fixed-point map $T:H\to H$ given by 
    $$ T(u) = u - \rho \mathcal R^{-1}\circ (Au - F), $$
    where it can be seen that $T$ is linear, and $\mathcal R$ is the Riesz map between $H$ and $H'$. Now, we look for $\rho$ such that $T$ is a contraction, which we do simply by hand. Consider thus two functions $u,V$ in $H$, then: 
    \begin{align*}
        \| T(u) - T(v)\|_H &= \|T(u - v) \|_H \\
                           &= (u-v, u-v)_H - 2\rho(u-v, \mathcal R^{-1}\circ A(u-v))_H + \rho^2(\mathcal R^{-1}\circ A(u-v), \mathcal R^{-1}\circ A(u-v))_H \\
                           &= \|u-v\|_H^2 - 2\rho\langle A(u-v), u-v\rangle_{H'\times H} + \rho^2 \| \mathcal R^{-1} \circ A(u-v)\|_H^2.
    \end{align*}
    We bound the second and third terms as follows: 
    $$ \langle A(u-v), u-v\rangle =^\text{by definition of $A$} a(u-v, u-v) \geq^\text{ellipticity} \alpha \| u-v\|_H^2, $$ 
        and 
        $$ \| \mathcal R^{-1} \circ A(u-v) \|_H =^\text{Riesz isometry} \| A(u-v) \|_{H'} =^\text{continuity} C \| u-v \|_H^2. $$
    Plugging this into our previous estimate we get
    $$ \| Tu - Tv \|_H \leq (1 - 2\rho \alpha + \rho^2 C^2)\| u-v \|_H^2 , $$
    which shows that $T$ is a contraction whenever $\rho\in (0,\frac{2\alpha}{C^2})$. Stability follows naturally from the properties of $a$:
    $$ \alpha \| u \|^2 \leq^\text{ellipticity} a(u,u) = F(u) \leq \| F \|_{H'} \|u \|_H, $$
    which shows the desired stability estimate: 
        $$ \| u \|_H \leq \frac{1}{\alpha} \| F \|_{H'}. $$
\end{proof}

\subsection{Examples}
\paragraph{The Poisson problem} Consider $f$ in $H^{-1}(\Omega)$ and $g$ in $H^{1/2}(\Gamma)$ with $\Gamma\coloneqq \partial\Omega$. The Poisson problem in strong form is given as the following boundary value problem: 
    \begin{align*}
        -\Delta u  &= f \qquad \tin\quad\Omega\\
        \gamma_0 u &= g \qquad \ton\quad \Gamma.
    \end{align*}
Note that the strong form must be understood in the distributional sense, i.e. as an equation in $H^{-1}(\Omega)$. To derive the weak formulation, consider a function $v$ in $H_0^1(\Omega)$, then using the boundary conditions we obtain that 
    $$ -\langle \Delta u,v\rangle = (\grad u, \grad v),$$
where $(\cdot, \cdot)$ is the $L^2(\Omega)$ product. Thus the weak formulation reads: Find $u$ in $H_0^1(\Omega)$ such that 
    $$ \int_\Omega \grad u\cdot \grad v\,dx = \langle f, v\rangle \qquad \forall v\in H_0^1(\Omega).$$
This problem can be shown to be well-posed using Lax-Milgram's lemma and the Poincar√© inequality. Small exercise: Extend the proof to the case of non-homogeneous Dirichlet boundary conditions.

In the case of having a boundary condition defined only on a portion $\Gamma_D$ of the boundary, the formulation changes, because (i) we need further information regarding the Neumann trace on the complement of the boundary, (ii) the test space looks different. In particular, we define the solution space given by 
    $$ V_0 = \{v\in H^1(\Omega): \quad v = 0 \quad\text{ on $\Gamma_D$}\}, $$
which using the generalized Poincar√© can be shown to still satisfy an ellipticity estimate. 

\paragraph{The pure Neumann problem} In general, having Neumann boundary conditions is problematic for two reasons: It results in a \emph{data compatibility} condition and (ii) it results in having a non-trivial kernel in the problem. The problem in general reads: Find $u$ in $H^1(\Omega)$ such that
    $$ \begin{aligned}
        -\Delta u &= f \\
        \grad u \cdot \vec n &= h.
       \end{aligned}
    $$
The weak formulation is 
    $$ (\grad u, \grad v) = \langle f, v \rangle \qquad \forall v\in H^1(\Omega),$$
where it is easy to see that if $u$ is a solution, then $u+c$ is also a solution for all $c\in \R$. This means that the problem has a kernel, which is given by the space of constant functions, i.e. $\texttt{span}(\{1\})$. The other problem is that, when one considers a test function in the kernel of the problem, this yields the following: 
    $$ (\grad u, \grad 1) = 0 = \langle f, 1\rangle. $$
This is a compatibility condition on the data, and it shows that having compatible data is \emph{necessary} for having a well-posed formulation. Because of these reasons, one considers a solution (and test) space that is orthogonal to the kernel: 
    $$ V = \{u\in H^1(\Omega): \int_\Omega u \,dx = 0\}, $$
where the null average condition can be seen as 
    $$ \int_\Omega u \,dx = (u, 1)_0 = (u,1)_0 + (\grad u, \grad 1)_0 = (u, 1)_1, $$
and thus the orthogonality is being considered with respect to the natural space $H^1(\Omega)$. With it, the weak formulation is given as: Consider $f$ a compatible function in $H^{-1}(\Omega)$, then find $u$ in $V$ such that
    $$ (\grad u, \grad v) = \langle f, v\rangle \qquad \forall v\in V. $$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Galerkin schemes for elliptic problems}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Instead of discretizing the differential operator, as one would do in the case of finite differences to obtain discrete derivatives, one can consider discrete functional spaces, with the idea that the discrete space somehow converges to the continuous space. This is known as a Galerkin scheme.  
\subsection{Galerkin schemes}
Consider thus an abstract differential problem given by finding $u$ in $V$ such that
    $$ a(u, v) = L(v) \qquad \forall v \in V, $$
such that the hypotheses of Lax-Milgram hold. In this case, we can consider a discrete space $V_h$ that approximates $V$, and define a Galerkin scheme as the following discrete problem: find $u_h \in V_h$ such that 
    \begin{equation*}\label{eq:galerkinscheme}
    a(u_h, v_h) = L(v_h) \qquad \forall v_h \in V_h.
    \end{equation*}
The most notable aspect of Lax-Milgram is that all of its hypotheses hold also in $V_h$, which implies that the Galerkin scheme is also invertible, and the \emph{a priori} estimate holds as well. The natural question is whether the discrete solution $u_h$ converges to the continuous solution $u$, which is studied through the \emph{error equation}. This is computed by setting the continuous test function $V$ as $V_h$ and then subtracting both problems: 
    $$ a(e_h, v_h) = 0 \qquad \forall v_h \in V_h, $$
where $e_h = u - u_h$.  This property is known as the \emph{Galerkin orthogonality}, and it can be used to compute the error estimate by considering an arbitrary function $z_h$ in $V_h$:
    $$ \begin{aligned}
        \alpha \| e_h \|_V^2 &\leq a(e_h, e_h) && \\ 
                             &= a(e_h, u - z_h) && \text{(Galerkin orth.)} \\
                             &\leq C\|e_h\|_V \|u - z_h\|_V &&\text{($a$ continuous)},
        \end{aligned} $$
where one obtains that for all $z_h$ it holds that
    $$ \| e_h \|_V \leq \frac C \alpha \|u - z_h\|_V. $$
Taking the infimum over $z_h$ one obtains the celebrated \emph{Ce√† estimate}: 
    $$ \| u - u_h \|_V \leq \frac C \alpha \inf_{v_h\in V_h} \|u - v_h\|_{V_h}. $$
This inequality can reveal many things. For example, if the number $C/\alpha$ is very big, it can hint on a very wide gap between the optimal solution (i.e. the projection) and the discrete one computed from the space $V_h$. A more precise characterization of the approximation properties of a space can be given by the Kolmogorov width, which has been studied in \cite{evans2009n}.

\subsection{Inverse inequalities}
All of the discrete spaces considered are finite dimensional, which means that all their norms are equivalent. Of course, these relationships don't hold in the continuous setting, so there is bound to be some dependence of these constants on $h$. The great thing about inverse inequalities is that in the discrete settings we can sometimes get away with operations that would be otherwise not feasible, but using adequate bounds can yield convergence anyway. We provide a general result first and then show some important consequences. For details, see \cite{ern2004theory}. We will require the following definition: we say that a family of affine meshes in $\R^d$ is \emph{shape regular} if there exists $\sigma_0$ such that 
    $$ \forall h: \sigma_K\coloneqq \frac{h_K}{\rho_K} \leq \sigma_0 \quad\forall K\in \T_h, $$
where $\rho_K$ is the diameter of the largest ball that can be inscribed in $K$, and $h_K$ is the diameter of $K$. It is \emph{quasi-uniform} if it is shape-regular and there is some $C$ such that
        $$ \forall h: h_K \geq C h \quad \forall K\in \T_h. $$

    \begin{theorem}[Global inverse inequality]
        Consider a finite element $(\hat K, \hat P, \hat \Sigma)$, $l\geq 0$ such that $\hat P\subset W^{l,\infty}(\hat K)$, a family of shape-regular and quasi-uniform meshes $\{\T_h\}_{h>0}$ with $h<1$, and set
            $$ W_h = \{v_h: v_h\circ T_K \in \hat P \qquad\forall K\in \T_h \}, $$
        where $T_K$ is the affine mapping from an element $K$ to the reference element $\hat K$; i.e. the family of discrete functions that locally belong to the finite element considered. Then, there is some positive $C$ such that for all $v_h$ in $W_h$ and $m\in [0,l]$ it holds that
            $$ \left(\sum_K \| v_h\|^p_{W^{l,p}(K)}\right)^{1/p} \leq Ch^{m-l+\min(0, d/p - d/q)} \left(\sum_K \|v_h\|_{W^{m,q}(K)}^q\right)^{1/q}. $$
    \end{theorem}
In particular this shows that
    $$ \| v_h \|_{W^{1,p}} \leq C h^{-1} \| v_h \|_{L^p}. $$
One can also obtain the following local estimate in 2D for simplices \cite{warburton2003constants}:
    $$ \| \vec v_h \|_{0,F} \leq C h_K^{-1/2} \| \vec v_h \|_{0,K}, $$
where $F$ is a facet (line or triangle) and $K$ is the element (triangle or tetrahedron).

\subsection{Conforming and non-conforming schemes} 
Galerkin schemes are based on discrete approximation spaces, which may or may not be contained in the underlying function space.
\begin{definition}[Conforming scheme]
    Let $V$ be an infinite-dimensional space, such as a Sobolev space, and $V_h$ a finite-dimensional approximation space associated with a discrete scheme. We say that this scheme is \textit{conforming} if $V_h\subset V$, and \textit{non-conforming} if $V_h\not\subset V$. 
\end{definition}
% \begin{lemma}[Conforming spaces]
%     Consider two non-overlapping Lipschitz domains $K_1$ and $K_2$ such that they meet at a common surface $\Sigma$. 
%     \begin{itemize}
%         \item Consider two scalar functions $p_1$ in $H^1(K_1)$ and $p_2$ in $H^1(K_2)$, and glue them as $p = p_1 I_{K_1} + p_2 I_{K_2}$. If $p_1|_\Sigma = p_2|_\Sigma$, then $p$ belongs to $H^1(K_1\cup K_2\cup \Sigma)$. 
%         \item Consider two vector functions $\vec u_1$ in $H(\dive;K_1)$ and $\vec u_2$ in $H(\dive; K_2)$, and glue them as $\vec u = \vec u_1 I_{K_1} + \vec u_2 I_{K_2}$. Then, if $\vec u_1\cdot \vec n= \vec u_2\cdot \vec n$ it holds that $\vec u$ belongs to $H(\dive; K_1\cup K_2 \cup \Sigma)$. 
%         \item Consider two vector functions $\vec u_1$ in $H(\curl;K_1)$ and $\vec u_2$ in $H(\curl; K_2)$, and glue them as $\vec u = \vec u_1 I_{K_1} + \vec u_2 I_{K_2}$. Then, if $\vec u_1\times \vec n= \vec u_2\times \vec n$ it holds that $\vec u$ belongs to $H(\curl; K_1\cup K_2 \cup \Sigma)$. 
%     \end{itemize}
%     \begin{proof}
%         Point (1) is proved in \cite{gatica2014simple}, (2) is in \cite{monk2003finite}, and (3) is homework :) . 
%     \end{proof}
% \end{lemma}

In order to have a good approximation, it is not necessary that the scheme is conforming. Some examples of conforming schemes are most finite element methods and spectral element methods, and among non-conforming schemes are discontinuous-Galerkin (DG) finite element methods, and methods that impose boundary conditions weakly. We will study several conforming finite element methods in Section~\ref{sec:fem}. 

Although conforming spaces have some nice properties, there exist some applications where the mesh and the domain boundaries may not match, or where traditional finite elements may not apply, forcing one to use other schemes such as spline-based methods, where the degrees of freedom are control points of the splines, rather than actual nodes. In these cases, we can use a non-conforming scheme. The following presentation is based on \cite{Chouly2024}. Consider the Poisson problem with a nonhomogeneous Dirichlet boundary condition: find $u:\Omega\to \mathbb{R}$ such that
$$
\begin{aligned}
    -\Delta u &= f & \tin \Omega \\
    \gamma_0 u &= g & \ton \partial\Omega,
\end{aligned}
$$
which we rewrite in weak form as follows: find $u\in H^1(\Omega)$ such that $u|_{\partial\Omega} = g$ and
$$a(u,v) = (f,v) \qquad \forall v\in H_0^1(\Omega),$$
where as usual we denote $a(u,v)=(\nabla u,\nabla v)$. Let $K^h$ be a discretization of $\Omega$ with mesh size $h$, which we assume is sufficiently regular. There exist several ways to enforce the Dirichlet boundary condition, such as the penalty method and the Nitsche method. We outline both methods below.
\begin{itemize}
    \item The penalty method: at the continuous level, this method can be formulated as follows: find $u^\varepsilon\in H^1(\Omega)$ such that 
$$a(u^\varepsilon, v) + \frac{1}{\varepsilon} (u^\varepsilon, v)_{\partial\Omega} = (f,v) + \frac{1}{\varepsilon} (g,v)_{\partial\Omega} \qquad \forall v\in H^1(\Omega),$$
where we introduced the penalty parameter $\varepsilon>0$. When going back to the strong form, we verify that $u^\varepsilon$ satisfies the Poisson equation $-\Delta u^\varepsilon = f$ and the Robin boundary condition 
$$\nabla u^\varepsilon \cdot\vec n = -\frac{1}{\varepsilon}(u^\varepsilon - g) \implies \varepsilon (\nabla u^\varepsilon) \cdot \vec n = -(u^\varepsilon - g),$$
which for $\varepsilon$ small enough approximates the nonhomogeneous Dirichlet boundary condition. By the Friedrich inequality, we can show that the bilinear form in the left hand side is elliptic on $H^1(\Omega)$ and thus the problem is well-posed by the Lax-Milgram lemma. In a discrete setting, we consider $\varepsilon = \varepsilon_0 h^\lambda$ for some $\varepsilon_0 > 0$ and $\lambda\geq 0$, both independent of the mesh, and we can prove that this discrete problem is well-posed and convergent. Often, the user has to manually tune the values of $\varepsilon_0$ and $\lambda$ to achieve good convergence rates. The critical choice lies in the value of $\varepsilon_0$: if the value is too small, the conditioning of the global stiffness matrix deteriorates, since its conditioning is $\mathcal{O}(\varepsilon_0^{-1}h^{-1-\lambda})$, and if the value is too large, the Dirichlet condition is approximated poorly. 

\item The Nitsche method: let $\gamma > 0$ be a positive function on $\partial\Omega$ and $\theta\in\mathbb{R}$ a fixed parameter. Integrating by parts the weak form of the Poisson problem we first get 
$$a(u,v) - (\nabla u\cdot \vec n, v)_{\partial\Omega} = (f,v),$$
and from the Dirichlet condition we can write 
$$(u,\gamma v -\theta\nabla v\cdot \vec n)_{\partial\Omega} = (g,\gamma v - \theta \nabla v \cdot \vec n)_{\partial\Omega}.$$
Adding these two equations together and rearranging, we get 
$$a(u,v) - (\nabla u \cdot \vec n, v)_{\partial\Omega} - \theta (u,\nabla v\cdot  \vec n) + (u,\gamma v)_{\partial\Omega} = (f,v) + (g,\gamma v - \theta \nabla v\cdot n)_{\partial\Omega}.$$
Let $\zeta$ denote a piecewise constant function on the boundary, that is defined locally by the value of the diameter of every boundary facet. Taking $\gamma = \gamma_0 \zeta^{-1}$ for some $\gamma_0>0$, and recalling the trace inequality 
$$\|\nabla v_h\cdot \vec n\|^2_{-1/2,\partial\Omega} \leq c_T \|\nabla v_h\|^2_{0,\Omega},$$
we can prove that this problem is well-posed provided that 
$$\frac{(1+\theta)^2 c_T}{\gamma_0}\leq 1.$$
Moreover, this method is convergent in the $H^1$ norm for large enough $\gamma_0$. In the case that we expect more regularity, for $u\in H^s(\Omega)$ with $3/2<s<1+k$ (where $k$ is the degree of the polynomial approximation space), we get
$$\|u-u_h\| + \|\nabla u\cdot \vec n - \nabla u_h\cdot \vec n\|_{-1/2,\partial\Omega} \leq Ch^s\|u\|_{s,\Omega}.$$
Remarkably, and in contrast to the penalty method, the constant $C>0$ does not depend on $\gamma_0$ provided that it is large enough, but does depend on the regularity of the mesh and on the polynomial order $k$. As expected, the value of $\gamma_0$ influences the condition number of the global stiffness matrix associated to the left hand side of this problem, and thus it must not be taken too large, but the impact of the value of $\gamma_0$ on the approximation of the Dirichlet boundary condition is much smaller than in the penalty method. 
\end{itemize}

In practice, the penalty method is much simpler to understand and to implement, but its accuracy in some specific problems may not always be satisfactory. The Nitsche method is still simple to implement, and it constitutes a better alternative to the penalty method, where one has to tune only one numerical parameter. There exist more variants to these methods, such as the penalty-free Nitsche method and methods with Lagrange multipliers. The interested reader is referred to \cite{Chouly2024} for more details.


\section{Finite elements}\label{sec:fem}
The idea of studying specifically finite elements will be that of having more significative ways of expressing the projection error present in Ce√†'s estimate: 
    $$ \inf_{v_h\in V_h} \| u - v_h\|_V. $$
A typical use of this inequality will be that of bounding the projection error through a finite element interpolant, such that one gets inequalities such as
    $$ \inf_{v_h\in V_h} \| u - v_h\|_V \leq \| u - I_h u \|_V \leq h^s \|u \|_W, $$
    where $W$ is some space of higher regularity, $s$ is some (hopefully positive) exponent and $I_h:V \to V_h$ is an interpolation operator. Our idea is that of producing FEM spaces for all of our relevant Hilbert spaces: $L^2$, $H^1$, $H(\dive)$, and $H(\curl)$. We will start by introducing first order finite elements in $\R$ by following an almost verbatim copy of \cite[Chapter 1]{ern2004theory}. 

\subsection{One-dimensional interpolation} 

To introduce approximation spaces for finite elements, we need to introduce suitable approximation techniques. A straightforward choice for approximation is polynomial interpolations, which we will begin to study in the one-dimensional case. For an integer $k \ge 0$, $\mathbb{P}_k$ denotes the space of the polynomials in one variable, of degree at most $k$, and with real coefficients.

\subsubsection{The mesh}
Let $\Omega = ]a, b[$ be a one-dimensional domain. We define a mesh on $\Omega$ as an indexed collection of intervals with non-zero measure $\{I_i = [x_{1,i}, x_{2,i}]\}_{0 \leq i \leq N}$ forming a partition of $\Omega$, i.e.
$$ \bigcup_{i=0}^N I_i = \bar{\Omega} $$
and $I_i^\circ \cap I_j^\circ = \emptyset$ for $i \neq j$.
The simplest way to construct a mesh is to take $(N+2)$ points in $\bar{\Omega}$ such that
$$ a = x_0 < x_1 < \dots < x_N < x_{N+1} = b, $$
and to set $x_{1,i} = x_i$ and $x_{2,i} = x_{i+1}$ for $0 \le i \le N$. The points in the set $\{x_0, \dots, x_{N+1}\}$ are called the vertices of the mesh. The mesh may have a variable step size
$$ h_i = x_{i+1} - x_i, \quad 0 \leq i \le N, $$
and we define the mesh size $h$ as
$$ h = \max_{0 \le i \le N} h_i.$$
In what follows, the intervals $I_i$ are also called elements (or cells) and the mesh is denoted by $\mathcal{T}_h = \{I_i\}_{0 \le i \le N}$, where the subscript $h$ refers to the refinement level given by the mesh size.

\subsubsection{The $\mathbb{P}_1$ Lagrange finite element}
One of the simplest methods to approximate functions over $\Omega$ is to define piecewise-linear functions. We denote the vector space of continuous, piecewise-linear functions
$$ P_h^1 = \{ v_h \in C(\bar{\Omega}): \forall i \in \{0, \dots, N\}, v_h|_{I_i} \in \mathbb{P}_1 \}.$$
This space can be used in conjunction with Galerkin methods to approximate one-dimensional PDEs, and for this reason, $P_h^1$ is called an approximation space.
We introduce the functions $\{\varphi_0, \dots, \varphi_{N+1}\}$ defined elementwise as follows: for $i \in \{0, \dots, N+1\}$, set
$$ \varphi_i(x) = \begin{cases} \frac{x - x_{i-1}}{h_{i-1}} & \text{if } x \in I_{i-1}, \\ \frac{x_{i+1} - x}{h_i} & \text{if } x \in I_i, \\ 0 & \text{otherwise}, \end{cases} $$
with obvious modifications if $i = 0$ or $N+1$. Clearly, $\varphi_i \in P_h^1$. These functions are often called ''hat functions'' in reference to the shape of their graph.
\begin{lemma} \label{hatbasis}
    The set $\{\varphi_0, \dots, \varphi_{N+1}\}$ is a basis for $P_h^1$.
    \begin{proof}
        The proof relies on the fact that $\varphi_i(x_j) = \delta_{ij}$ where $\delta$ is the Kronecker delta, for $0 \le i,j \le N+1$. Let $(a_0, \dots, a_{N+1})^\top \in \mathbb{R}^{N+2}$ and assume that the continuous function $w = \sum_{i=0}^{N+1} a_i \varphi_i$ vanishes identically in $\bar{\Omega}$. Then, for $0 \le i \le N+1$, $a_i = w(x_i) = 0$; hence, the set $\{\varphi_0, \dots, \varphi_{N+1}\}$ is linearly independent. Furthermore, for all $v_h \in P_h^1$, it is clear that $v_h = \sum_{i=0}^{N+1} v_h(x_i) \varphi_i$ since, on each element $I_i$, the functions $v_h$ and $\sum_{i=0}^{N+1} v_h(x_i) \varphi_i$ are affine and coincide at two points, namely $x_i$ and $x_{i+1}$.
    \end{proof}
\end{lemma}
\begin{definition}
    Choose a basis $\{\gamma_0, \dots, \gamma_{N+1}\}$ for $\mathcal{L}(P_h^1;\mathbb{R})$; henceforth, the linear forms in this basis are called the global degrees of freedom in $P_h^1$. The functions in the dual basis are called the global shape functions in $P_h^1$. 
\end{definition}
For $i \in \{0, \dots, N+1\}$, choose the linear form

$$ \gamma_i: C(\bar{\Omega}) \ni v \mapsto \gamma_i(v) = v(x_i) \in \mathbb{R}.$$
The proof of Lemma~\ref{hatbasis} shows that a function $v_h \in P_h^1$ is uniquely defined by the $(N+2)$-tuple $(v_h(x_i))_{0 \le i \le N+1}$, i.e. the values at the nodes. In other words, $\{\gamma_0, \dots, \gamma_{N+1}\}$ is a basis for $\mathcal{L}(P_h^1; \mathbb{R})$. Choosing the linear forms $\{\gamma_i\}_i$ defined above as the global degrees of freedom in $P_h^1$, the global shape functions are precisely the hat functions $\{\varphi_0, \dots, \varphi_{N+1}\}$, since $\gamma_i(\varphi_j) = \delta_{ij}$, $0 \le i,j \le N+1$.


In this space, we define the interpolation operator
$$ I_h^1: C(\bar{\Omega}) \ni v \mapsto \sum_{i=0}^{N+1} \gamma_i(v) \varphi_i \in P_h^1.$$
For a function $v \in C^0(\bar{\Omega})$, $I_h^1 v$ is the unique continuous, piecewise linear function that takes the same value as $v$ at all the mesh vertices. The function $I_h^1 v$ is called the Lagrange interpolant of $v$ of degree 1. Note that the approximation space $P_h^1$ is the codomain of $I_h^1$. 

Now we connect the polynomial approximation space we just defined with the Sobolev space $H^1(\Omega)$, to ensure that our method is conforming. 
\begin{lemma}\label{approxP1}
    It holds that $P_h^1 \subset H^1(\Omega)$.
    \begin{proof}
    Let $v_h \in P_h^1$. Clearly, $v_h \in L^2(\Omega)$. Furthermore, owing to the continuity of $v_h$, its first-order distributional derivative is the piecewise constant function $w_h$ such that
    $$\forall I_i \in \mathcal{T}_h,\quad  w_h|_{I_i} = \frac{v_h(x_{i+1}) - v_h(x_i)}{h_i}.$$
    It is clear that $w_h\in L^2(\Omega)$, and thus by definition we conclude $v_h\in H^1(\Omega)$.
    \end{proof}
\end{lemma}
We can characterize the properties of the interpolation operator. 
\begin{lemma}\label{cont1D}
    $I_h^1$ is a linear continuous mapping from $H^1(\Omega)$ to $H^1(\Omega)$, and $\|I_h^1\|_{\mathcal{L}(H^1(\Omega);H^1(\Omega))}$ is uniformly bounded with respect to $h$.
    \begin{proof}
    For the first part, in one dimension, a function in $H^1(\Omega)$ is continuous. Indeed, for $v \in H^1(\Omega)$ and $x,y \in \overline{\Omega}$,
    \begin{equation}\label{eq:interpolator_continuity_1d}
        |v(y) - v(x)| \le \int_x^y |v'(s)| ds  \le |y-x|^{1/2} |v|_{1,\Omega},
    \end{equation}
    owing to the Cauchy-Schwarz inequality, which can be justified rigorously by a density argument. Furthermore, taking $x$ to be a point where $|v|$ reaches its minimum over $\overline{\Omega}$, the above inequality implies
    \begin{equation*}\label{eq:supnorm_p1}
        \|v\|_{L^\infty(\Omega)} \le |b-a|^{-1/2} \|v\|_0 + |b-a|^{1/2}|v|_{1,\Omega},
    \end{equation*}
    since $|v(x)| \le |b-a|^{-1/2} \|v\|_{0}$. Therefore, $I_h^1 v$ is well-defined for $v \in H^1(\Omega)$. Moreover, Lemma~\ref{approxP1} implies $I_h^1 v \in H^1(\Omega)$; hence, $I_h^1$ maps $H^1(\Omega)$ to $H^1(\Omega)$.

    For the second part, let $I_i \in \mathcal{T}_h$ for $0 \le i \le N$. From the definition of the distributional derivative $w_h$ above, $(I_h^1 v)'|_{I_i} = h_i^{-1}(v(x_{i+1}) - v(x_i))$; hence, using the previous inequality we get the estimate $|I_h^1 v|_{1,I_i} \le |v|_{1,I_i}$. Therefore, $|I_h^1 v|_{1,\Omega} \le |v|_{1,\Omega}$. Moreover, since $\|I_h^1 v\|_{0,\Omega} \le |b-a|^{1/2} \|I_h^1 v\|_{L^\infty(\Omega)}$ and $\|I_h^1 v\|_{L^\infty(\Omega)} \le \|v\|_{L^\infty(\Omega)}$, we deduce that $\|I_h^1 v\|_{0,\Omega} \le c \|v\|_{1,\Omega}$ where $c$ is independent of $h$ (assuming $h$ bounded). The conclusion follows readily.
    \end{proof}
\end{lemma}
We can explicitly control the interpolation error via the mesh size.
\begin{lemma}
    For all $h$ and $v \in H^2(\Omega)$,
    $$ \|v - I_h^1 v\|_{0,\Omega} \le h^2 \|v''\|_{0,\Omega},\qquad  |v - I_h^1 v|_{1,\Omega} \le h \|v''\|_{0,\Omega}. $$
\begin{proof}

    Consider an interval $I_i \in \mathcal{T}_h$. Let $w \in H^1(I_i)$ be such that $w$ vanishes at some point $\xi$ in $I_i$. Then, owing to inequality~\ref{eq:interpolator_continuity_1d}, we infer $\|w\|_{0,I_i} \le h_i |w|_{1,I_i}$.
    Let $v \in H^2(\Omega)$, let $i \in \{0, \dots, N\}$, and set $w_i = (v - Iautomatically_h^1 v)'|_{I_i}$. Note that $w_i \in H^1(I_i)$ and that $w_i$ vanishes at some point $\xi$ in $I_i$ owing to the mean-value theorem. Applying the estimate derived above to $w_i$ and using the fact that $(I_h^1 v)''$ vanishes identically on $I_i$ yields $|v - I_h^1 v|_{1,I_i} \le h_i |v''|_{0,I_i}$. The second estimate is then obtained by summing over the mesh intervals. To prove the first estimate, observe that the result above can also be applied to $(v - I_h^1 v)|_{I_i}$ yielding
    $$ \|v - I_h^1 v\|_{0,I_i} \le h_i |v - I_h^1 v|_{1,I_i} \le h_i^2 |v''|_{0,I_i}. $$
    Conclude by summing over the mesh intervals.
\end{proof}
\end{lemma}
We remark here that the bound on the interpolation error involves second-order derivatives of $v$. This is reasonable since the larger the second derivative, the more the graph of $v$ deviates from the piecewise linear interpolant. Also, if the function to be interpolated is in $H^1(\Omega)$ only, one can prove the following results: for all $h$,
\begin{align*}
    \|v - I_h^1 v\|_{0,\Omega} &\le h |v|_{1,\Omega}\\
    \lim_{h \rightarrow 0} |v - I_h^1 v|_{1,\Omega} &= 0.
\end{align*}
The proof of the previous lemma shows that the operator $I_h^1$ is endowed with local interpolation properties, i.e., the interpolation error is controlled elementwise before being controlled globally over $\Omega$. This motivates the introduction of local interpolation operators. Let $I_i = [x_i, x_{i+1}] \in \mathcal{T}_h$ and let $\Sigma_i = \{\sigma_{i,0}, \sigma_{i,1}\}$ where $\sigma_{i,0}, \sigma_{i,1} \in \mathcal{L}(\mathbb{P}_1; \mathbb{R})$ are such that, for all $p \in \mathbb{P}_1$,
$$ \sigma_{i,0}(p) = p(x_i),\qquad \sigma_{i,1}(p) = p(x_{i+1}). $$
Note that $\Sigma_i$ is a basis for $\mathcal{L}(\mathbb{P}_1; \mathbb{R})$. The triplet $\{I_i, \mathbb{P}_1, \Sigma_i\}$ is called a (one-dimensional) $\mathbb{P}_1$ Lagrange finite element, and the linear forms $\{\sigma_{i,0}, \sigma_{i,1}\}$ are the corresponding local degrees of freedom. The functions $\{\theta_{i,0}, \theta_{i,1}\}$ in the dual basis of $\Sigma_i$ (i.e., $\sigma_{i,m}(\theta_{i,n}) = \delta_{mn}$ for $0 \le m, n \le 1$) are called the local shape functions. One readily verifies that
$$ \theta_{i,0}(t) = 1 - \frac{t - x_i}{h_i},\qquad \theta_{i,1}(t) = \frac{t - x_i}{h_i}. $$
Finally, introduce the family $\{\mathcal{I}_{I_i}^1\}_{I_i \in \mathcal{T}_h}$ of local interpolation operators such that, for $i \in \{0, \dots, N\}$,
$$ \mathcal{I}_{I_i}^1: C(I_i) \ni v \mapsto \sum_{m=0}^1 \sigma_{i,m}(v) \theta_{i,m}. $$
The proofs of the two previous lemmas can now be rewritten using the local interpolation operators $\mathcal{I}_{I_i}^1$. In particular, the key properties are, for $0 \le i \le N$ and $v \in H^2(I_i)$,
$$ \|v - \mathcal{I}_{I_i}^1 v\|_{0,I_i} \le h_i^2 |v|_{2,I_i},\qquad|\mathcal{I}_{I_i}^1 v|_{1,I_i} \le h_i |v|_{2,I_i}. $$

\subsubsection{The $\mathbb{P}_k$ Lagrange finite elements}
The interpolation technique presented for the piecewise-linear case (polynomial degree $1$) generalizes to higher-degree polynomials. Consider the mesh $\mathcal{T}_h = \{I_i\}_{0 \le i \le N}$ introduced previously. Let
$$ P_h^k = \{ v_h \in C(\bar{\Omega}): \forall i \in \{0, \dots, N\}, v_h|_{I_i} \in \mathbb{P}_k \}. $$
To investigate the properties of the approximation space $P_h^k$ and to construct an interpolation operator with codomain $P_h^k$, it is convenient to consider Lagrange polynomials. Recall the following:

\begin{definition}[Lagrange polynomials]\label{def:lagrangepolynomials}
    Let $k \ge 1$ and let $\{s_0, \dots, s_k\}$ be $(k+1)$ distinct numbers. The Lagrange polynomials $\{\mathcal{L}_0^k, \dots, \mathcal{L}_k^k\}$ associated with the nodes $\{s_0, \dots, s_k\}$ are defined to be
    $$ \mathcal{L}_m^k(t) = \frac{\prod_{l \ne m}(t - s_l)}{\prod_{l \ne m}(s_m - s_l)}, \quad 0 \le m \le k. $$
    The Lagrange polynomials satisfy the important property
    $$ \mathcal{L}_m^k(s_l) = \delta_{ml}, \quad 0 \le m,l \le k. $$
\end{definition}
For $i \in \{0, \dots, N\}$ introduce the nodes $\xi_{i,m} = x_i + \frac{m}{k}h_i$, $0 \le m \le k$, in the mesh interval $I_i$. Let $\{\mathcal{L}_{i,0}^k, \dots, \mathcal{L}_{i,k}^k\}$ be the Lagrange polynomials associated with these nodes. For $j \in \{0, \dots, k(N+1)\}$ with $j = ki + m$ and $0 \le m \le k-1$ define the function $\varphi_j$ elementwise as follows: For $1 \le m \le k-1$,
$$ \varphi_{ki+m}(x) = \begin{cases} \mathcal{L}_{i,m}^k(x) & \text{if } x \in I_i, \\ 0 & \text{otherwise}, \end{cases} $$
and for $m=0$,
$$ \varphi_{ki}(x) = \begin{cases} \mathcal{L}_{i-1,k}^k(x) & \text{if } x \in I_{i-1}, \\ \mathcal{L}_{i,0}^k(x) & \text{if } x \in I_i, \\ 0 & \text{otherwise}, \end{cases} $$
with obvious modifications if $i = 0$ or $N+1$.
\begin{lemma}\label{basis_functions_Pk}
    The construction above results in degree $k$ polynomials, i.e. $\varphi_j \in P_h^k$.
\begin{proof}
    Let $j \in \{0, \dots, k(N+1)\}$ with $j = ki + m.$ If $1 \le m \le k-1$, $\varphi_j(x_i) = \varphi_j(x_{i+1}) = 0$; hence, $\varphi_j \in C(\bar{\Omega})$. Moreover, the restrictions of $\varphi_j$ to the mesh intervals are in $\mathbb{P}_k$ by construction. Therefore, $\varphi_j \in P_h^k$. Now, assume $m = 0$ (i.e., $j=ki$) and $0 < i < N+1$. Clearly, $\varphi_{ki}$ is continuous at $x_i$ by construction and $\varphi_{ki}(x_{i-1}) = \varphi_{ki}(x_{i+1}) = 0$; hence, $\varphi_{ki} \in P_h^k$. The cases $i = 0$ and $i = N+1$ are treated similarly.
\end{proof}
\end{lemma}
Introduce the set of nodes $\{a_j\}_{0 \le j \le k(N+1)}$ such that $a_j = \xi_{i,m}$ where $j = ik + m$. For $j \in \{0, \dots, k(N+1)\}$, consider the linear form
$$ \gamma_j: C(\bar{\Omega}) \ni v \mapsto \gamma_j(v) = v(a_j). $$

\begin{lemma}
    $\{\varphi_0, \dots, \varphi_{k(N+1)}\}$ is a basis for $P_h^k$, and $\{\gamma_0, \dots, \gamma_{k(N+1)}\}$ is a basis for $\mathcal{L}(P_h^k; \mathbb{R})$.

\begin{proof}
    Similar to that of Lemma~\ref{hatbasis}, since $\gamma_j(\varphi_{j'}) = \delta_{jj'}$ for $0 \le j,j' \le k(N+1)$.
\end{proof}
\end{lemma}
The global degrees of freedom in $P_h^k$ are chosen to be the $(k(N+1)+1)$ linear forms $\gamma_j$ defined above; hence, the global shape functions in $P_h^k$ are the functions $\{\varphi_0, \dots, \varphi_{k(N+1)}\}$.

The main advantage of using high-degree polynomials is that smooth functions can be interpolated to high-order accuracy. Define the interpolation operator $I_h^k$ to be
$$ I_h^k: C(\bar{\Omega}) \ni v \mapsto \sum_{j=0}^{k(N+1)} \gamma_j(v) \varphi_j \in P_h^k. $$
$I_h^k v$ is called the Lagrange interpolant of $v$ of degree $k$. Clearly, $I_h^k$ is a linear operator, and $I_h^k v$ is the unique function in $P_h^k$ that takes the same value as $v$ at all the mesh nodes. The approximation space $P_h^k$ is the codomain of $I_h^k$. 

\begin{lemma}
    The approximation space $P_h^k$ constructed above satisfies $P_h^k \subset H^1(\Omega)$.
\begin{proof}
    Similar to that of Lemma~\ref{approxP1}.
\end{proof}
\end{lemma}
To investigate the properties of $I_h^k$, it is convenient to introduce a family of local interpolation operators. On $I_i = [x_i, x_{i+1}] \in \mathcal{T}_h$ choose the local degrees of freedom to be the $(k+1)$ linear forms $\{\sigma_{i,0}, \dots, \sigma_{i,k}\}$ defined as follows:
$$ \sigma_{i,m}: \mathbb{P}_k \ni p \mapsto \sigma_{i,m}(p) = p(\xi_{i,m}), \quad 0 \le m \le k. $$
The triplet $\{I_i, \mathbb{P}_k, \Sigma_i\}$ is called a (one-dimensional) $\mathbb{P}_k$ Lagrange finite element, and the points $\{\xi_{i,0}, \dots, \xi_{i,k}\}$ are called the nodes of the finite element. Clearly, the local shape functions $\{\theta_{i,0}, \dots, \theta_{i,k}\}$ are the $(k+1)$ Lagrange polynomials associated with the nodes $\{\xi_{i,0}, \dots, \xi_{i,k}\}$, i.e., $\theta_{i,m} = \mathcal{L}_{i,m}^k$ for $0 \le m \le k$. Finally, introduce the family $\{\mathcal{I}_{I_i}^k\}_{I_i \in \mathcal{T}_h}$ of local interpolation operators such that, for $i \in \{0, \dots, N\}$,
$$ \mathcal{I}_{I_i}^k: C(I_i) \ni v \mapsto \sum_{m=0}^k \sigma_{i,m}(v) \theta_{i,m}, $$
i.e., for all $0 \le i \le N$ and $v \in C(\bar{\Omega})$, $(I_h^k v)|_{I_i} = \mathcal{I}_{I_i}^k(v|_{I_i})$.

Let us show that the family $\{\mathcal{I}_{I_i}^k\}_{I_i \in \mathcal{T}_h}$ can be generated from a single reference interpolation operator. Let $\hat{K} = [0, 1]$ be the unit interval, henceforth referred to as the reference interval. Set $\hat{P} = \mathbb{P}_k,$ and define the $(k+1)$ linear forms $\{\hat{\sigma}_0, \dots, \hat{\sigma}_k\}$ as follows:
$$ \hat{\sigma}_m: \mathbb{P}_k \ni \hat{p} \mapsto \hat{\sigma}_m(\hat{p}) = \hat{p}(\hat{\xi}_m), \quad 0 \le m \le k, $$
where $\hat{\xi}_m = \frac{m}{k}$. Let $\{\hat{\mathcal{L}}_0^k, \dots, \hat{\mathcal{L}}_k^k\}$ be the Lagrange polynomials associated with the nodes $\{\hat{\xi}_0, \dots, \hat{\xi}_k\}$. Set $\hat{\theta}_m = \hat{\mathcal{L}}_m^k$, $0 \le m \le k$, so that $\hat{\sigma}_m(\hat{\theta}_n) = \delta_{mn}$ for $0 \le m,n \le k$. Then, $\{\hat{K}, \hat{P}, \hat{\Sigma}\}$ is a $\mathbb{P}_k$ Lagrange finite element, and the corresponding interpolation operator is
$$ \mathcal{I}_{\hat{K}}^k: C(\hat{K}) \ni \hat{v} \mapsto \sum_{m=0}^k \hat{\sigma}_m(\hat{v}) \hat{\theta}_m. $$
$\{\hat{K}, \hat{P}, \hat{\Sigma}\}$ is called the reference finite element and $\mathcal{I}_{\hat{K}}^k$ the reference interpolation operator. For $i \in \{0, \dots, N\}$, consider the affine transformations
$$ T_i: \hat{K} \ni t \mapsto x = x_i + th_i \in I_i. $$
Since $T_i(\hat{K}) = I_i$, the mesh $\mathcal{T}_h$ can be constructed by applying the affine transformations $T_i$ to the reference interval $\hat{K}$. Moreover, owing to the fact that $T_i(\hat{\xi}_m) = \xi_{i,m}$ for $0 \le m \le k$, it is clear that $\theta_{i,m} \circ T_i = \hat{\theta}_m$ and $\sigma_{i,m}(v) = \hat{\sigma}_m(v \circ T_i)$ for all $v \in C(I_i)$. Hence, using
\begin{align*}
    \mathcal{I}_{I_i}^k(v)(T_i(\hat{x})) &= \sum_{m=0}^k \sigma_{i,m}(v) \theta_{i,m}(T_i(\hat{x})) \\
    &= \sum_{m=0}^k \sigma_{i,m}(v) \hat{\theta}_m(\hat{x}) \\
    &= \sum_{m=0}^k \hat{\sigma}_m(v \circ T_i) \hat{\theta}_m(\hat{x})\\
    & = \mathcal{I}_{\hat{K}}^k(v \circ T_i)(\hat{x}),
\end{align*}
and thus we infer
$$ \forall v \in C(I_i), \quad \mathcal{I}_{I_i}^k(v) \circ T_i = \mathcal{I}_{\hat{K}}^k(v \circ T_i). $$
In other words, the family $\{\mathcal{I}_{I_i}^k\}_{I_i \in \mathcal{T}_h}$ is entirely generated by the transformations $\{T_i\}_{I_i \in \mathcal{T}_h}$ and the reference interpolation operator $\mathcal{I}_{\hat{K}}^k$. The property above plays a key role when estimating the interpolation error; see the proof of Lemma~\ref{interpolationerrPk} below.

As well as with the one-dimensional case, we can characterize the interpolation operators.
\begin{lemma}
    $I_h^k$ is a linear continuous mapping from $H^1(\Omega)$ to $H^1(\Omega)$, and $\|I_h^k\|_{\mathcal{L}(H^1(\Omega);H^1(\Omega))}$ is uniformly bounded with respect to $h$.
\begin{proof}
    To prove that $I_h^k$ maps $H^1(\Omega)$ to $H^1(\Omega)$, use the same argument as in the proof of Lemma~\ref{cont1D}.
    Let $v \in H^1(\Omega)$ and $I_i \in \mathcal{T}_h$. Since $\sum_{m=0}^k \theta_{i,m}' = 0$,
    $$ (\mathcal{I}_{I_i}^k v)' = \sum_{m=0}^k [v(\xi_{i,m}) - v(x_i)] \theta_{i,m}'. $$
    Inequality~\ref{eq:interpolator_continuity_1d} yields $|v(\xi_{i,m}) - v(x_i)| \le h_i^{1/2} |v|_{1,I_i}$ for $0 \le m \le k$. Furthermore, changing variables in the integral, it is clear that $|\theta_{i,m}|_{1,I_i} = h_i^{-1/2} |\hat{\theta}_m|_{1,\hat{K}}$. Set $c_k = \max_{0 \le m \le k} |\hat{\theta}_m|_{1,\hat{K}}$ and observe that this quantity is mesh-independent. A straightforward calculation yields
    $$ |\mathcal{I}_{I_i}^k v|_{1,I_i} \le (k+1) c_k |v|_{1,I_i}, $$
    showing that $|\mathcal{I}_h^k v|_{1,\Omega}$ is controlled by $|v|_{1,\Omega}$ uniformly with respect to $h$. In addition, since $\sum_{m=0}^k \theta_{i,m} = 1$,
    $$ \mathcal{I}_{I_i}^k v - v(x_i) = \sum_{m=0}^k [v(\xi_{i,m}) - v(x_i)] \theta_{i,m}, $$
    implying, for $x \in I_i$, $|\mathcal{I}_{I_i}^k v(x)| \le \|v\|_{L^\infty(\Omega)} + (k+1) d_k h_i^{1/2} |v|_{1,I_i}$ with the mesh-independent constant $d_k = \max_{0 \le m \le k} \|\hat{\theta}_m\|_{L^\infty(\hat{K})}$. Then, from inequality~\ref{eq:supnorm_p1} we get that $\|\mathcal{I}_h^k v\|_{L^\infty(\Omega)}$ is controlled by $\|v\|_{1,\Omega}$ uniformly with respect to $h$. To conclude, use the fact that $\|\mathcal{I}_h^k v\|_{0,\Omega} \le |b-a|^{1/2} \|\mathcal{I}_h^k v\|_{L^\infty(\Omega)}$.
\end{proof}
\end{lemma}
\begin{lemma}\label{interpolationerrPk}
    Let $0 \le l \le k$. Then, there exists $c$ such that, for all $h$ and $v \in H^{l+1}(\Omega)$
    $$ \|v - I_h^k v\|_{0,\Omega} + h |v - I_h^k v|_{1,\Omega} \le c h^{l+1} |v|_{l+1,\Omega} $$
    and for $l \ge 1$
    $$ \sum_{m=2}^{l+1} h^m \left(\sum_{i=0}^N |v - I_h^k v|_{m,I_i}^2\right)^{1/2} \le c h^{l+1} |v|_{l+1,\Omega}. $$

\begin{proof}
    Let $0 \le l \le k$, $0 \le m \le l+1$. Let $v \in H^{l+1}(\Omega)$.
    Consider a mesh interval $I_i$. Set $\hat{v} = v \circ T_i$. Then, we can go back to $\hat{K}$ via the transformations $T_i$, i.e. change variables in the integral to obtain
    $$ |v - \mathcal{I}_{I_i}^k v|_{m,I_i} = h_i^{-m+1/2} |\hat{v} - \mathcal{I}_{\hat{K}}^k \hat{v}|_{m,\hat{K}}. $$
    Similarly, $|\hat{v}|_{l+1,\hat{K}} = h_i^{l+1/2} |v|_{l+1,I_i}$. 
    Consider the linear mapping
    $$ \mathcal{F}: H^{l+1}(\hat{K}) \ni \hat{v} \mapsto \hat{v} - \mathcal{I}_{\hat{K}}^k \hat{v} \in H^m(\hat{K}). $$
    Note that $\mathcal{I}_{\hat{K}}^k \hat{v}$ is meaningful since in one dimension, $\hat{v} \in H^{l+1}(\hat{K})$ with $l \ge 0$ implies $\hat{v} \in C(\hat{K})$. Moreover, $\mathcal{F}$ is continuous from $H^{l+1}(\hat{K})$ to $H^m(\hat{K})$. Indeed, one can easily adapt the proof of Lemma~\ref{cont1D} to prove that $\mathcal{I}_{\hat{K}}^k$ is continuous from $H^1(\hat{K})$ to $H^s(\hat{K})$ for all $s \ge 1$. Furthermore, it is clear that $\mathbb{P}_k$ is invariant under $\mathcal{F}$ since, for all $\hat{p} \in \mathbb{P}_k$ with $\hat{p} = \sum_{n=0}^k \alpha_n \hat{\theta}_n$
    $$ \mathcal{I}_{\hat{K}} \hat{p} = \sum_{m,n=0}^k \alpha_n \hat{\sigma}_m(\hat{\theta}_n) \hat{\theta}_m = \sum_{m,n=0}^k \alpha_n \delta_{mn} \hat{\theta}_m = \sum_{n=0}^k \alpha_n \hat{\theta}_n = \hat{p}. $$
    Since $l \le k$, $\mathbb{P}_l$ is invariant under $\mathcal{F}$. As a result,
    \begin{align*}
        |\hat{v} - \mathcal{I}_{\hat{K}}^k \hat{v}|_{m,\hat{K}} &= |\mathcal{F}(\hat{v})|_{m,\hat{K}} = \inf_{\hat{p} \in \mathbb{P}_l} |\mathcal{F}(\hat{v} + \hat{p})|_{m,\hat{K}} \\
        &\le \|\mathcal{F}\|_{\mathcal{L}(H^{l+1}(\hat{K});H^m(\hat{K}))} \inf_{\hat{p} \in \mathbb{P}_l} \|\hat{v} + \hat{p}\|_{l+1,\hat{K}} \\
        &\le c \inf_{\hat{p} \in \mathbb{P}_l} \|\hat{v} + \hat{p}\|_{l+1,\hat{K}} \le c |\hat{v}|_{l+1,\hat{K}}.
    \end{align*}
    the last estimate resulting from the Deny-Lions Lemma; see Lemma B.67 in \cite{ern2004theory}. The identities derived in step 1 yield
    $$ |v - \mathcal{I}_{I_i}^k v|_{m,I_i} = h_i^{-m+1/2} |\hat{v} - \mathcal{I}_{\hat{K}}^k \hat{v}|_{m,\hat{K}} \le h_i^{-m+1/2} c |\hat{v}|_{l+1,\hat{K}} = h_i^{-m+1/2} c h_i^{l+1/2} |v|_{l+1,I_i} = c h_i^{l+1-m} |v|_{l+1,I_i}. $$
    To derive the desired, sum over the mesh intervals. When $m = 0$ or 1, global norms over $\Omega$ can be used since $\mathbb{P}_k \subset H^1(\Omega)$ owing to the fact that $P_h^k\subset H^1(\Omega)$.
\end{proof}
\end{lemma}
Note that the proofs above show that the interpolation properties of $I_h^k$ are local. If the function to be interpolated is smooth enough, say $v \in H^{k+1}(\Omega)$, the interpolation error is of optimal order. In particular, the first error estimate yields
$$ \|v - I_h^k v\|_{0,\Omega} + h |v - I_h^k v|_{1,\Omega} \le c h^{k+1} |v|_{k+1,\Omega}. $$
However, one should bear in mind that the order of the interpolation error may not be optimal if the function to be interpolated is not smooth. For instance, if $v \in H^s(\Omega)$ and $v \notin H^{s+1}(\Omega)$ with $s \ge 2$, considering polynomials of degree larger than $s-1$ does not improve the interpolation error. If the function to be interpolated is in $H^1(\Omega)$ only, one can still prove $\lim_{h \rightarrow 0} |v - I_h^k v|_{1,\Omega} = 0$. To this end, use the density of $H^2(\Omega)$ in $H^1(\Omega)$ and the first error estimate; details are left as an exercise.

\subsection{Finite elements: definitions and examples}
With one-dimensional interpolation already covered, we now present a general definition of finite elements and local interpolation operators. We will see several examples in two and three dimensions. 

\subsubsection{Main definitions}
\begin{definition}[Finite element]\label{def:finiteelements}
    A finite element consists of a triplet $\{K, P, \Sigma\}$, where:
    \begin{enumerate}
        \item $K$ is a compact, connected, Lipschitz subset of $\mathbb{R}^d$ with non-empty interior.
        \item $P$ is a vector space of functions $p: K \rightarrow \mathbb{R}^m$ for some positive integer $m$ (typically $m = 1$ or $d$).
        \item $\Sigma$ is a set of $n_{sh}$ linear forms $\{\sigma_1, \dots, \sigma_{n_{sh}}\}$ acting on the elements of $P$, and such that the linear mapping
        \begin{equation*}
            \label{eq:finite_element_mapping}
            P \ni p \mapsto (\sigma_1(p), \dots, \sigma_{n_{sh}}(p)) \in \mathbb{R}^{n_{sh}}
        \end{equation*}
        is bijective, i.e., $\Sigma$ is a basis for $\mathcal{L}(P; \mathbb{R})$. The linear forms $\{\sigma_1, \dots, \sigma_{n_{sh}}\}$ are called the local degrees of freedom.
    \end{enumerate}
\end{definition}

\begin{lemma}\label{lemma:basis_from_bijectivity}
    There exists a basis $\{\theta_1, \dots, \theta_{n_{sh}}\}$ in $P$ such that
    $$ \sigma_i(\theta_j) = \delta_{ij}, \quad 1 \le i,j \le n_{sh}. $$
    \begin{proof}
        Direct consequence of the bijectivity of the mapping \eqref{eq:finite_element_mapping} above.
    \end{proof}
\end{lemma}


\begin{definition}
    $\{\theta_1, \dots, \theta_{n_{sh}}\}$ are called the local shape functions.
\end{definition}

Note that condition (3) in Definition~\ref{def:finiteelements} amounts to proving that
    $$ \forall(\alpha_1, \dots, \alpha_{n_{sh}}) \in \mathbb{R}^{n_{sh}}, \quad \exists ! p \in P, \quad \sigma_i(p) = \alpha_i \text{ for } 1 \le i \le n_{sh}, $$
    which, in turn, is equivalent to
    $$ \begin{cases} \dim P = |\Sigma| = n_{sh}, \\ \forall p \in P, (\sigma_i(p) = 0, 1 \le i \le n_{sh}) \Rightarrow (p = 0). \end{cases} $$
This property is usually referred to as unisolvence. In the literature, the bijectivity of the mapping \eqref{eq:finite_element_mapping} is sometimes not included in the definition and, if this property holds, the finite element is said to be unisolvent.

\begin{definition}[Lagrange finite element]\label{def:lagrange_finite_element}
    Let $\{K, P, \Sigma\}$ be a finite element. If there is a set of points $\{a_1, \dots, a_{n_{sh}}\}$ in $K$ such that, for all $p \in P$, $\sigma_i(p) = p(a_i)$, $1 \le i \le n_{sh}$, $\{K, P, \Sigma\}$ is called a Lagrange finite element. The points $\{a_1, \dots, a_{n_{sh}}\}$ are called the nodes of the finite element, and the local shape functions $\{\theta_1, \dots, \theta_{n_{sh}}\}$ (which are such that $\theta_i(a_j) = \delta_{ij}$ for $1 \le i,j \le n_{sh}$) are called the nodal basis of $P$.
\end{definition}

\subsubsection{Local interpolation operator}
Let $\{K, P, \Sigma\}$ be a finite element. Assume that there exists a normed vector space $V(K)$ of functions $v: K \rightarrow \mathbb{R}^m$ such that:
\begin{enumerate}
    \item $P \subset V(K)$
    \item The linear forms $\{\sigma_1, \dots, \sigma_{n_{sh}}\}$ can be extended to $V(K)'$
\end{enumerate}
Then, the local interpolation operator $\mathcal{I}_K$ can be defined as follows:
$$ \mathcal{I}_K: V(K) \ni v \mapsto \sum_{i=1}^{n_{sh}} \sigma_i(v) \theta_i \in P. $$
$V(K)$ is the domain of $\mathcal{I}_K$ and $P$ is its codomain. Note that the term ''interpolation'' is used in a broad sense since $\mathcal{I}_K v$ is not necessarily defined by matching point values of $v$.

\begin{lemma}\label{lemma:P_invariant_under_IK}
    $P$ is invariant under $\mathcal{I}_K$, i.e., $\forall p \in P$, $\mathcal{I}_K p = p$.
    \begin{proof}
        Letting $p = \sum_{j=1}^{n_{sh}} \alpha_j \theta_j$ yields $\mathcal{I}_K p = \sum_{i,j=1}^{n_{sh}} \alpha_j \sigma_i(\theta_j) \theta_i = p$.
    \end{proof}
\end{lemma}

\example{
    For Lagrange finite elements, one may choose $V(K) = [C^0(K)]^m$ or $V(K) = [H^s(K)]^m$ with $s > \frac{d}{2}$. The local Lagrange interpolation operator is defined as follows:
    \begin{equation*}\label{eq:lagrange_interp_operator}
        \mathcal{I}_K: V(K) \ni v \mapsto \mathcal{I}_K v = \sum_{i=1}^{n_{sh}} v(a_i) \theta_i,
    \end{equation*}
    i.e., the Lagrange interpolant is constructed by matching the point values at the Lagrange nodes.
}

At this point, it may seem more appropriate to define a finite element as a quadruplet $\{K, P, \Sigma, V(K)\}$ where the triplet $\{K, P, \Sigma\}$ complies with Definition~\ref{def:finiteelements} and $V(K)$ satisfies properties (1)-(2). However, for the sake of simplicity, we hereafter employ the well-established triplet-based definition, and always implicitly assume that there exists a normed vector space $V(K)$ satisfying properties (1)-(2). In many textbooks, $V(K)$ is implicitly assumed to be of the form $C^s(K)$ for some integer $s \ge 0$.
\subsection{Finite elements in higher dimensions}
We now extend the above definitions to the higher-dimensional setting, by introducing simplicial and tensor product finite elements.
\subsubsection{Simplicial Lagrange finite elements}
\paragraph{Simplices and barycentric coordinates} Let $\{a_0, \dots, a_d\}$ be a family of points in $\mathbb{R}^d$, $d \ge 1$. Assume that the vectors $\{a_1 - a_0, \dots, a_d - a_0\}$ are linearly independent. Then, the convex hull of $\{a_0, \dots, a_d\}$ is called a simplex, and the points $\{a_0, \dots, a_d\}$ are called the vertices of the simplex. The unit simplex of $\mathbb{R}^d$ is the set
$$ \{x \in \mathbb{R}^d: x_i \ge 0, 1 \le i \le d, \text{ and } \sum_{i=1}^d x_i \le 1 \}. $$
A simplex can be equivalently defined to be the image of the unit simplex by a bijective affine transformation. For $0 \le i \le d$, define $F_i$ to be the face of $K$ opposite to $a_i$, and define $n_i$ to be the outward normal to $F_i$. Note that in dimension 2 a face is also called an edge, but this distinction will not be made unless necessary.
Given a simplex $K$ in $\mathbb{R}^d$, it is often convenient to consider the associated barycentric coordinates $\{\lambda_0, \dots, \lambda_d\}$ defined as follows: For $0 \le i \le d$,
\begin{equation*}\label{eq:barycentric_coords}
    \lambda_i: \mathbb{R}^d \ni x \mapsto \lambda_i(x) = 1 - \frac{(x - a_i) \cdot n_i}{(a_j - a_i) \cdot n_i} \in \mathbb{R},
\end{equation*}
where $a_j$ is an arbitrary vertex in $F_i$ (the definition of $\lambda_i$ is clearly independent of the choice of the vertex in $F_i$). The barycentric coordinate $\lambda_i$ is an affine function; it is equal to 1 at $a_i$ and vanishes at $F_i$. Furthermore, its level-sets are hyperplanes parallel to $F_i$. Note that the barycenter $G$ of $K$ has barycentric coordinates $(\frac{1}{d+1}, \dots, \frac{1}{d+1})$. The barycentric coordinates satisfy the following properties: For all $x \in K$, $0 \le \lambda_i(x) \le 1$ and for all $x \in \mathbb{R}^d$,
$$ \sum_{i=1}^{d+1} \lambda_i(x) = 1 \text{ and } \sum_{i=1}^{d+1} \lambda_i(x) (x - a_i) = 0. $$
See Exercise 1.4 in \cite{ern2004theory} for further properties in dimension 2 and 3.

\example{In the unit simplex, $\lambda_0 = 1 - x_1 - x_2$, $\lambda_1 = x_1$, and $\lambda_2 = x_2$ in dimension 2, and $\lambda_0 = 1 - x_1 - x_2 - x_3$, $\lambda_1 = x_1$, $\lambda_2 = x_2$, $\lambda_3 = x_3$ in dimension 3.}

\paragraph{The polynomial space $\mathbb{P}_k$} Let $x = (x_1, \dots, x_d)$ and let $\mathbb{P}_k$ be the space of polynomials in the variables $x_1, \dots, x_d$, with real coefficients and of global degree at most $k$,
$$ \mathbb{P}_k = \{ p(x) = \sum_{0 \le i_1 + \dots + i_d \le k} \alpha_{i_1 \dots i_d} x_1^{i_1} \dots x_d^{i_d} : \alpha_{i_1 \dots i_d} \in \mathbb{R} \}. $$
One readily verifies that $\mathbb{P}_k$ is a vector space of dimension
$$ \dim \mathbb{P}_k = \binom{d+k}{k} = \begin{cases} k+1 & \text{if } d=1, \\ \frac{1}{2}(k+1)(k+2) & \text{if } d=2, \\ \frac{1}{6}(k+1)(k+2)(k+3) & \text{if } d=3. \end{cases} $$

\begin{lemma}\label{lemma:simplicial_lagrange_fe}
    Let $K$ be a simplex in $\mathbb{R}^d$. Let $k \ge 1$, let $P = \mathbb{P}_k$, and let $n_{sh} = \dim \mathbb{P}_k$. Consider the set of nodes $\{a_i\}_{1 \le i \le n_{sh}}$ with barycentric coordinates
    $$ \left(\frac{i_0}{k}, \dots, \frac{i_d}{k}\right), \quad 0 \le i_0, \dots, i_d \le k, \quad i_0 + \dots + i_d = k. $$
    Let $\Sigma = \{\sigma_1, \dots, \sigma_{n_{sh}}\}$ be the linear forms such that $\sigma_i(p) = p(a_i)$, $1 \le i \le n_{sh}$. Then, $\{K, P, \Sigma\}$ is a Lagrange finite element.
    \begin{proof}
        See Exercise 1.3 in \cite{ern2004theory}.
    \end{proof}
\end{lemma}

For $k=1$, the $(d+1)$ local shape functions are the barycentric coordinates
$$ \theta_i = \lambda_i, \quad 0 \le i \le d. $$
For $k=2$, the local shape functions are
$$ \begin{cases} \lambda_i(2\lambda_i - 1), & 0 \le i \le d, \\ 4\lambda_i \lambda_j, & 0 \le i < j \le d, \end{cases} $$
and for $k=3$
$$ \begin{cases} \frac{1}{2}\lambda_i(3\lambda_i - 1)(3\lambda_i - 2), & 0 \le i \le d, \\ \frac{9}{2}\lambda_i(3\lambda_i - 1)\lambda_j, & 0 \le i,j \le d, i \ne j, \\ 27\lambda_i \lambda_j \lambda_k, & 0 \le i < j < k \le d. \end{cases} $$

\subsubsection{Tensor product Lagrange finite elements}
\paragraph{Cuboids}
Given a set of $d$ intervals $\{[c_i, d_i]\}_{1 \le i \le d}$, all with non-zero measure, the set $K = \prod_{i=1}^d [c_i, d_i]$ is called a cuboid. For $x \in K$, there exists a unique vector $(t_1, \dots, t_d) \in [0, 1]^d$ such that, for all $1 \le i \le d$, $x_i = c_i + t_i(d_i - c_i)$. The vector $(t_1, \dots, t_d)$ is called the local coordinate vector of $x$ in $K$.

\paragraph{The polynomial space $\mathbb{Q}_k$} Let $\mathbb{Q}_k$ be the polynomial space in the variables $x_1, \dots, x_d$, with real coefficients and of degree at most $k$ in each variable. In dimension 1, $\mathbb{Q}_k = \mathbb{P}_k$; in dimension $d \ge 2$
$$ \mathbb{Q}_k = \{ q(x) = \sum_{0 \le i_1, \dots, i_d \le k} \alpha_{i_1 \dots i_d} x_1^{i_1} \dots x_d^{i_d} : \alpha_{i_1 \dots i_d} \in \mathbb{R} \}. $$
One readily verifies that $\mathbb{Q}_k$ is a vector space of dimension
$$ \dim \mathbb{Q}_k = (k+1)^d = \begin{cases} (k+1)^2 & \text{if } d=2, \\ (k+1)^3 & \text{if } d=3. \end{cases} $$
Note the inclusions $\mathbb{P}_k \subset \mathbb{Q}_k \subset \mathbb{P}_{kd}$.

\begin{lemma}\label{lemma:tensor_product_lagrange_fe}
    Let $K$ be a cuboid in $\mathbb{R}^d$. Let $k \ge 1$, let $P = \mathbb{Q}_k$, and let $n_{sh} = \dim \mathbb{Q}_k$. Consider the set of nodes $\{a_i\}_{1 \le i \le n_{sh}}$ with local coordinates
    $$ \left(\frac{i_1}{k}, \dots, \frac{i_d}{k}\right), \quad 0 \le i_1, \dots, i_d \le k. $$
    Let $\Sigma = \{\sigma_1, \dots, \sigma_{n_{sh}}\}$ be the linear forms such that $\sigma_i(p) = p(a_i)$, $1 \le i \le n_{sh}$. Then, $\{K, P, \Sigma\}$ is a Lagrange finite element.
\end{lemma}
For $1 \le i \le d$, set $\xi_{i,l} = c_i + \frac{l}{k}(d_i - c_i)$, $0 \le l \le k$, and let $\{\mathcal{L}_{i,0}^k, \dots, \mathcal{L}_{i,k}^k\}$ be the Lagrange polynomials in the variable $x_i$ associated with the nodes $\{\xi_{i,0}, \dots, \xi_{i,k}\}$; see Definition~\ref{def:lagrangepolynomials}. Then, the local shape functions are
$$ \theta_{i_1 \dots i_d}(x) = \mathcal{L}_{1,i_1}^k(x_1) \dots \mathcal{L}_{d,i_d}^k(x_d), \quad 0 \le i_1, \dots, i_d \le k. $$

\subsection{Finite elements in $H(\dive)$ and $H(\curl)$}
\paragraph{$H(\dive)$:} Here we consider the local polynomial in $\R^d$ given by
    $$ D_k = [\P_{k-1}]^d \oplus \widetilde{\P_{k-1}} \vec x, $$
where $\vec x$ is the identity map, and $\widetilde{\P_k}$ is the set of homogeneous\footnote{A homogeneous polynomial is one whose non-zero terms all have the same degree, such as $x^2 + xy$.} polynomials of degree exactly $k$. If $k=1$, this space will have $d+1$ degrees of freedom, characterized in a triangle/tetrahedron by degrees of freedom defined through the normal component of the polynomial on its facets. These elements are known as Raviart-Thomas elements. Thus, given a triangulation of the geometry $\T_h$, this allows us to define the space
    $$ W_h = \left\{ \vec u_h \in H(\dive; \Omega):  \vec u_h|_K \in D_k \qquad\forall K \in \T_h \right\}, $$
where $W_h$ is conforming in $H(\dive)$. In addition, we get the following result regarding an interpolation operator: 
    \begin{theorem}[$H(\dive)$ interpolation]
        Consider $0<\delta<1/2$ and $s\in [1/2+\delta, k]$. Then, if $\vec u$ belongs to $\vec H^s(\Omega)$, there exists an interpolation operator $\vec w_h$ and a positive constant $C$ such that
            $$ \|\vec u - \vec w_h \vec u\|_0 \leq C h^s \| \vec u \|_{\vec H^s(\Omega)} $$
        and 
            $$ \|\dive\left(\vec u - \vec w_h \vec u\right)\|_0 \leq C h^s \| \dive \vec u \|_{H^s(\Omega)}. $$
    \end{theorem}
Naturally, this results implies the estimate in the $H(\dive)$ norm. 
\paragraph{$H(\curl)$:} The procedure for this space is roughly similar to the previous one. For it, we define the space
    $$ S_k = \{ \vec p \in [\widetilde{\P_k}]^3 | \vec p \cdot \vec x = 0 \}, $$
which is somehow the orthogonal complement to the space $\widetilde{\P_{k-1}}\vec x$. With it, we define the local space as 
    $$ R_k = [\P_{k-1}]^3\oplus S_k, $$
and given a triangulation $\T_h$ of the geometry, we consider the space
    $$ V_h = \left\{ \vec u_h \in H(\curl;\Omega): \vec u_h|_K \in R_k\qquad \forall K\in \T_h\right\}. $$
This space is conforming in $H(\curl)$. In addition, we also get some nice interpolation properties. 
    \begin{theorem}[$H(\curl)$ interpolation] The theorem states: 
        \begin{itemize}
            \item Consider $\delta>0$ and $s$ in $[1/2+\delta, k]$. Then, if $\vec u$ is in $\vec H^s(\Omega)$, there exists a positive constant $C$ and an interpolation operator $\vec r_h$ such that 
                $$ \| \vec u - \vec r_h \vec u\|_0 + \| \curl(\vec u - \vec r_h \vec u) \|_0 \leq C h^s \left(\| \vec u\|_{H^s} + \| \curl \vec u \|_{H^s}\right) . $$
            \item Consider $0<\delta\leq 1/2$. If $\vec u$ belongs to $\vec H^{1/2+\delta}(\Omega)$ and $[\curl \vec u]|_K$ belongs to $D_k$ in all elements $K$, then we further have that
                $$ \|\vec u - \vec r_h \vec u\|_0 \leq C\left(h_K^{1/2+\delta} \| \vec u\|_{\vec H^{1/2+\delta}} + h_K \| \curl \vec u\|_0 \right). $$
        \end{itemize}
    \end{theorem}
\paragraph{$H^1$:} From the Lemma regarding conforming spaces, we can immediately see that the following is a conforming space in $H^1$: 
    $$ U_h = \{ v_h \in H^1(\Omega): v_h|_K \in \P_k \qquad\forall K\in \T_h\}, $$
with a nice interpolation operator:
    \begin{theorem}
        There exists a positive constant $C$ and an interpolation operator $\pi_h$ such that, for a positive $\delta$: 
         $$\|p - \pi_h p \|_1 \leq C h^{s-1} \|p\|_{H^s} \qquad 3/2+\delta \leq s \leq k+1 . $$
    \end{theorem}
\paragraph{$L^2$:} We can finally write the space 
    $$ Z_h = \{ p_h \in L^2(\Omega): p_h|K \in P_{k-1} \qquad \forall K\in \T_h\}, $$
and there exists an interpolation operator $P_0$ such that for $v$ in $W^{l,p}(\Omega)$ and $0\leq l \leq k, p\in[1,\infty]$, there is some positive $C$ such that
    $$ \| v - P_0v\|_{L^p} \leq C h^l |v|_{W^{l,p}}. $$
For sufficiently smooth functions ($\delta>0$ in the above), these spaces induce a de Rham complex that can be extremely useful in FEM analysis. For this, we can consider the following spaces for $\delta > 0$: 
    $$
    \begin{aligned}
        U &= H^{3/2+\delta}(\Omega) \\
        V &= \{\vec v \in \vec H^{1/2+\delta}(\Omega) : \curl \vec v \in \vec H^{1/2+\delta}(\Omega)\} \\
        W &= \{ \vec w \in \vec H^{1/2+\delta}(\Omega): \dive \vec w \in \vec L^2(\Omega) \}
    \end{aligned},
    $$
which yield the following: 

    \begin{center} 
    \begin{tabular}{ccccccc}
    $H^1(\Omega)$ & $\xrightarrow{\quad\nabla\quad}$& $H(\curl;\Omega)$ & $\xrightarrow{\quad\curl\quad}$ & $H(\dive;\Omega)$ & $\xrightarrow{\quad\dive\quad} $ & $L^2(\Omega)$ \\
    $U\subset$ & & $V\subset$ & & $W \subset$ & &  \\
    $\pi_h \Bigg\downarrow$ && $\vec r_h \Bigg\downarrow$ && $\vec w_h\Bigg\downarrow$ && $P_0 \Bigg\downarrow$ \\
    $U_h$ & $\xrightarrow{\quad\phantom{nabla}\quad}$& $V_h$ & $\xrightarrow{\quad\phantom{\curl}\quad}$ & $W_h$ & $\xrightarrow{\quad\phantom{\dive}\quad} $ & $Z_h$ \\
    \end{tabular}
    \end{center} 
There is an intrinsic relationship between this structure and inf-sup conditions. It additionally commutes, and thus one can obtain identities such as 
    $$ \vec r_h \grad = \grad \pi_h, $$
which further relates the interpolation results. 

\subsection{FEM Implementation}
% 8.1.2 Ern & Guermond
% C21-23 Federico
Let us now illustrate how to efficiently approximate the solution a boundary value problem using the finite element method. Given $\Omega\subset \R^d$ with boundary $\overline{\partial \Omega} = \overline{\partial\Omega_D} \cup \overline{\partial\Omega_N}$, we define an approximating mesh  $\mathcal{T}_h = \{\Omega^e\}_{e=1}^{N_{el}}$ with $N_{el}$ elements and parameterized by $h$ the mesh size. With a mild abuse of notation,  we can write the mesh describing the boundary $\partial\Omega$ as 
$$\partial\mathcal{T}_h = \partial\mathcal{T}_h^D \cup\partial\mathcal{T}_h^N = \{\partial\Omega^e_D\}_{e=1}^{N_{el}^D} \cup \{\partial\Omega^e_N\}_{e=1}^{N_{el}^N}.$$
Let $U$ be a Banach space of functions defined on $\Omega$. Consider the Poisson problem with mixed Dirichlet and Neumann boundary conditions, which consists in finding $u\in U$ such that
$$
\begin{cases}
    -\Delta u = f & \text{in } \Omega, \\
    u = g_D & \text{on } \partial\Omega_D, \\
    \nabla u \cdot n = g_N & \text{on } \partial\Omega_N,
\end{cases}
$$
where $\partial\Omega_D$ and $\partial\Omega_N$ are the non-overlapping Dirichlet and Neumann parts of the boundary. The discrete space $U_h$ associated to $U$ can be constructed from any of the finite element approximation spaces defined above, which is often chosen as a $U$-conforming space, and equipping it with appropriate boundary conditions. To this end, choose for instance $P_h^k$ as our approximation space. We apply a lifting to account for the (possibly) nonhomogeneous Dirichlet condition $u=g_D$ on $\partial\Omega_D$, and thus the suitable approximation space is
$$U_h = P_h^k \cap U = \{v_h\in P_h^k: v_h = 0 \;\ton \partial\Omega_D\}.$$
With this, the Galerkin scheme introduced in equation~\ref{eq:galerkinscheme} applied to our boundary value problem consists in finding $u_h \in U_h$ such that 
$$ a_h(u_h, v_h) = l_h(v_h) \qquad \forall v_h \in U_h, $$
where $a_h$ and $l_h$ are the discrete approximations of the bilinear form $a(u,v)=(\nabla u, \nabla v)$ and the linear form $l(v) = (f,v) + (g_N, v)_{\partial\Omega_N}$. These approximations are naturally defined by splitting the integrals into its discrete elements, that is, 

$$ a_h(u,v) = \sum_{e=1}^{N_{el}} \int_{\Omega^e} \nabla u \cdot \nabla v \, dx \qquad l(v) = \sum_{e=1}^{N_{el}} \int_{\Omega^e} f v \, dx + \sum_{e \in \mathcal{T}_h^N} \int_{\partial\Omega_N^e} g_N v \, dS. $$
We note that one might be tempted to think that naturally the written $a_h$ matches $a$. This is typically not the case because we compute the integrals using quadrature rules, which might induce an approximation error. Another reason that we do not consider here is that the discrete problem might have some kind of stabilization to improve the overall conditioning of the linear system.  Since $U_h$ is finite-dimensional, we can expand $u_h$ through the basis functions $\{\varphi_i\}$ as 
$$u_h = \sum_{i=1}^N \alpha_i \varphi_i,$$ 
where the coordinate vector $\vec\alpha = (\alpha_1, \dots, \alpha_N)^T$ is the unknown that defines the discrete solution. Since the Galerkin scheme is valid for all $v_h \in U_h$, we can choose $v_h = \varphi_i$ for $i=1,\dots,N_{el}$. For every $i=1,\dots,N_{el}$ we have
\begin{align*}
    a_h(u_h, \varphi_i) &= a_h(\sum_{j=1}^{N_{el}}\alpha_j\varphi_j, \varphi_i)\\
    &= \sum_{j=1}^{N_{el}} \alpha_j a_h(\varphi_j, \varphi_i)\\
    &= l_h(\varphi_i),
\end{align*}
and thus we obtain the linear system of equations 
$$\ten K \vec \alpha = \vec F,$$
where we have defined the \textit{stiffness matrix} $\ten K$ and the \textit{force vector} $\vec F$ as
$$ K_{ij} = a_h(\varphi_j, \varphi_i) \quad \text{and} \quad F_i = l_h(\varphi_i). $$

Here, we note that computing $K_{ij} = a(\varphi_j, \varphi_i)$ by integrating over the entire domain $\Omega$ for all pairs $(i,j)$ is computationally inefficient, because the basis functions $\varphi_j$ in finite element methods are typically chosen to have compact support, meaning $\varphi_j(x)$ is non-zero only on a small part of the domain. For basis functions associated with nodes of a mesh, the supports of $\varphi_i$ and $\varphi_j$ only overlap for indices $i$ and $j$ corresponding to neighboring nodes or elements, and thus $a(\varphi_j, \varphi_i)$ will be nonzero only for those neighboring $(i,j)$ pairs. This results in the stiffness matrix $\ten K$ being sparse, i.e. most of its entries are zero, and thus a na√Øve computation method involving nested loops over all global indices $i$ and $j$ would spend an unnecessary amount of time computing zero entries. 

To put this into practice, let $(\Omega, V_h, \Sigma)$ be a finite element. Within each element $\Omega^e$, the global basis functions $\varphi_j$ of $V_h$ that are non-zero on $\Omega^e$ are those associated with the nodes of that element. Let $n_{dof}$ number of degrees of freedom (nodes) of the finite element. We define local shape functions $\phi_a^e$, $a=1, \dots, n_{dof}$, which are the restrictions of the global basis functions to the element $\Omega^e$, re-indexed locally from 1 to $n_{dof}$. We define the local or \textit{element stiffness matrix} $\ten k^e \in \R^{n_{dof}\times n_{dof}}$ and local or \textit{element force vector} $\vec f^e \in \R^{n_{dof}}$ as integrals over $\Omega^e$ using these local shape functions, i.e.
$$ k_{ab}^e = \int_{\Omega^e} \nabla \phi_b^e \cdot \nabla \phi_a^e \, dx \quad \text{and} \quad f_a^e = \int_{\Omega^e} f \phi_a^e \, dx. $$
If the element $\Omega^e$ has a Neumann boundary segment on $\partial \Omega_N$, there is also a local Neumann boundary force vector $f_{N,a}^e$:
$$ f_{N,a}^e = \int_{\partial\Omega_N^e} g_N \phi_a^e \, dS. $$
The assembly process of adding the contributions from the local element matrices and vectors into the global system requires precisely mapping the local effects within each element to their corresponding locations in the global stiffness matrix $\ten K$ and force vector $\vec{F}$. This mapping accounts for both the topological connectivity of the mesh elements and the boundary conditions applied to the domain. We use several arrays to manage this mapping.
\begin{enumerate}
    \item The element node matrix $\texttt{IEN}\in \mathbb{Z}^{n_{el}\times n_{dof}}$ is a connectivity array with terms $\texttt{IEN}[e, a]$, which provides the global node index corresponding to the local node $a$ (ranging from 1 to $n_{dof}$) within element $e$ (ranging from 1 to $N_{el}$). Explicitly, if we have three elements in 1D, whose global node indexes are $(0,1)$, $(1,2)$ and $(2,3)$, respectively, then the $\texttt{IEN}$ matrix is 
        $$ \texttt{IEN} = \begin{bmatrix}
    0 & 1 \\
    1 & 2 \\
    2 & 3
    \end{bmatrix}. $$
\item The destination array $\texttt{ID}\in \mathbb{Z}^{n_{nodes}}$ maps each global node index $i$ to its corresponding equation number or global degree of freedom index in the assembled linear system. If global node $i$ has a prescribed boundary condition (such as a Dirichlet value), $\texttt{ID}(i)$ is typically set to 0 or a negative value to indicate that this node does not correspond to an unknown degree of freedom in the system. For example, if we have $n_{nodes}=4$ nodes in a 1D mesh, where where nodes $1$ and $3$ have Dirichlet boundary conditions, the $\texttt{ID}$ array might look like:
    $$ \texttt{ID} = \begin{bmatrix}
    0 \\
    1 \\
    0 \\
    2
    \end{bmatrix}, $$
    where the nonzero entries correspond to free nodes.
\item The location matrix $\texttt{LM}\in \mathbb{Z}^{n_{el}\times n_{dof}}$ corresponds to the composite local-to-global mapping, where for a given element $e$ and a local node $a$, $\texttt{LM}[e,a]$ corresponds to the global degree of freedom index. This allows us to write the important relation 
    $$\texttt{LM}[e,a] = \texttt{ID}[\texttt{IEN}[e,a]].$$
\end{enumerate}
The full mapping to degrees of freedom in the presence of general boundary conditions is captured by the $\texttt{ID}[\texttt{IEN}[e,a]]$ relation, which is the index ultimately used to locate the correct row and column in the global system during assembly. The pseudocode below details the assembly algorithm using this mapping to add the contributions from the element matrices $\ten k^e$ and vectors $\vec f^e$ to the global system $\ten K$ and $\vec F$.
\begin{algorithmic}[1]
    \State Initialize global stiffness matrix $\ten K$ and force vector $\vec{F}$ to zero
    \For{each element $e = 1, \dots, N_{el}$}
    ¬† ¬† \State Compute element stiffness matrix $\ten k^e\in\R^{n_{dof} \times n_{dof}}$ and force vector $\vec f^e\in\R^{n_{dof}}$
    ¬† ¬† \For{each local node $a = 1, \dots, n_{dof}$}
    ¬† ¬† ¬† ¬† \State $i = \texttt{LM}[e,a]$
    ¬† ¬† ¬† ¬† \For{each local node $b = 1, \dots, n_{dof}$}
    ¬† ¬† ¬† ¬† ¬† ¬† \State $j = \texttt{LM}[e,b]$
    ¬† ¬† ¬† ¬† ¬† ¬† \If{$i \neq 0$ and $j \neq 0$} \Comment{Free nodes}
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† \State $K_{i,j} \gets K_{i,j} + k^e_{ab}$
    ¬† ¬† ¬† ¬† ¬† ¬† \EndIf
    ¬† ¬† ¬† ¬† ¬† ¬† \If{$j$-th node is on the Dirichlet boundary $\partial\mathcal{T}_h^D$} \Comment{Dirichlet nodes}
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† \State $F_i \gets F_i - g_D k^e_{ab}$
    ¬† ¬† ¬† ¬† ¬† ¬† \EndIf
    ¬† ¬† ¬† ¬† \EndFor
    ¬† ¬† ¬† ¬† \If{$i \neq 0$} \Comment{Non-Dirichlet nodes}
    ¬† ¬† ¬† ¬† ¬† ¬† \State $F_i \gets F_i + f^e_a$
    ¬† ¬† ¬† ¬† \EndIf
    ¬† ¬† ¬† ¬† \If{$i$-th node is on the Neumann boundary $\partial\mathcal{T}_h^N$} \Comment{Neumann nodes}
    ¬† ¬† ¬† ¬† ¬† ¬† \State $F_i \gets F_i + f^e_{N,a}$
    ¬† ¬† ¬† ¬† \EndIf
    ¬† ¬† \EndFor
    ¬† ¬† \EndFor
    \end{algorithmic}

\paragraph{Efficient computation of element integrals via the master element}
Computing the integrals for $\ten k^e$ and $\vec f^e$ directly on each physical element $\Omega^e$ can be complicated due to the varied shapes and sizes of elements in a mesh. The standard technique is to map each physical element $\Omega^e$ to a single, simple reference element, called the master element $\hat{\Omega}$. This master element is always the same (e.g., $[-1,1]$ for 1D segments, a standard triangle or square for 2D, etc.).

Let $\vec{x} = \vec{x}(\vec{\xi})$ be the transformation mapping a point $\vec{\xi}$ in the master element $\hat{\Omega}$ to a point $\vec{x}$ in the physical element $\Omega^e$. This transformation is typically defined using the shape functions:
$$ \vec{x}(\vec{\xi}) = \sum_{a=1}^{n_{dofs}} \vec{x}_a^e \hat{\phi}_a(\vec{\xi}), $$
where $\vec{x}_a^e$ are the physical coordinates of the nodes of element $e$, and $\hat{\phi}_a(\vec{\xi})$ are the shape functions defined on the master element. The local shape functions on the physical element are related to the master element shape functions by $\phi_a^e(\vec{x}) = \hat{\phi}_a(\vec{\xi}(\vec{x}))$, where $\vec{\xi}(\vec{x})$ is the inverse transformation.

The change of variables formula for integrals states that
$$ \int_{\Omega^e} \psi(\vec{x}) \, d\Omega = \int_{\hat{\Omega}} \psi(\vec{x}(\vec{\xi})) |\det J| \, d\hat{\Omega}, $$
where $J = \frac{\partial \vec{x}}{\partial \vec{\xi}}$ is the Jacobian matrix of the transformation, and $|\det J|$ is the determinant of the Jacobian.
The gradient in physical coordinates is related to the gradient in master element coordinates by the inverse transpose of the Jacobian matrix:
$$ \nabla_{\vec{x}} \phi_a^e(\vec{x}) = J^{-T}(\vec{\xi}) \nabla_{\vec{\xi}} \hat{\phi}_a(\vec{\xi}). $$
Using these, the element integrals become integrals over the master element:
$$ k_{ab}^e = \int_{\hat{\Omega}} (J^{-T} \nabla_{\vec{\xi}} \hat{\phi}_b) \cdot (J^{-T} \nabla_{\vec{\xi}} \hat{\phi}_a) |\det J| \, d\hat{\Omega}, $$
$$ f_a^e = \int_{\hat{\Omega}} f(\vec{x}(\vec{\xi})) \hat{\phi}_a(\vec{\xi}) |\det J| \, d\hat{\Omega}, $$
$$ f_{N,a}^e = \int_{\partial\hat{\Omega}_N} g_N(\vec{x}(\vec{\xi})) \hat{\phi}_a(\vec{\xi}) |\det J_{\partial}| \, d\hat{\Gamma}, $$
where $J_\partial$ is the Jacobian of the transformation on the boundary segment. These integrals over the master element can be computed efficiently using numerical integration rules, known as \textit{quadratures}. The shape functions $\hat{\phi}_a$ and their gradients on the master element, and the Jacobian and its determinant, only need to be evaluated at the fixed quadrature points on $\hat{\Omega}$.

\example{In 2D, we can define a triangular (simplicial) element with vertices $\vec{x}_0^e$, $\vec{x}_1^e$, and $\vec{x}_2^e$. The master element is the reference triangle with vertices $(0,0)$, $(1,0)$, and $(0,1)$. In this setting the local shape functions are given by 
\begin{align*}
\hat{\phi}_0(\xi_1, \xi_2) &= \xi_1,\\
\hat{\phi}_1(\xi_1, \xi_2) &= \xi_2, \\
\hat{\phi}_2(\xi_1, \xi_2) &=  1 - \xi_1 - \xi_2.
\end{align*}
Thus, the mapping from the master element with coordinates $\vec\xi = (\xi_1,\xi_2)$ to the physical element with coordinates $\vec x = (x_1, x_2)$ is given by
$$ \vec{x}(\xi_1, \xi_2) =  \vec{x}_0^e\xi_1  + \vec{x}_1^e \xi_2 + \vec{x}_2^e (1-\xi_1-\xi_2). $$
}
\subsubsection{Nonhomogeneous Dirichlet boundary conditions}
% 8.4 Ern & Guermond
There exist several approaches to treat the implementation of nonhomogeneous Dirichlet boundary conditions, which we introduce from \cite{ern2004theory}. Assume that there exist $N_D$ Dirichlet nodes in the mesh, and thus $N_{free} = N_{el} - N_D$ free nodes.
\begin{itemize}
    \item \underline{Eliminating the Dirichlet nodes}: a straightforward method is to reorder the global degrees of freedom so that the first $N_D$ rows of the system correspond to the Dirichlet nodes, and the remaining $N_{free}$ rows correspond to the free nodes. Introduce the notation $\vec x = (\hat{\vec x},\tilde{\vec x})$ where $\hat{\vec x} = (x_1,\dots,x_{N_D})^\top$ and $\tilde{\vec x} = (x_{N_D + 1},\dots,x_{N_el})$ are the Dirichlet and non-Dirichlet parts of the vector $\vec x$. This induces a block problem 
    $$
        \left[
    \begin{array}{cc}
    \ten I & \vec{0} \\
    \ten B & \ten{\tilde{K}}
    \end{array}
    \right]
    \begin{bmatrix}
    \hat{\vec\alpha} \\
    \tilde{\vec\alpha}
    \end{bmatrix}
    =
    \begin{bmatrix}
    \hat{\vec F} \\
    \tilde{\vec F}
    \end{bmatrix},
    $$
    where $B_{ij} = a_h(\varphi_j, \varphi_i)$ for $i=N_D+1,\dots,N_{el}$ and $j=1,\dots,N_{D}$ and $\tilde{K}_{ij} = a_h(\varphi_j, \varphi_i)$ for $i,j=N_D+1,\dots,N_{el}$. Here, it is clear that the subsystem associated to the Dirichlet nodes is trivial, i.e. $\hat{\vec \alpha} = \hat{\vec F}$, and thus we can eliminate the Dirichlet nodes from the system. The resulting system is given by 
    $$\ten{\tilde{K}} \tilde{\vec\alpha} = \tilde{\vec F} - \ten B \hat{\vec F},$$
    which implies that we need to assemble now two matrices, with sparsity profiles that may not be inherited from $\ten K$. This is exactly equivalent to performing a lifting on the original system, as seen in Remark 8.17 in \cite{ern2004theory}.
    \item \underline{Keeping the Dirichlet nodes}: instead of eliminating the Dirichlet nodes, we can keep them in the system and assemble as usual, which leads to the stiffness matrix and force vector associated to the Neumann problem. After assembling, we correct the rows of $\ten K$ corresponding to the $N_D$ Dirichlet nodes, by setting them equal to zero, except the diagonal which is set to 1. Similarly, we set the entries of $\vec F$ corresponding to the Dirichlet nodes equal to the prescribed values. Although the resulting system has more degrees of freedom, by using an appropriate iterative solver (such as a Krylov subspace method), the Dirichlet data will be exactly satisfied in the solution for every iteration. 
    \item \underline{The penalty method}: as we studied previously in the context of non-conforming spaces, one can add a penalty term to the Dirichlet nodes to enforce them approximately. First, one assembles $\ten K$ and $\vec F$ as usual, which corresponds to the Neumann problem system. Then, in every row corresponding to a Dirichlet node, one adds the penalty term 
    $$\varepsilon^{-1}\alpha_i = \varepsilon^{-1}g_{D,i}$$
    where $g_{D,i}$ is the Dirichlet data at node $i$ and $\varepsilon$ is a small positive parameter. In contrast to the previous two methods, this method does not ensure that the Dirichlet data is satisfied exactly, but it allows one to inherit the possible symmetry of the original stiffness matrix $\ten K$, which is a very useful property for iterative solvers. The penalty method is also particularly useful when the Dirichlet data is not smooth, as it allows one to control the convergence of the solution to the Dirichlet data by adjusting the penalty parameter $\varepsilon$.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Beyond ellipticity}\label{section:beyond-ellipticity}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In the previous section we have thorougly studied elliptic problems and many approximation propoerties. Still, one might rightfully notice that ellipticity can be quickly broken. For example, the operator $-\Delta: H_0^1(\Omega)\to H^{-1}(\Omega)$ is elliptic, but if we remove the minus sign, it loses that property. As this case is linear, it is possible to remap the unknown with $u\mapsto -u$, but it still feels unsatisfactory that the property is so fragile. Because of this, in this section we review two important theories that go beyond ellipticity: the inf-sup theory and the theory of Fredholm operators.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Inf-sup conditions}

We are interested in showing first that the inf-sup condition means 
surjectivity. This will require the use of certain results from 
functional analysis. For detailed proofs and omitted details, see \cite{chen2024infSup} 
and \cite{gatica2014simple}. A first important result is to characterize 
operators with closed range. 

\begin{lemma}
    Let \(U\) and \( V\) be Banach spaces and \(T:U\to V\) a linear 
    continuous operator. Then, \(T\) is injective and its range \(R(T)\) is 
    closed if and only if  \(T\) is bounded below, i.e., there exists 
    a positive constant \(c\) such that 
    \begin{displaymath}
        \lVert Tu \rVert \ge c\lVert u \rVert
        \quad \text{ for all } u\in U
    \end{displaymath}
\end{lemma}
\begin{proof}
    First, we assume \(T\) is bounded below. If \(Tu=0\), the inequality 
    implies \(u=0\), i.e., \(T\) is injective. Let \(\{Tu_k\}\) be a convergent sequence in \(V\). By the inequality, we have that
    \begin{displaymath}
        \lVert Tu_k - Tu_m \rVert \ge c\lVert u_k - u_m \rVert
    \end{displaymath}
    for some \(k,m\). Then, because \(\{Tu_k\}\) is a Cauchy sequence, we know that \(\{u_k\}\) is also a Cauchy sequence and, because both spaces are Banach, the sequence then converges to some \(u\in U\). The continuity of \(T\) shows that \(Tu_k\) converges to \(Tu\) and thus the range of \(T\) is closed. 

    Now, assume \(T\) is injective and its range is closed. Then, because 
    \(R(T)\) is a closed subspace of a Banach space, it is also Banach. As \( T\) is injective, \(T^{-1}\) is well defined on \(R(T)\), i.e., 
    \(T:U\to R(T)\subset V\) is invertible. In addition, Open Mapping Theorem 
    shows that \(T^{-1}\) is continuous. Then
    \begin{displaymath}
        \lVert u \rVert = \lVert T^{-1}(Tu) \rVert 
        \le \lVert T^{-1} \rVert \lVert Tu \rVert 
    \end{displaymath}
    which implies \(T\) is bounded below with constant 
    \(c = \lVert T^{-1} \rVert^{-1}\).
\end{proof}

Now, we would like to characterize the surjectivity of the operator using its dual.

\begin{lemma}
    Let \(U\) and \(V\) be Banach spaces and let \(T\) be a linear continuous 
    operator. \(T\) is surjective if and only if \(T'\) is injective with closed 
    range, where \(T'\) is the dual (or transpose) operator of \(T\).
\end{lemma}
\begin{proof}
    First, if \(T\) is surjective, then \(R(T) = V\) is closed. By Closed Range 
    Theorem, \(R(T')\) is also closed and 
    \begin{displaymath}
        R(T) = N(T')^\bot = 
        \{v\in V: \langle v', v\rangle = 0 \,\,\, \forall v' \in N(T')\}
    \end{displaymath}
    where \(N(T')\) is the null space of \(T'\). Now, since 
    \(V = R(T) = N(T')^\bot\), then, for each \(v' \in N(T')\), it holds
    that \( \langle v', v\rangle = 0\,\,\, \forall v \in V\), which means that 
    \(v' = 0\). Thus, \(N(T') = \{0\}\), so \(T'\) is injective.

    For the proof in the other direction, we get from Closed Range Theorem that also $R(T)$ is closed. By contradiction, consider $v$in $V$ such that $v\not\in R(T)$. By Hahn-Banach we get that there exists $f$ in $V'$ such that $f(R(T)) = 0$ and $f(v)=1$\footnote{This is an improvement over Urysohn's Lemma, which yields only a possibly nonlinear function}. Then, $T'f$ is such that
        $$ \langle T'f', u\rangle = \langle f, Tu\rangle = u \qquad \forall u\in U.$$
    This implies that $f=0$, which contradicts that $f(v)=1$. 

\end{proof}

Combining both lemmas, we can write
\begin{align*}
    T \text{ surjective}
    &\Longleftrightarrow T'\text{ injective and with closed range}
    \\&\Longleftrightarrow T'\text{ is bounded below}
\end{align*}

Let \(U\) and \(V\) Hilbert spaces and \(T:U\to V\) linear continuous operator
with transpose \(T':V'\to U'\), considering its adjoint operator \(T^*:V \to U\), 
we can write \[T^* = R_U^{-1} \circ T' \circ R_V\] and 
\[\lVert T^* \rVert_{V\to U} = \lVert T' \rVert_{V'\to U'}\] 
where \(R_U: U\to U'\) and 
\(R_V: V\to V'\) are Riesz operators. Then, using the Riesz representation 
Theorem, we have that

\begin{align*}
    T \text{ surjective}
    &\Longleftrightarrow 
    T'\text{ is bounded below}
    \\&\Longleftrightarrow 
    \lVert T'v' \rVert_{U'} \ge c \lVert v' \rVert_{V'} \quad \forall v'\in V'
    \\&\Longleftrightarrow 
    \lVert (T'\circ R_V) (v) \rVert_{U'} \ge c \lVert v \rVert_{V} 
    \quad \forall v\in V
    \\&\Longleftrightarrow 
    \lVert (R_U \circ R_U^{-1} \circ T'\circ R_V) (v) \rVert_{U'} 
    \ge c \lVert v \rVert_{V} \quad \forall v\in V
    \\&\Longleftrightarrow 
    \lVert (R_U \circ T^*) (v) \rVert_{U'} 
    \ge c \lVert v \rVert_{V} \quad \forall v\in V
    \\&\Longleftrightarrow 
    \lVert T^* v \rVert_{U} 
    \ge c \lVert v \rVert_{V} \quad \forall v\in V
    \\&\Longleftrightarrow 
    c \lVert v \rVert_{V}  
    \le \sup_{u\in U}\frac{(Tu, v)}{\lVert u \rVert_{U}} \quad \forall v\in V
    \\&\Longleftrightarrow 
    0 < c \le \inf_{v\in V}\sup_{u\in U}
    \frac{(Tu, v)}{\lVert u \rVert_{U}\lVert v \rVert_{V}}
\end{align*}

We also can characterize the injectivity of the operator.

\begin{lemma}
    Let \(U\) and \(V\) be Banach spaces and let \(T:U\to V\) be a linear 
    continuous operator. Then, \(T\) is injective if and only if 
    \[\sup_{v'\in V'} \langle Tu, v'\rangle_{V\times V'} > 0 
    \quad \forall u\in U\text{, }u \neq 0\]
\end{lemma}
\begin{proof}
    Suppose that \(T\) is injective. If \(u\neq 0\), then \(Tu \neq 0\), and so, 
    for some \(v'\in V'\), we have that \(\langle Tu, v'\rangle_{V\times V'} 
    \neq 0\). Which implies the right side of the equivalence.

    Now, assume the right side of the equivalence. By contradiction, if \(T\)
    is non injective, there exists \(u\in U \) with \(u\neq 0\) such that \(Tu = 0\), 
    which contradicts the hypothesis.
\end{proof}

We can now postulate a general Lax-Milgram Theorem. We write it in Hilbert Spaces.

\begin{theorem}(Generalized Lax-Milgram)
    Consider \(H_1\), \(H_2\) Hilbert spaces and a bounded bilinear form 
    \(B:H_1\times H_2 \to \mathbb{R}\). Then, there exists a unique \(u\in H_1\) for each 
    \(F\in H_2'\) such that \[B(u,v) = F(v),\quad \forall v \in H_2\] if and only if 
    \begin{enumerate}
        \item \(\exists \alpha > 0\) such that \[\sup_{u\in H_1,u\neq 0}
        \frac{B(u,v)}{\lVert u \rVert_{H_1}} \ge \alpha \lVert v \rVert_{H_2}
        \quad \forall v\in H_2\quad \text{ (surjective) }\]
        \item \[\sup_{v\in H_2} B(u,v) > 0 
        \quad \forall u\in H_1,\,\, u\neq 0
        \quad \text{ (injective) }\]
    \end{enumerate}
\end{theorem}
We note that injectivity can be equivalently stated as an inf-sup condition simply by inverting the arguments in the surjectivity inf-sup.  The main difficulty in using this theory is that well-posedness of a discrete problem is not inherited from the continuous one. In fact, consider $U_N\subset U$ and $v'_N\in V'_N\subset V$ discrete spaces, then 
$$ \|T'v_N'\|_{U'} = \sup_{u\in U}\frac{\langle u, T'v_N'\rangle}{\|u\|_U} \geq \sup_{u_N\in U_N}\frac{\langle u_N, T'v_N'\rangle}{\|u_N\|_U}, $$
which establishes our claim. Despite this, one can still recover convergence as before. To see this, consider $u$ and $u_N$ the continuous and discrete solutions given by the following problems with $a:U\times V \to \R$:
    $$\begin{aligned}
        a(u,v) &= f(v) &&\forall v\in V,\\
        a(u_N, v_N) &= f(v_N) &&\forall v_N\in V_N.
    \end{aligned}$$
This bilinear forms make sense as operator equations as they induce an $A:U\to V'$. Both problems are well-posed are both operators are surjective, which implies that 
    $$ a(u-u_N, v) = 0  \qquad \forall v_N \in V_N. $$
From the injectivity seen as an inf-sup condition we get that
$$ \alpha \| u - u_N \|_U \leq \sup_{v_N\in V_N} \frac{a(u-u_N, v_N)}{\|v_N\|} \leq \| a \| \|u - \xi_N \|, $$
for some $\xi_N$ in $U_N$, as in the Lax-Milgram proof. Taking the infimum in $\xi_N$ and setting $C=\|a\|$ yields that
    $$ \| u - u_N \| \leq \frac{C}{\alpha}\text{dist}(u, U_N). $$


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Fredholm operators}

Most of the forms in which we present our problems comes from the fantastic notes from Andrea Moiola on time-harmonic acoustic waves \cite{moiola2021scattering}. Our reference problem will be the Helmholtz equation, given by the following strong form:

\[
\left\{
\begin{array}{rc}
    -\Delta u -k^2 u = f, &\Omega,\\
    u=0, &\partial\Omega,
\end{array}\right.
\]

for some $f\in H^{-1}(\Omega)$ and some boundary condition. This equation can be recoverred in the following way: consider the wave equation
    $$ \ddot u - \Delta u = 0, $$
and consider a variable separation procedure with $u(t,x) = V(t)U(x)$. This is a common technique, and it results in 
    $$ \frac{\Delta U}{U} = k^2.$$
multiplying by $U$ gives the desired result. Regarding well-posedness, let's first explore what we can conclude with Lax-Milgram. We will assume $u$ in $H_0^1(\Omega)$ for simplicity, which yields the following weak form: Find $u$ in $H_0^1(\Omega)$ such that

\[
a(u,v) = (\nabla u, \nabla v) - k^2(u,v) = \langle f, v\rangle, \quad \forall v\in H_0^1.
\]

Using $H_0^1(\Omega)$ with the norm induced by the seminorm $|v|_1 = \|\nabla v\|_0$, Poincar√© inequality gives us $\|u\|_0\leq C_{\Omega}\|\nabla u\|_0$, and so the Lax-Milgram hypotheses look as follows: 

\begin{itemize}
    \item Boundedness: 
    \begin{align*}
        a(u,v) &\leq \|\nabla u\|_0\|\nabla v\|_0 + k^2\|u\|_0\|v\|_0,\\ 
        &\leq (1+k^2C_\Omega^2)|u|_1|v|_1.
    \end{align*}

    \item Coercivity: 
    \begin{equation*}
        a(v,v) = \|\nabla v\|^2_{0,\Omega} - k^2\|v\|^2_{0,\Omega}.
    \end{equation*}

    We note that, by Poincar√© inequality,

    \[ k^2\|v\|^2_0 \leq k^2 C^2_{\Omega}\|\nabla v\|^2_0, \]

    and thus,

    \begin{align*}
        a(v,v) &\geq \|\nabla v\|^2_0 - k^2C_\Omega^2\|\nabla v\|^2_0 \\
        &= (1- k^2 C_\Omega^2)\|\nabla v\|^2_{0,\Omega}.
    \end{align*}

    In other words, the problem is well-possed if 
    $$1-k^2C_\Omega^2 > 0 \iff k^2 < \frac{1}{C_\Omega^2}$$
\end{itemize}

This is a very limited answer, so we will now answer what happens for arbitrary $k\in\R$.

\begin{definition}
    A linear operator $K:H_1 \to H_1$ is \textbf{compact} if the image of a bounded sequence admits a converging subsequence.
\end{definition}
\begin{definition}
    A bounded linear operator is a \textbf{Fredholm operator} if it is the sum of an invertible and a compact operator. 
\end{definition}

\begin{theorem}[Fredholm alternative]
    A Fredholm operator is injective if and only if it is surjective. In such case, it has a bounded inverse.
\end{theorem}

A simpler way of showing that an operator is Fredholm, is through a G√•rding inequality.

\begin{definition}
    Consider $H\subset V$ Hilbert spaces with a continuous embedding $H\hookrightarrow V$. 
    A bilinear form $a:H\times H\to \R$ satisfies a \textbf{G√•rding inequality} if there exist two positive constants $\alpha$, $C_V$ subject to, 
    
    \[a(v,v) \geq \alpha \|v\|^2_H - C_V\|v\|^2_V,\quad \forall v\in H.\]
\end{definition}

\textbf{Proposition:}
 If the inclusion $H\hookrightarrow V$ is compact, then the operator $A: H\to H^*$ associated to $a:H\times H\to \R$, i.e,
 \[\langle Ax,y\rangle = a(x,y),\]

 is such that if $a$ satisfies G√•rding then $A$ is Fredholm. 

\begin{proof}
    We note that, by definition of G√•rding,  \(a(u,v) + C_V(u,v)_V\) is invertible because of Lax-Milgram. 
    We now try to write the equation in operator form. The form $(u,v)_V$ is handled as follows: set $T:V\to H^*$ as

    \[\langle Tv, w\rangle_{H^*\times H} = "(v,w)_V"\equiv (v,iw)_V,\]

    where $i: H\to V$ is the compact embedding. $T$ is clearly bounded:

    \begin{align*}
        \|Tv\|_{H^*} &= \sup_{w\in H}\frac{(v,iw)_V}{\|w\|_H},\\
        &\leq \sup_{w\in H}\frac{\|v\|_V\|iw\|_V}{\|w\|_H},\\
        &\leq \sup_{w\in H}\frac{\|v\|_V\|i\|\|w\|_H}{\|w\|_H},\\
        &\leq \|i\|\|v\|.
    \end{align*}

    Then, the operator associated to the problem is $B :=A + C_V T\circ i$, where:
    \begin{itemize}
        \item $B$ is invertible,
        \item $T$ is continuous,
        \item $i$ is compact. 
    \end{itemize}

    The last two impliy that $T\circ i$ is compact, as the composition of continuous and compact operators is compact. 
    It follows that $A = B - C_V T\circ i$ is Fredholm.
\end{proof}



\subsection{Galerkin stability}

We have seen that elliptic problems have the following a-priori stability estimate

\[\alpha \|u\|_H \leq \|f\|_{H^*}\quad\text{ for }\quad a(u,v) = \langle f, v\rangle, \quad \forall v\in H.\]

which carries naturally to the discrete problem:

\[\alpha \|u_h\|_H \leq \|f\|_{H^*}\quad\text{ for }\quad a(u_h,v_h) = \langle f, v\rangle, \quad \forall v_h\in H_h.\]

This can be rewritten as follows: denote the orthogonal projection $\Pi_h: H\to H^*$, and $R_H:H\to H^*$ the Riesz map.

Then:

\begin{align*}
    &a(u,v) = \langle f, v\rangle_{H^*\times H}\quad \forall v\in H,\\
    \iff &\langle Au, v\rangle_{H'\times H} = \langle f, v\rangle_{H'\times H}\quad \forall v\in H,\\
    \iff &Au = f \quad\text{ in $H'$ and }(R^{-1}\circ A)u = R^{-1}f\quad\text{in }H.
\end{align*}

Also:

\begin{align*}
    &a(u_h,v_h) = \langle f, v_h\rangle\quad\forall v_h\in H_h,\\
    \iff &(R^{-1}\circ A u_h,v_h)_H = (R^{-1}f,v_h)_H\quad\forall v_h\in H_h,\\
    \iff &\Pi_h\mathbb{A}u_h = \Pi_h\mathbb{F},
\end{align*}

where $\mathbb{A} = R^-1\circ A$ and $\mathbb{F} = R^{-1}f$. The stability estimate gives, 

\[\|u_h\| \leq\frac{1}{\alpha}\|\Pi_h\mathbb{F}\| = \frac{1}{\alpha}\|\Pi_h\mathbb{A} u_h\|.\]

Finally, by duality, we get

\begin{align*}
    \|u_h\| &\leq \frac{1}{\alpha}\|\Pi_h\mathbb{A}u_h\| = \frac{1}{\alpha}\sup_{v_h}\frac{(\Pi_h\mathbb{A}u_h,v_h)}{\|v_h\|}
    =\frac{1}{\alpha}\sup_{v_h}\frac{\langle Au_h,v_h\rangle_{H'\times H}}{\|v_h\|} = \frac{1}{\alpha}\sup_{v_h}\frac{a(u_h,v_h)}{\|v_h\|}.
\end{align*}

We call this result,

\[\|u_h\| \leq C\sup_{v_h\in V_h}\frac{a(u_h,v_h)}{\|v_h\|},\qquad \forall u_h\in H_h,\]

an \textbf{inf-sup condition}, which we will later use to prove \textbf{surjectivity} and \textbf{injectivity} separately. One may readily see that this condition implies (discrete) injectivity as $Au_h$ implies $u_h = 0$.

We will require the following Lemma regarding discrete stability of Fredholm operators. Details are provided in \cite{sayas2019variational}.
\begin{lemma}
Consider a bilinear form $a:H\times H\to \R$ associated to an injective Fredholm operator $A+K$. Then, there exists $C, h_0>0$ such that

\[\|u_h\|_H\leq C\sup_{v_h}\frac{a(u_h,v_h)}{\|v_h\|},\quad \forall u_h\in H_h, \quad h\leq h_0.\]
In other words, discrete stability holds only for sufficiently fine meshes. 
\end{lemma}

Consider the discrete problem
\[a(u_h,v_h) + b(u_h,v_h) = \langle f, v_h\rangle\]

where $a$ and $b$ are the bilinear forms associated to an elliptic and a compact operator respectively, and consider the Galerkin projection $G_h: H\to H_h$

\[a(G_hu, v_h) + b(G_hu, v_h) = a(u, v_h) + b(u,v_h).\]

Under the previous hypothesis, for $h\leq h_0$ we have, 

\[\|G_hu\|\leq C\sup_{v_h}\frac{a(G_hu, v_h) + b(G_hu, v_h)}{\|v_h\|}\leq C\|A+K\|\|u\|.\]

Then $G_h$ is bounded. We observe that, as $G_h$ is a porjection, it holds that $G_h\Pi_h = \Pi_h$, and thus:
\begin{align*}
    \|u - G_hu\|&\leq \|u-\Pi_hu\| + \|\Pi_hu - G_hu\|,\\
    &= \|u-\Pi_hu\| + \|G_h(\Pi_hu - u)\|,\\
    &\leq (1+C\|A+K\|)\|u - \Pi_h u\|,
\end{align*}

which implies

\[\|u-u_h\|\leq (1+C\|A+K\|)\inf_{v_h\in H_h}\|u-v_h\|,\] 

which is a C√©a estimate for sufficiently small $h$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Saddle point problems}

In this section we will study the well-posedness theory of saddle point problems. The presentation has been taken, mostly verbatim, from \cite{gatica2014simple}. A saddle point problem has the form:
\begin{displaymath}
    \begin{bmatrix}
        A & B^{T}\\ 
        B & 0
    \end{bmatrix}
    \begin{bmatrix}
        u \\ p
    \end{bmatrix}
    =
    \begin{bmatrix}
        f \\ g
    \end{bmatrix}.
\end{displaymath}

To grasp the relevance of this formulations, let's first see
some examples:
\begin{itemize}
    \item \textbf{Stokes}: The variational formulation of
    Stokes reads:
    \begin{displaymath}
    \begin{cases}
        (\nabla u, \nabla v) - (p, \dive v)
        &=
        \langle f, v \rangle
        \quad\forall v,
        \\
        (\dive u, q) &= 0 \hspace{3em}\forall q.
    \end{cases}
    \end{displaymath}
    By inspection we see that \(A=-\Delta\), \(B=\dive\).

    \item \textbf{Darcy (or mixed Poisson)}: We are dealing 
    with the problem
    \begin{displaymath}
        -\Delta u = f\quad \text{ in } \Omega
    \end{displaymath}
    Introducing the variable \(\sigma = -\nabla u\), the problem
    now reads:
    \begin{displaymath}
    \left\{
    \begin{aligned}
        \dive \sigma &= f && \quad \text{ in } \Omega\\ 
        \sigma + \nabla u &= 0 && \quad \text{ on } \partial\Omega
    \end{aligned}
    \right.
    \end{displaymath}
    Testing the equation on \(\tau\) and :
    \begin{displaymath}
    \left\{
    \begin{aligned}
        (\sigma, \tau) - (u, \dive \tau) &= 0
        &&\quad\forall \tau, \\ 
        (\dive \sigma, v) &= \langle f,v \rangle
        &&\quad \forall v,
    \end{aligned}
    \right.
    \end{displaymath}
    where $A$ is the identity operatir and $B=\dive$ as before.

    \item \textbf{Primal-Mixed Poisson (Dirichlet with multipliers)}
    Consider the problem
    \begin{displaymath}
    \left\{
    \begin{aligned}
        -\Delta u &= f &&\quad\text{ in } \Omega \\ 
        u &= g &&\quad\text{ on } \partial\Omega
    \end{aligned}
    \right.
    \end{displaymath}
    Integration by parts yields:
    \begin{displaymath}
        (\nabla u, \nabla v) 
        - \langle \gamma_N u, \gamma_D v \rangle
        = \langle f,v \rangle.
    \end{displaymath}
    Define \(\xi = -\gamma_N u\) and impose Dirichlet boundary
    conditions weakly. That is,
    \begin{displaymath}
        \forall \lambda \in H^{-1/2}(\partial\Omega)
        \colon\qquad
        \langle \lambda, \gamma_D u \rangle
        =
        \langle \lambda, g \rangle
    \end{displaymath}
    Writing everything together shows a saddle point problem:
    \begin{displaymath}
    \left\{
    \begin{aligned}
        (\nabla u, \nabla v) + \langle \xi, \gamma_D v \rangle
        &= \langle f,v \rangle
        &&\quad \forall v,\\ 
        \langle \lambda, \gamma_D u \rangle &= \langle \lambda, g \rangle
        &&\quad  \forall \lambda.
    \end{aligned}
    \right.
    \end{displaymath}
\end{itemize}

Now that we know some examples of saddle points problems, a natural
question is to ask for conditions for the existence and uniqueness
of solutions. Fortunately, this has already been done and it's known
as the \emph{Ladyzhenskaya-Babu\v{s}hka-Brezzi Theory}, typically denoted as LBB theory.

\begin{theorem}
    Consider the problem
    \begin{equation*}\label{mixed}
        \begin{bmatrix}
            A & B^{T}\\ 
            B & 0
        \end{bmatrix}
        \begin{bmatrix}
            u \\ p
        \end{bmatrix}
        =
        \begin{bmatrix}
            f \\ g
        \end{bmatrix}
    \end{equation*}
    Let \(V = \ker B\) and \(\Pi\colon H\to V\) the orthogonal
    projector. Suppose that
    \begin{itemize}
        \item \(\Pi A\colon V\to V\) is a bijection;
        \item the bilinear form \(b\) (associated to \(B\))
        satisfies the inf-sup condition with constant \(\beta\).
    \end{itemize}
    Then, for each \((f,g)\) in \(H'\times Q'\) there exists
    a unique pair \((u,p)\in H\times Q\) such that~\eqref{mixed}
    holds. Moreover, there is a positive constant
    \(C=C(\|A\|, \| (\Pi A)^{-1}\|, \beta)\) such that
    \begin{equation*}
        \| (u,p) \|
        \le
        C \left( \|f\| + \|g\| \right) 
    \end{equation*}
\end{theorem}
\begin{proof}
    Exercise :)
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Common example: Darcy} We will present a the worked example of Darcy's problem, to be analyzed using the LBB theory. Its weak form is given as follows: Consider $\vec H=H(\dive; \Omega)\cap \{\vec u\cdot \vec n=0\}$ and $Q=L^2(\Omega)$, then find $(u,p)$ in $H\times Q$ such that
    \[ \begin{aligned}
    (\ten K^{-1}\vec u, \vec v) - (\dive \vec v, p) &= \langle f,\vec v\rangle && \forall \vec v\in \vec H \\
    (\dive \vec u, q)  &= \langle g, q\rangle && \forall q \in Q,
    \end{aligned} \]
where $\ten K$ is symmetric and positive definite ($ k_1|\vec x|^2 \leq \vec x\cdot \ten K^{-1}\vec x \leq k_2 |\vec x|^2$), $f$ is in $\vec H'$ and $g$ is in $Q'$. We omit details regarding continuity as it is a simply application of the Cauchy-Schwartz inequality. The bilinear forms to be studied here are $a(\vec u, \vec v) = (\ten K^{-1}\vec u, \vec v)$ and $b(\vec v, q) = (\dive \vec v, q)$.
    \begin{itemize}
        \item First, we need to show that $\Pi A|_V$ is invertible. To better understand this operator, we recall that $\Pi$ is the orthogonal projector of $\vec H$ into $\vec V = \ker B$, so let's look into all the pieces to make sense out of it. First, $B:H\to Q$ is the operator given by 
        $$ (B\vec v, q) = (\dive \vec v, q),$$
        and thus, as $L^2$ can be identified with its dual, we have that simply $B = \dive$ and thus $\vec V = \{\vec v \in \vec H: \dive \vec v = 0\}$, i.e. the space of solenoidal functions in $H(\dive;\Omega)$ with null normal component on the boundary. Second, the projection $\Pi$ is surjective, and thus 
            \[ \langle \Pi A|_V \vec u, \vec v\rangle \]
        which is defined for all $\vec u, \vec v$ in $\vec H$, can be written analogously as 
            \[ \langle \Pi A \vec u, \vec v\rangle \]
        for $\vec u$ in $\vec V$ and $\vec v$ in $\vec H$, where $A\vec u$ belongs to $\vec H$. Finally, $\Pi:\vec H\to\vec V$ is an orthogonal projector, meaning that if we denote $\vec H = \vec V \oplus \vec V^\perp$ and $\vec v = \vec v_0 + \vec v^\perp$, we obtain
            \[  \langle A \vec u, \vec v_0\rangle \]
        with $\vec u$ in $\vec V$ and $\vec v_0$ in $\vec V$. In other words, the operator $\Pi A|_V$ is simply the restricted bilinear form $a: \vec V\times \vec V \to \R$, given by
        \[ a(\vec u, \vec v) = (\ten K^{-1}\vec u, \vec v) \qquad\forall \vec u, \vec v \in \vec V. \]
        In such a space, the form $a$ is elliptic: 
        \[ a(\vec u, \vec u) = (\ten K^{-1}\vec u, \vec u) \geq k_1 \| \vec u \|_0^2 = k_1 \| \vec u\|_{\dive}^2 \qquad\forall \vec u \in \vec V,\]
        where the term $\|\dive \vec u\|$ can be trivially added as $\dive \vec u = 0$ for $\vec u$ in $\vec V$. This shows that $\Pi A|_V$ is invertible using the Lax-Milgram lemma.
        \item We now show the inf-sup property. For this, we will use the technique of the auxiliary problem. Consider the following problem
        \[ \begin{aligned}
            - \Delta z &= q && \Omega \\
            \grad z\cdot \vec n &= 0 &&\partial\Omega,
        \end{aligned} \]
        which we have already shown to be invertible in the subspace of $H^1$ that is orthogonal to the constants. Thus, it holds that the function $\tilde{\vec u} = -\grad z$ is such that it belongs to $\vec H$ and satisfies that $\|\tilde{\vec u}\|_0 \leq C \| q\|_0$, which comes from the \emph{a-priori} (or stability) estimate. Also, as $\dive \tilde{\vec u} = q$, we have $\|\tilde{\vec u}\|_{\dive} \leq (C+1)\|q\|$. Going back to the original problem, we want to show that there exists $\beta>0$ such that
        \[ \sup_{\vec v\in \vec H}\frac{b(\vec v, q)}{\|\vec v\|} \geq \beta \|q\|_0 \qquad \forall q \in Q.\]
        Using our previously constructed solution, we obtain
        \[ \sup_{\vec v\in \vec H}\frac{b(\vec v, q)}{\|\vec v\|} \geq \frac{(\dive \tilde{\vec u}, q)}{\|\tilde{\vec u}\|_{\dive}}\geq \frac{\|q\|_0^2}{(C+1)\|q\|} = \tilde\beta \|q\|, \]
        where $\tilde \beta = 1/(C+1)$. This concludes the proof. We note that it was actually sufficient to show that for each $q$ there existed an element $\tilde{\vec u}$ in the desired space, as it proved the survectivity of the operator $B$, but showing the complete inf-sup estimate was more instructive. 
    \end{itemize}
    Given that we have shown the required properties for $a$ and $b$, then there exists a unique and stable solution $(u,p)$ of Darcy's problem. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Discretization of saddle point problems}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We consider finite dimensional and conforming spaces $\{H_h\}_h \subset H$ and $\{Q_h\}_h \subset Q$. Then, given $\vec F$ in $H'$ and $G$ in $Q'$, the discrete problem reads: Find $(u_h, p_h)$ in $H_h \times Q_h$ such that 
    \[ \begin{aligned}
        a(u_h, v_h) + b(v_h, p_h) &= \langle F, v_h\rangle &&\forall v_h \in H_h \\
        b(u_h, q_h)  &= \langle G, q_h\rangle &&\forall q_h \in Q_h. 
    \end{aligned} \]
The previous theory can be used in this context unchanged, with only some mild changes in the definition of the operators, which we do in the following before announcing the operators. Consider the induced operators $A_h:H_h\to H_h$ and $B_h: H_h\to Q_h$, defined using convenient Riesz operators as done previously, and define the kernel space $V_h = \ker B_h = \{v_h \in H_h: b(v_h, q_h) = 0 \quad\forall q_h \in Q_h\}$. 
\begin{theorem} Consider the orthogonal projection $\Pi_h: H_h \to Q_h$. Then, if 
    \begin{itemize}
        \item The operator $\Pi_h A_h|_{V_h}: V_h\to V_h$ is injective (or surjective), and
        \item the bilinear form $b:H_h\times Q_h\to \R$ satisfies and inf-sup condition, then
    \end{itemize}
for each pair of functions $(F, G)$ there is a unique solution $(u_h, p_h)$ in $H_h\times Q_h$ such that 
    \[ \| (u_h, p_h) \|_{H\times Q} \leq C_h\left( \|F|_{H_h}\|_{H_h'} + \| G|_{Q_h} \|_{Q_h'} \right), \]
where $C_h = C_h(\| A_h\|, \| (\Pi_h A)^{-1} \|, \beta_h)$. 
\end{theorem}
From this result, it is easy to see that the Galerkin projection $G_h: H\times Q \to H_h\times Q_h$ is well-posed: 
    \[\begin{aligned}
        a(\Pi_H \circ G_h(u,p), v_h) + b(v_h, \Pi_Q \circ G_h(u,p)) &= a(u, v_h) + b(p, v_h) && \forall v_h \in H_h\\
        b(\Pi_H \circ G_h(u,p), q_h) &= b(u, q_h) && \forall q_h \in Q_h,
    \end{aligned}\]
where $\Pi_H$ and $\Pi_Q$ are simply component projections, i.e. $\Pi_H(u,p) = u$ and $\Pi_Q(u,p) = p$. One can further prove a C√©a estimate: 
    \[ \| u - u_h\| \leq C_1 \|\inf_{\zeta_h \in H_h}\| u -\zeta_h \|_H + C_2\inf_{w_h\in Q_h}\| p -w_h\|\]
    \[ \|p - p_h\| \leq C_3 \|\inf_{\zeta_h \in H_h}\| u -\zeta_h \|_H + C_4\inf_{w_h\in Q_h}\| p -w_h\|.\]
Note that the discrete inf-sup condition \emph{does not} follow from the continuous one, so it is typically an additional difficulty during the analysis. Still, there is a classical lemma that allows to infer the discrete inf-sup in some conditions. 

\begin{lemma}[Fortin's Lemma]
Consider $b:H\times Q\to \R$ that satisfies an inf-sup condition with constant $\beta >0$. If there exists a famliy of discrete projectors $\Pi_h: H\to H_h$ such that 
    \[ \|\Pi_h \|\leq \tilde C \quad \forall h \qquad \text{ and } \qquad b(\Pi_h u, q_h) = b(u, q_h) \quad\forall u\in H, q_h \in Q_h,\]
then the discrete inf-sup of $b:H_h\to Q_h$ holds with $\tilde \beta = \beta / \tilde C$.
\end{lemma}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Beyond linearity}\label{sec:nonlinear}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We will now cover the notion of differentiability in Banach spaces. The presentation closely follows that of \cite{ambrosetti1995primer}, with substantially less detail.
\begin{definition}
    Set $u\in U \subset X$ with $U$ an open set, and $F:U\to Y$. We say $F$ is Fr√©chet differentiable at $u$ if there exists a linear operator $A\in\mathcal{L}(X,Y)$ such that
    \begin{equation*}
        F(u+h)=F(u)+A(h)+o(\|h\|),
    \end{equation*}
    where we denote $\mathcal{L}(X,Y)$ as the set of linear operators from $X$ to $Y$, and the residual term $o(h)$ corresponds to a function $f(h)$ such that $\|f(h)\|/\|h\| \to 0$ when $h\to 0$. We denote the Fr√©chet derivative at $u$ in the direction $h$ as $dF(u)[h] := A(h)$.
\end{definition}
This construction yields two properties about Fr√©chet differentiability:
\begin{enumerate}
    \item For a given $F:U\to Y$, $dF(u)$ is unique. To prove this, assume $A\neq B$ are two Fr√©chet derivatives of $F$, that is, 
    \begin{align*}
        F(x+h) &= F(x) + Ah + o(\|h\|) \\
        F(x+h) &= F(x) + Bh + o(\|h\|).
    \end{align*}
    Subtracting these equations, we have $Ah - Bh = o(\|h\|)$. Since $A\neq B$, there exists a direction $h^*$ such that $Ah^* \neq Bh^*$. Setting $h=th^*$, with $t\in \mathbb{R}$, we note that
    \begin{equation*}
        \frac{\|Ah-Bh\|}{\|h\|} = \frac{t\|Ah^*-Bh^*\|}{t\|h^*\|} = \text{const.} \nrightarrow 0,
    \end{equation*}
    since it does not depend on $\|h\|$, and thus $Ah - Bh \neq o(\|h\|)$, which is a contradiction. 
    \item If $F:U\to Y$ is Fr√©chet differentiable, then $F$ is continuous.
\end{enumerate}

Furthermore, we recover some classical properties of differentiation:
\begin{enumerate}
    \item (Linear combination) If $F$ and $G$ are Fr√©chet differentiable, then for any $a,b\in\mathbb{R}$, $aF+bG$ is Fr√©chet differentiable.
    \item (Chain rule) Let $F:U\to Y$ and $G:V\to Z$, with $U\subset X$ an open set and $F(U)\subset V\subset Y$. Then, if $F$ is Fr√©chet differentiable at $u\in U$ and $G$ is Fr√©chet differentiable at $F(u)\in V$, then the composition $G\circ F:U\to Z$ is Fr√©chet differentiable at $u$, and we have
        \begin{equation*}
            dG\circ F(u)[h] = dG(F(u))[dF(u)[h]].
        \end{equation*}
\end{enumerate}

We now define the Fr√©chet derivative map.
\begin{definition}
    Let $F:U\to Y$ be a Fr√©chet differentiable function in $U$ (i.e. Fr√©chet differentiable at every point $u\in U$). The map
    \begin{align*}
        F':U&\to \mathcal{L}(X,Y)\\
        u&\mapsto dF(u)
    \end{align*}
    is called the Fr√©chet derivative of $F$. If $F'$ is continuous, we say that $F\in C^1$ and write $F\in C^1(U,Y)$.
\end{definition}

We note that the definition of the Fr√©chet derivative gives no hints to actually compute it from a given $F$. We begin by defining a different notion of differentiability.
\begin{definition}
    We define the G√¢teaux derivative of $F:U\to Y$ at a fixed $u\in U$ as
    \begin{equation*}
        d_G F(u)[h] := \lim_{\varepsilon\to 0} \frac{F(u+\varepsilon h)-F(u)}{\varepsilon} = \frac{d}{d\varepsilon}\left.\left(F(u+\varepsilon h)\right)\right|_{\varepsilon = 0}.
    \end{equation*}
    This corresponds to the directional derivative of $F$ in the direction $h$.
\end{definition}
In order to show the equivalence between Fr√©chet and G√¢teaux derivatives, we require the mean value theorem. 

\begin{theorem}[Mean value]
Let $F:U\to Y$ be a function that is G√¢teaux differentiable in $U$ (i.e. at every point $u\in U$). Define the function interval (convex combination)
\begin{equation*}
    [u,v] := \{tu+(1-t)v, t\in[0,1]\}.
\end{equation*}
Then, we have
\begin{equation*}
    \|F(u)-F(v)\| \leq \left(\sup_{w\in[u,v]} \|d_G F(w)\|\right) \|u-v\|. 
\end{equation*}
\end{theorem}
\begin{proof}
    Assume that $F(u)-F(v)\neq 0$. We build the norm using the Hahn-Banach theorem: there exists $\psi \in Y^*$ with $\|\psi\|=1$ such that 
    \begin{equation*}
        \langle \psi, F(u)-F(v)\rangle = \|F(u)-F(v)\|. 
    \end{equation*}    
    Define $\gamma(t) = tu + (1-t)v$, with $t\in [0,1]$, and $h(t)=\langle \psi, F(\gamma(t))\rangle$. Then, by linearity, 
    \begin{equation*}
        \frac{h(t+\tau) - h(t)}{\tau} = \langle \psi, \frac{F(\gamma(t) + \tau(u-v)) - F(\gamma(t))}{\tau} \rangle.
    \end{equation*}
    By definition of the G√¢teaux derivative, we have
    \begin{equation*}
        \lim_{\tau\to 0} \frac{h(t+\tau) - h(t)}{\tau} = h'(t) = \langle \psi, d_G F(\gamma(t)) [u-v]\rangle, 
    \end{equation*}
    and the scalar mean-value yields $h(1)-h(0) = h'(\theta)$ for some $\theta\in(0,1)$. Computing all the terms, we get
    \begin{equation*}
        h(1)-h(0) = \langle \psi, F(u)-F(v)\rangle = \|F(u)-F(v)\|,
    \end{equation*}
    and 
    \begin{equation*}
        h'(\theta) = \langle \psi, d_G F(\gamma(\theta))[u,v]\rangle \leq \underbrace{\|\psi\|}_{=1} \|d_G F(\gamma(\theta))\| \|u-v\|,
    \end{equation*}
    where we used the Cauchy-Schwarz inequality followed by the continuity of $d_G F(\gamma(\theta))$. Taking $\sup$ over $\theta$ completes the proof. 
\end{proof}
\begin{theorem}[Equivalence of G√¢teaux and Fr√©chet derivatives]
    If the G√¢teaux derivative $d_G F$ of a function $F:X\to Y$ is continuous at $u^*\in X$, then $F$ is Fr√©chet differentiable at $u^*$, and they coincide, i.e. $dF(u^*) = d_G F(u^*)$.
\end{theorem}
\begin{proof}
    Because of uniqueness, we simply need to verify that the G√¢teaux derivative is indeed the Fr√©chet one. Fix $u\in U$ and define $R(h) = F(u+h) - F(u) - d_G F(u)[h]$. We now need to prove $R(h)=o(\|h\|)$, so that $Ah=d_G F(u)[h]$ in the definition of the Fr√©chet derivative. The G√¢teaux derivative of $R$ at $h$ in the direction $k$ is
    \begin{align*}
        d_G R(h)[k] &= \left.\frac{d}{d\varepsilon}\right|_{\varepsilon = 0} R(h+\varepsilon k) \\
        & = \left.\frac{d}{d\varepsilon}\right|_{\varepsilon = 0} F(u+h+\varepsilon k) - F(u) - d_G F(u)[h + \varepsilon k] \\
        & = d_G F(u+h)[k] - d_G F(u)[k].
    \end{align*}    
    Thus, by the mean value theorem, we get
    \begin{align*}
        \|R(h)\| &\leq \sup_{w\in [0,h]} \|d_G(R(w))\| \|h\|\\
        &=    \sup_{t\in [0,1]} \|d_G R(th)\| \|h\|\\
        &=    \sup_{w\in [0,h]} \|d_G F(u+h)[k] - d_G F(u)[k]\| \|h\|,
    \end{align*}
    and thus dividing by $\|h\|$ and taking the limit $\|h\|\to 0$ yields
    \begin{equation*}
        \lim_{\|h\|\to 0} \frac{\|R(h)\|}{\|h\|} \leq \lim_{\|h\|\to 0} \|d_G F(u+th) - d_G F(u)\| = 0,
    \end{equation*}    
    because of the continuity of $d_G F$.
\end{proof}
Typically, one computes the G√¢teaux derivative and hope it is continuous.


For higher order derivatives, we start from the Fr√©chet derivative $F'(u) := dF(u)\in \mathcal{L}(X,Y)$, where $F:U\to Y$. Repeating the calculation, we have
\begin{equation*}
    d^2 F(u) = dF'(u), 
\end{equation*} 
where $F': U\to \mathcal{L}(X,Y)$, and so $dF'(u) \in \mathcal{L}(X, \mathcal{L}(X,Y))$. Notably, the space $\mathcal{L}(X, \mathcal{L}(X,Y))$ is isometric to $\mathcal{L}(X\times X,Y)$ through the isometry
\begin{equation*}
    \Psi_A(u_1,u_2) = [A(u_1)](u_2),
\end{equation*} 
for $A\in \mathcal{L}(X,\mathcal{L}(X,Y))$. For this reason, most people write the second derivative as $d^2 F(u) [h_1,h_2]$, instead of $(d^2 F(u)[h_1])[h_2]$. In calculus of variations, it is common to study the second varition of a functional. This is simply $d^2 \Pi(u)[h,h]$, as seen in the second order term of the Taylor series
\begin{equation*}
    f(x+h)\approx f(x) + \nabla f(x) \cdot h + \frac{1}{2} h^\top (Hf)(x) h,
\end{equation*} 
where we wrote $d^2 f = Hf$ as the Hessian of $f$ at $x$.

Let us list some useful properties:
\begin{itemize}
    \item Set $F$ twice differentiable and define $F_h(u)=dF(u)[h]$. Then,
    \[ dF_h(u)[k] = F''(u)[h,k] \] 
    We can then compute the second derivative by fixing $h$ in the first derivative and differentiating like we've done in respect to $u$ for a direction $k$.
    \item If $F$ is twice differentiable, then $F''(u)\in\mathcal{L}(X\times X, Y)$ is symmetric.
    \item Partial derivatives are defined using projections, i.e. if $F: U\times V \to Y$, set $\sigma_v(u)=(u,v)$. Then, the partial derivatives w.r.t. $u$ is
    \[ d(F\circ \sigma_v)(u)[h], \]
    denoted simply $d_u F(u^*,v^*)$. Notably, one obtains
    \[ d_u F(u^*,v^*)[h] = d_G(F\circ \sigma_v)(u)[h] = \left.\frac{d}{d\varepsilon}\right|_{\varepsilon = 0} F(u+\varepsilon h, v) \]
\end{itemize}

\example{
    Let $\psi(u)=\frac{1}{p}\int_\Omega u^p dx$, defined in $L^p(\Omega)$.

    \begin{align*}
        d\psi(u)[v] &= \frac{1}{p}\frac{d}{d\varepsilon} |_{\varepsilon=0} \int_\Omega (u+\varepsilon v)^p dx \\
        &= \frac{1}{p} \int_\Omega p(u+\varepsilon v)^{p-1}v dx|_{\varepsilon=0} \\
        &= \int_\Omega u^{p-1}v dx
    \end{align*}

    \begin{align*}
        d^2\psi(u)[v_1,v_2] &= d(d F_{v_1}(u))[v_2] \\
        &= \frac{d}{d\varepsilon}|_{\varepsilon=0}\int_\Omega(u+\varepsilon v_2)^{p-1} v_1 dx\\
        &= (p-1)\int_\Omega u^{p-2}v_1 v_2 dx
    \end{align*}
}

In calculus of variations $d^2 F(u)[h] = d^2 F(u)[h,h]$ is used for checking the properties of a problem.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Local analysis}
Consider the set 
\[ \text{Inv}(X,Y) = \left\{ A\in \mathcal{L}(X,Y): A \text{ is invertible} \right\} \]

Then, we have the following local inversion result

\begin{theorem}
    Consider $F\in C^1(X,Y)$, $F'(u^*)\in\text{Inv}(X,Y)$. Then, $F$ is locally invertible at $u^*$ with $F^{-1}\in C^1$. In fact,
    \[ dF^{-1}(v) = (F'(u))^{-1}, \quad u=F^{-1}(v) \]
\end{theorem}

\example{
    \[
    \left\{
    \begin{array}{rc}
        -\Delta u + u^p  = h, &\Omega,\\
        u=0, &\partial\Omega,
    \end{array}\right.
    \]
    where $p>1$, $h\in H^{-1}(\Omega)$, so $F:X\to Y$ with $X\coloneqq H_0^1(\Omega)$ and $Y\coloneqq H^{-1}(\Omega)$. We compute
    \[ dF(u)[w] = -\Delta w + p u^{p-1} w \]
    We want to show that $dF(u)\in\text{Inv}(X,Y)$. That is, for a given $g\in Y$, we need to prove that there exists a $w\in X$ that solves
    \[ 
    a(w,v) = (\nabla w, \nabla v) + p(u^{p-1}w,v) = \langle g,v \rangle \quad \forall v\in X
    \]
    We've proven many times before the bound for the first term, so let us focus in the second:
    \[ 
    |(u^{p-1}w,v)|\leq \|u^{p-1} \|_{L^\infty} \|w\|_{0} \|v \|_{0} 
    \]
    Note that if $u\geq 0$, then $a$ is elliptic. Then, if $u\in X\cap L^\infty \cap \{u\geq 0\}$ we have that $dF(u)\in\text{Inv}(X,Y)$ and we can use the theorem.

}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Fixed point theorems}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The main idea is that certain non-linearities would be less terrible if we could fix one of the functions. For example, consider the problem of finding a function $u$ such that
\begin{equation}
\begin{aligned}
    -\Delta u &= \sin (u) &&\qquad \text{in $\Omega$}, 
    u &= 0 && \qquad \text{on $\partial\Omega$}.
\end{aligned}
\end{equation}
This problem is non-linear, but given $w$, finding $u$ such that
\begin{equation}\label{eq:fixed-point:lap u sin u}
     -\Delta u = sin(w)
\end{equation}
is linear on $u$. It also induces a mapping $w \mapsto T(w) = u$ such that, \emph{if it has a fixed point} $u=T(u)$, this fixed point solves the initial problem. For this section, we follow \cite{ciarlet2013linear,pata2019fixed}.


\textbf{Definition:} A Lipschitz function $f:X\to X$
\begin{align*}
    ||f(x)-f(y)||\leq L||x-y||
\end{align*}
is said to be:
\begin{itemize}
    \item $L = 1:$ Non-expansive.
    \item $L<1:$ A contraction.
\end{itemize}

\begin{theorem}[Banach Fixed Point]
 Let $X$ be a complete metric space and $f$ a contraction of constant $\lambda<1$. Then $f$ has a unique fixed point.

\begin{proof} Set $x^{n+1}=f(x^n)$, which we typically refer to as a \emph{Picard iteration}, and consider some initial $x^0\in X$. By induction we get that
\begin{align*}
     \|x^{n+1}-x^n\| &\leq \lambda \|x^n-x^{n-1}\|\leq \underbrace{...}_\text{induction} \lambda^n\|x^1-x^0\|\\
    \Rightarrow  \| x^{n+m}-x^n\| &\leq \|x^{n+m}-x^{n+m-1}\|+ ... \|x^{n+1}-x^n\|\\
    &\leq (\lambda ^{n+m-1}+...+\lambda^n)\|x^1-x^0\|\\
    &=\lambda^n\left(\displaystyle\sum_{j=0}^m\lambda^j\right)\|x^1-x^0\|\\
    &\leq\lambda^n\left(\displaystyle\sum_{j=0}^\infty\lambda^j\right)\|x^1-x^0\|\\
    &=\lambda^n\dfrac{1}{1-\lambda}\|x^1-x^0\|
\end{align*}
which implies that $\{x_n\}_n$ is Cauchy, and since $X$ is complete, this gives $x_n\to \bar{x}$. Finally, since $f$ is continuous, $f(\bar{x}) = \lim_{n\to \infty}f(x_n) = \lim_{x\to\infty}x_{n+1}=\bar{x}$.
\end{proof}
\end{theorem}

We note that if $X$ is compact, then this theorem can be extended to \textit{weak} contractions, ie $\|f(x)-f(y)\|< \|x-y\|$.

\example{Consider problem \eqref{eq:fixed-point:lap u sin u}, where we note that Lax-Milgram immediately yields the existence of a unique solution $u$ for each $w$. From the mean value theorem one has that differentiable functions satisfy $f(x) - f(y) = f'(\xi)(x-y)$ for some $\xi$ in $(x,y)$. This in particular shows that $\sin$ is a Lipschitz function as $\sup_{\xi\in \R}\sin(\xi) = 1$. Using this fact we can compute the following for some given $w_1, w_2$:  
    $$ -\Delta(T(w_1)-T(w_2))= \sin(w_1)-\sin(w_2),$$ 
    and the \emph{a-priori} estimate gives
    $$\|T(w_1)-T(w_2)\|\leq C\|\sin(w_1)-\sin(w_2)\|\leq C \|w_1-w_2\|.$$
Then, if $C<1$ there exists $\bar u$ such that $T(\bar u)= \bar u$. }

\begin{theorem}[Brouwer fixed point] Set $K$ a non-empty, compact and convex subset of a finite-dimensional Banach space. Then every continuous $f:K\to K$ has a fixed point.
\end{theorem}

\example{
Let's go back to \eqref{eq:fixed-point:lap u sin u}, and consider its Galerkin approximation for some given $w_h$:
\begin{align*}
    (\grad u_h,\grad v_h)=(\sin(w_h),v_h) \qquad \forall v_h \in V_h
\end{align*}
where $dim(V_h)<\infty$. Then, from the \emph{a-priori} bound we have 
\begin{align*}
    \|u_h\|_{1}\leq C||\sin (w_h)||< C
\end{align*}
using that $|\sin (x)|\leq 1$. This implies that $u_h$ is contained in a finite-dimensional ball of $V_h$, $\mathcal{B}(0,r), r=C$. To show that it is continuous, we consider two functions $w_1,w_2$ in $V_h$, and obtain the difference equation setting $u_i=T(w_i)$: 
    $$(\grad [u_1 - u_2], \grad v_h) = (\sin(w_1) - \sin(w_2), v_h) \qquad \forall v_h \in V_h. $$
Again, using the \emph{a-priori} bound we use $\sin' = \cos \leq 1$ to obtain 
    $$ \| u_1 - u_2\|_1 \leq \|w_1 - w_2\|_0.$$
This yields that $T_h:\mathcal{B}(0,r)\to \mathcal{B}(0,r)$ is continuous and thus has at least one fixed point.
}

From the previous result we can extend to $h\to 0$ through compactness. This can be generalized to another fixed point theorem known as Schauder's fixed point theorem, which has 2 forms.

\begin{theorem}[Schauder fixed point]
The following statements hold. 
\begin{enumerate}
    \item Set $K$ a compact, convex subset of $X$ normed space, and $f:K\to K$ a continuous mapping. Then $f$ has at least one fixed point.
    \item Let $\mathcal{C}$ a closed, convex subset of a Banach space $X$, and $f:\mathcal{C}\to \mathcal{C}$ continuous s.t. $\overline{f(\mathcal{C})}$ is compact. Then $f$ has at least one fixed point.
\end{enumerate}
\end{theorem}


\example{ 
Consider again \eqref{eq:fixed-point:lap u sin u}, with its induced Picard mapping $w \mapsto u\coloneqq T(w)$ given by 
\begin{align*}
    -\Delta u &= \sin(w) \text{ on }\Omega,\\
    u&=0\text{ on }\partial\Omega,
\end{align*}
whose solution is guaranteed by Lax-Milgram's Lemma, with $u$ in $H_0^1(\Omega)$.  From the \emph{a-priori} bound $\|u||\leq\dfrac{1}{\alpha}\|f\|$ we get $\|u\|_1\leq C\|\sin(w)\|_0\leq C|\Omega|=:r_0$. Our candidate set will be $\mathcal C = \bar{B}(0,r_0)\subset L^2(\Omega)$ which is closed, convex and contained in both $L^2(\Omega)$ and $H_0^1(\Omega)$. Now we are only missing that $\overline{T(\mathcal C)}$ is compact. 

Consider a sequence $(w_k)_{k>1}$ in $\mathcal C$, which yields $(v_k)_{k>1}=(T(w_k))_{k>1}$ in $\mathcal C$ and additionally $v_k\in H_0^1(\Omega)$ for all $k$. Now, we note that we have a sequence in $H^1$, and as the unit ball is weakly compact, we obtain that is has some weakly convergent sequence in $H^1$. The final detail is that, if we consider $\mathcal C$ to be a ball in $L^2$, then the mapping $T$ can be written additionally as $T\circ i$, where $i:L^2(\Omega)\to H^1(\Omega)$ is the compact embedding of $H^1$ into $L^2$. We thus have that $T\circ i$ is the composition of a continuous and a compact mapping, and thus a compact map. This in particular implies that our sequence $(v_k)$ has a strongly convergent subsequence in $L^2$. This concludes that $T:L^2(\Omega)\to L^2(\Omega)$ is compact, and thus we can use Schauder's fixed point theorem to show the existence of a fixed point $\bar u = T(\bar u)$ in $L^2(\Omega)$. Naturally, the equation itself reveals that actually $\bar u$ belongs to $H_0^1(\Omega)$, so we have some additional regularity on the fixed point we just found.
}
A consequence of the Schauder fixed point theorem is the Schaefer fixed point theorem. We state it for completeness without proof.
\begin{theorem}[Schaefer fixed point]
    Let $X$ be a Banach space and $f:X\to X$ a compact operator. If 
    \begin{equation*}
        \{x\in X: \sigma f(x)=x,\sigma\in[0,1]\}\subset B(0,r)
    \end{equation*}
    for some $r>0$, then there exists a fixed point of $f$.
\end{theorem}
    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Monotone operators}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Among nonlinear operators, there is a class of operators that provide efficient means to prove existence and uniqueness of solutions. These operators are called \textit{monotone}, and they have been extensively studied. We follow this section closely from \cite{ciarlet2013linear}.
\begin{definition}[Monotone operator]
    Let $V$ be a normed vector space and $\langle\cdot,\cdot\rangle$ its duality pairing. An operator $A:V\to V'$ is called monotone if
    \begin{equation*}
        \langle A(v)-A(u),v-u\rangle \geq 0 \qquad \forall u,v\in V,
    \end{equation*}
    and strictly monotone if the previous inequality is strict for $u\neq v$.
\end{definition}
We note that if $f:X\to \R$ is a convex and differentiable function, then $\nabla f$ is a monotone operator. Indeed, from convexity applied to $x,y\in X$ and adding we get
\begin{align*}
    f(x) &\geq f(y) + \langle \nabla f(y), x-y\rangle\\
    f(y) &\geq f(x) + \langle \nabla f(x), y-x\rangle\\
    \implies 0 &\geq \langle \nabla f(y)-\nabla f(x),x-y\rangle \\
    \implies 0&\leq \langle \nabla f(x)-\nabla f(y),x-y\rangle.
\end{align*}
We can also build monotone operators from an elliptic operator. If $A:V\to V'$ is an operator such that $a(u,v) = \langle A(u),v\rangle$ is continuous (linear) and elliptic, then $A$ is strictly monotone:
\begin{align*}
    \langle A(u)-A(v),u-v\rangle &= \langle A(u-v), u-v\rangle\\
    &= a(u-v,u-v)\\
    &\geq \alpha \|u-v\|^2 \geq 0,
\end{align*}
and the inequality is strict for $u\neq v$ since $\|\cdot\|$ is a norm. 

A very useful property can be derived from the monotonicity in the case that $V$ is complete.
\begin{theorem}
    If $V$ is a real Banach space and $A:V\to V'$ is monotone, then $A$ is locally bounded, i.e. for any $u\in V$ there exists $r=r(u)>0$ and $\rho=\rho(u)>0$ such that
    \begin{equation*}
        \|u-v\|\leq r \implies \|A(u)-A(v)\|\leq \rho,
    \end{equation*}
    which similar to a Lipschitz property. Moreover, if $A$ is linear, then $A$ is continuous.
\end{theorem}
We now introduce the notion of coercive ($\neq$ elliptic) and hemicontinuous operators.
\begin{definition}
    An operator $A:V\to V'$ is coercive if 
    \begin{equation*}
        \lim_{\|v\|\to \infty} \frac{\langle A(v),v\rangle}{\|v\|} = +\infty.
    \end{equation*}
\end{definition}
\begin{definition}
    An operator $A:V\to V'$ is hemicontinuous if for every $u,v,w\in V$ there exists $t_0=t_0(u,v,w)>0$ such that the map 
    \begin{align*}
        \varphi:(-t_0,t_0)&\to \R\\
        t&\mapsto \langle A(u+tv),w\rangle
    \end{align*}
    at $t=0$. 
\end{definition}
Hemicontinuity is a weaker property than continuity, as we shall prove in the following theorem.
\begin{theorem}
    If $A:V\to V'$ is continuous, then it is hemicontinuous.
    \begin{proof}
        Let $\varphi(t)=\langle A(u+tv),w\rangle$. Then, we get
        \begin{align*}
            |\varphi(t)-\varphi(0)| &= |\langle A(u+tv),w\rangle - \langle A(u),w\rangle|\\
            &\leq |\langle A(u+tv)-A(u),w\rangle| \tag{Linearity of $A$}\\
            &\leq \|A(u+tv)-A(u)\|_{V'} \|w\|_V. \tag{Cauchy-Schwarz}
        \end{align*}
        Thus, as $\|(u+tv)-u\|_V=\|tv\|\leq |t|\|u\|_V$, then $u+tv\overset{t\to 0}{\to} u$ in norm, and since $A$ is continuous, we get $A(u+tv)\overset{t\to 0}{\to} A(u)$ in $V'$. Thus, taking limit as $t\to 0$ we immediately conclude that $\varphi$ is continuous at $t=0$.
    \end{proof}
\end{theorem}
Now that we have defined hemicontinuous operators, we are ready to introduce a theorem that gives sufficient conditions that guarantee the surjectivity of a hemicontinuous monotone operator.
\begin{theorem}[Minty-Browder]
    Let $V$ a real, separable and reflexive Banach space (e.g. $L^p$ for $p>1$), and $A:V\to V'$ a coercive and hemicontinuous monotone operator. Then, $A$ is surjective, i.e. given any $f\in V'$ there exists $u\in V$ such that $A(u)=f$. Moreover, if $A$ is strictly monotone, then $A$ is also injective, and thus there exists a unique solution for $A(u)=f$.
\end{theorem}
A common example of an operator that can be analyzed via the Minty-Browder theorem is the p-Laplacian $-\Delta_p$, which for $p\geq 1$ is defined as the map
\begin{align*}
    -\Delta_p : W_0^{1,p}(\Omega)&\to W^{-1,q}(\Omega) = (W_0^{1,p}(\Omega))'\\
    v&\mapsto -\Delta_p v = -\nabla\cdot(|\nabla v|^{p-2}\nabla v),
\end{align*}
where $q$ is the conjugate exponent of $p$, such that $1/p + 1/q = 1$. Note that for arbitrary $u,v\in W_0^{1,p}(\Omega)$ the duality is given by 
\begin{equation*}
    \langle \Delta_p u, v\rangle = \langle \nabla\cdot (|\nabla u|^{p-2}\nabla u), v\rangle = -\langle |\nabla u|^{p-2}\nabla u, \nabla v\rangle = -\int_\Omega |\nabla u|^{p-2}\nabla u\cdot\nabla v dx,
\end{equation*}
which is well-defined by H√∂lder's inequality:
\begin{equation*}
    \left|\int_\Omega |\nabla u|^{p-2}\nabla u\cdot\nabla v dx\right| \leq \|\nabla u\|^{p-1}_{W_0^{1,p}(\Omega)}\|\nabla v\|_{W_0^{1,p}(\Omega)},
\end{equation*}
and noting that $w\mapsto \|\nabla w\|_{W_0^{1,p}(\Omega)}$ is a norm on $W_0^{1,p}(\Omega)$. We define now the functional
\begin{align*}
    \Psi:W_0^{1,p}(\Omega)&\to \R\\
    u&\mapsto \Psi(u):= \frac{1}{p}\int_\Omega |\nabla u|^p dx.
\end{align*}
This functional is G√¢teaux-differentiable, and we explicitly calculate its the G√¢teaux derivative for $u,v\in W_0^{1,p}(\Omega)$:
\begin{align*}
    d\Psi(u)[v] &= \left.\frac{d}{d\varepsilon}\right|_{\varepsilon=0} \Psi(u+\varepsilon v)\\
    &= \left.\frac{d}{d\varepsilon}\right|_{\varepsilon=0} \frac{1}{p}\int_\Omega \underbrace{|\nabla (u+\varepsilon v)|^p}_{|\nabla(u+\varepsilon v)\cdot \nabla(u+\varepsilon v)|^{p/2}} dx\\
    &= \left.\frac{1}{p}\int_\Omega \frac{p}{2} |\nabla (u+\varepsilon v)\cdot\nabla(u+\varepsilon v)|^{p/2-1}\cdot 2(\nabla u\cdot\nabla v) dx\right|_{\varepsilon = 0}\\
    &= \int_\Omega |\nabla u\cdot\nabla u|^{\frac{p-2}{2}}\nabla u\cdot\nabla v dx\\
    &= \int_\Omega |\nabla u|^{p-2}(\nabla u\cdot\nabla v) dx\\
    &= \langle -\Delta_p u, v\rangle,
\end{align*}
and thus $d\Psi = -\Delta_p$. This operator is hemicontinuous: take $t\in \R$ and $u,v,w\in W_0^{1,p}(\Omega)$, and define $\varphi(t) = \langle A(u+tv),w\rangle$. With this, we have
\begin{align*}
    |\varphi(t)-\varphi(0)| &= \left|\int_\Omega \left(|\nabla (u+tv)|^{p-2}\nabla (u+tv) - |\nabla u|^{p-2}\nabla u\right)\cdot \nabla w dx \right|\\
    &= \left|\int_\Omega \left((|\nabla u + t\nabla v|^{p-2} - |\nabla u|^{p-2})\nabla u \cdot \nabla w + t|\nabla u + t\nabla v|^{p-2} \nabla v\cdot \nabla w \right) dx\right|\\
    &\leq \int_\Omega \left(|\nabla u + t\nabla v|^{p-2} - |\nabla u|^{p-2}\right)|\nabla u||\nabla w| dx + |t|\int_\Omega |\nabla u + t\nabla v|^{p-2}|\nabla v||\nabla w| dx,
\end{align*}
and taking the limit $t\to 0$ we conclude that $-\Delta_p$ is hemicontinuous. Moreover, since $\Psi$ is strictly convex, we obtain that for all $u\neq v \in W_0^{1,p}(\Omega)$, it holds that
\begin{equation*}
    \langle \Delta_p u - \Delta_p v, u-v\rangle < 0 \implies \langle -\Delta_p u - (-\Delta_p v), u-v\rangle > 0,
\end{equation*}
which implies that $-\Delta_p$ is strictly monotone. To prove coercivity, we take a nonzero $u\in W_0^{1,p}(\Omega)$ and get
\begin{align*}
    \frac{\langle A(u),u\rangle}{\|u\|_{W_0^{1,p}(\Omega)}} &= \frac{1}{\|u\|_{W_0^{1,p}(\Omega)}}\int_\Omega \underbrace{|\nabla u|^{p-2}(\nabla u\cdot\nabla u)}_{|\nabla u|^p} dx\\
    &= \frac{1}{\|u\|_{W_0^{1,p}(\Omega)}}\|u\|_{W_0^{1,p}(\Omega)}^p\\
    &= \|u\|_{W_0^{1,p}(\Omega)}^{p-1}\xlongrightarrow{\|u\|_{W_0^{1,p}(\Omega)}\to\infty}\infty,
\end{align*}
since $p>1$, which proves that $-\Delta_p$ is coercive. Since $W_0^{1,p}(\Omega)$ is separable and reflexive for $p>1$, the conditions of the Minty-Browder theorem are satisfied, and thus we conclude that $-\Delta_p$ is bijective, i.e. for any $f\in W^{-1,q}(\Omega)$ there exists a unique $u\in W_0^{1,p}(\Omega)$ such that $-\Delta_p u = f$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Time dependent problems}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We now study time-dependent problems, where the solutions are functions $V\ni u:[0,T]\times \Omega\to \R$. In this section, we consider $\mathcal{L}:V\to V'$ an elliptic operator, and follow the presentations from \cite{thomee2007galerkin,quarteroni2008numerical}.

\begin{definition}[Parabolic and hyperbolic generic operators]
    Let $\partial_t$ and $\partial_{tt}$ represent the first and second order time derivative operators, respectively. Then, we say that
    \begin{align*}
        \partial_t + \mathcal{L} &\qquad \text{is parabolic,}\\
        \partial_{tt} + \mathcal{L} &\qquad \text{is hyperbolic.}
    \end{align*}
\end{definition}
The fundamental difference between parabolic and hyperbolic systems is the behaviour of their \textit{energy} $E(t)$. Let us formally derive this fact.

\begin{enumerate}
    \item Parabolic systems: we can readily write the weak form of the parabolic equation $(\partial_t + \mathcal{L})u=0$ as
    \begin{equation*}
        (\partial_t u, v) + (\mathcal{L}u, v) = 0 \qquad \forall v\in V.
    \end{equation*}
    We note that this weak formulation seems unbalanced as it is being tested only on the space variable. We will accept this for now, but see later on that it is indeed a good weak formulation for analyzing the problem using our knowledge of elliptic (and G√•rding) operators. Since this is true for all $v\in V$, we can choose $v=u$, and we obtain
    \begin{equation*}
        (\partial_t u, u) + (\mathcal{L}u,u) = 0.
    \end{equation*}
    Note that $\partial_t(u^2) = 2u\dot{u}$, and thus we can write $(\partial_t u, u) = \int_\Omega u\dot{u} = \frac{1}{2}\int_\Omega \partial_t (u^2),$ which leads to
    \begin{equation*}
        \frac{1}{2}\int_\Omega \partial_t (u^2) + \underbrace{\int \mathcal{L}u \cdot u}_{:= a(u,u)} = 0, 
    \end{equation*}
    and integrating in time in $[0,t]$ with $t\leq T$ we get
    \begin{equation*}
        \frac{1}{2} \int_\Omega (u(t)^2-u(0)^2)  + \int_0^t a(u,u)ds = 0,
    \end{equation*}
    where $u(0) = u(0,x)$ is a (fixed) initial condition. This implies that
    \begin{equation*}
        \frac{1}{2}\int_\Omega u(t)^2 =  \frac{1}{2} \int_\Omega u(0)^2 - \int_0^t a(u,u)ds.
    \end{equation*}
    Defining the energy as $E(t) := \int_\Omega u(t)^2$, we get
    \begin{equation*}
        \frac{1}{2}E(t) = \frac{1}{2}E(0) - \int_0^t \underbrace{a(u,u)}_{\geq \alpha\|u\|^2>0} ds \implies \boxed{E(t) < E(0)}.
    \end{equation*}
    We observe that energy decreases from the initial condition in a parabolic system. Thus, parabolic systems are called \textit{dissipative}.
    \item Hyperbolic systems: since we need a second-order time derivative, we note that $\partial_t(\dot{u}^2) = 2\dot{u}\ddot{u}$, and thus as before we can write
    \begin{equation*}
        \int_\Omega \ddot{u}v + \int_\Omega \mathcal{L}u\cdot v = 0 \qquad \forall v\in V.
    \end{equation*}
    Setting $v=\dot{u}$, we obtain
    \begin{equation*}
        \frac{1}{2}\int_\Omega \partial_t (\dot{u}^2) + \int_\Omega \mathcal{L}u\cdot \dot{u} = 0 \qquad \forall v\in V.
    \end{equation*}
    We now restrict ourselves to elliptic operators $\mathcal{L}$ that can be written as $\mathcal{L}=B^\top B$, such that $\mathcal{L}u\cdot v = Bu\cdot Bv$. This way, we see that 
    \begin{equation*}
        \partial_t (Bu)^2 = 2Bu \cdot B\dot{u} = 2\mathcal{L}u\cdot \dot{u}.
    \end{equation*}
    As before, we integrate in time and get 
    \begin{equation*}
        \frac{1}{2} \int_\Omega \left[\dot{u}^2(t) + (Bu(t))^2\right] = \frac{1}{2} \int_\Omega \left[\dot{u}^2(0) + (Bu(0))^2\right].
    \end{equation*}
    Now, setting the energy as $E(t) := \int_\Omega \left[u(t)^2 + (Bu(t))^2\right]$, we conclude that $E(t) = E(0)$. Thus, hyperbolic systems are \textit{conservative}.
\end{enumerate}
The parabolic initial value problem is given by:
\begin{equation*}
    \begin{aligned}
        \partial_t + \mathcal{L}u &= f &&\quad \text{in }\Omega_T := (0,T) \times \Omega\\
        \hfill Bu &= g &&\quad \text{in }\Sigma_T := (0,T) \times \partial\Omega\\
        \hfill u(0,x)&= u_0(x) &&\quad \text{in }\Omega,
    \end{aligned}
\end{equation*}
with $f,g:\Omega_T\to \R$ and $u_0:\Omega\to\R$. Let us analyze how to deal with the time dependence of our system by introducing the Bochner integral.

\begin{definition}[Bochner integral]
We seek to integrate functions $f:\R\to X$, where $X$ is a Banach space. We extend the notion of simple functions
\begin{equation*}
    f^N(t) = \sum_{i=1}^N \lambda_i \phi_i(t),
\end{equation*}
where $\lambda_i \in X$ $\forall i$, and $\phi_i(t)$ are indicator functions. If $I\subset \R$, then the time integral yields
\begin{equation*}
    \int_I f^N(t) dt = \sum_{i=1}^N \lambda_i \int_I \phi_i(t) dt.
\end{equation*}
Here, if we have absolute convergence of the corresponding series, i.e. 
\begin{equation*}
    \sum_{i=0}^{\infty} \|\lambda_i\|_X \int_I \phi_i(t)dt <\infty,
\end{equation*}
and if $f(t) = \sum_{i=1}^{\infty} \lambda_i \phi_i(t)$ for every $t$ where the series converges, then we say $f$ is Bochner-integrable, and
\begin{equation*}
    \int_0^t f(s)ds = \sum_{i=1}^\infty \lambda_i \int_0^t \phi_i(s)ds.
\end{equation*}    
\end{definition}
\begin{remark}
    A useful property: if $f$ is Bochner-integrable, then $|f|$ is Lebesgue-integrable. 
\end{remark}

We now define the Bochner spaces we will be using for our analysis:
\begin{equation*}
    L^p(0,T; X) := \left\{v:(0,T)\to X: v \text{ is Bochner-integrable}, \int_0^T \|v\|_X^p ds <\infty \right\}.
\end{equation*}
\begin{equation*}
    H^1(0,T; X) := \left\{v:(0,T)\to X: v\in L^2(0,T;X), \partial_t v\in L^2(0,T;X) \right\}.
\end{equation*}
In general, $\partial_t v$ should be interpreted as an element of $V'$, since the weak form we will be using is, given $f:\Omega_T\to \R$,
\begin{equation*}
    (\dot{u},v) + a(u,v) = \langle f, v\rangle \quad \forall v\in V,
\end{equation*}
and thus $\dot{u}\in V'$ for the first term to exist. The natural definition that gives sense to this object is as follows: setting $X=L^2(\Omega)$ and $L^2(\Omega_T) := L^2(0,T; L^2(\Omega))$, we denote
\begin{equation*}
    H^1(0,T;L^2(\Omega)) := \left\{v:(0,T)\to X: v\in L^2(\Omega_T), \partial_t v\in L^2(\Omega_T) \right\}.
\end{equation*}
This yields a norm equivalence:
\begin{align*}
    \|v\|_{L^2(\Omega_T)} &= \int_0^T \|v(s)\|_{0,\Omega}^2 ds\\
    &= \int_0^T \left(\left(\int_\Omega v(s,x)^2 dx\right)^{1/2}\right)^2ds\\
    &= \int_0^T \int_\Omega v^2 dxds\\
    &= \int_{\Omega_T} v^2\\
    &= \|v\|^2_{L^2(\Omega_T)}.
\end{align*}
We now state an embedding theorem for Bochner spaces.
\begin{theorem}
    Let $\Omega\subseteq \R^d$ be a Lipschitz domain, $s\geq 0$ and $r>1/2$. For any $\theta\in[0,1]$, the following embedding is continuous:
    \begin{equation*}
        L^2(0,T;H^s(\Omega))\cap H^r(0,T;L^2(\Omega)) \longrightarrow H^{\theta r}(0,T;H^{(1-\theta)s}(\Omega))\cap C^0(0,T;H^{\sigma_0}(\Omega)),
    \end{equation*}
    with $\sigma_0 = \frac{(2r-1)s}{2r}$. Furthermore, if $s>0$ and $|\Omega|<\infty$, then the following is compact:
    \begin{equation*}
        L^2(0,T;H^s(\Omega))\cap H^r(0,T;L^2(\Omega)) \longrightarrow H^{r_1}(0,T;H^{s_1}(\Omega))\cap C^0(0,T;H^{\sigma_1}(\Omega)),
    \end{equation*}
    for any $s_1\geq 0$, $0\leq r_1 < r(1-s_1/s)$ and $0\leq \sigma_1 < \sigma$.
\end{theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Faedo-Galerkin and the method of lines}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Space and time discretization}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
As usual, we consider an approximation space $V_h\subset V$  and compute first the semi-discrete problem: 

\begin{equation}\label{eq:semi-discrete in time}
    \begin{aligned}
        (\dot u_h(t), v_h) + a(u_h(t), v_h) &= (f(t),v_h) &&\qquad \forall v_h\in V_h, t \in(0,T)  \\
                                u_h(0)&= \Pi_h(u_0), 
    \end{aligned}
\end{equation}
where $\Pi_h:V\to V_h$ is an orthogonal projector. In what follows, for any given bilinear form $a:V\times V\to \R$ we well denote the \textbf{Ritz projector} as the operator $R_h:V\to V_h$ such that 
    $$ a(R_h z, v_h) = a(z, v_h) \qquad \forall v_h \in V_h. $$
In particular, the \emph{a-priori} bound yields $\|R_h z \| \leq \| z \|$.  We first prove the following convergence result for the semi-discrete problem: 

\begin{theorem}
In problem \eqref{eq:semi-discrete in time}, if both $u(t)$ and $\dot u(t)$ belong to $H^1(\Omega)$ for all $t$ in $(0,T)$, then 
    $$ \| u(t) - u_h(t) \|_V \leq \|u_0 - \Pi_h(u_0)\|_v + Ch^r\left(\|u_0\|_r+\int_0^t\|\dot u(s)\|_r\,ds\right). $$
\begin{proof}
The methodology consists in separating the error into \emph{projection} and \emph{consistency} errors: 
    $$ e_h \coloneqq u - u_h = \underbrace{u - \Pi_h u}_{\xi_h} + \underbrace{\Pi_h u - u_h}_{\eta_h} = \xi_h + \eta_h. $$
We now consider the error equation: 
    $$ (\dot e_h, v_h) + a(e_h, v_h) = 0 \qquad \forall v_h\in V_h, $$
and we consider as the projector the Ritz projector, i.e. $\Pi_h=R_h$, which gives
    $$ (\dot \xi_h + \dot \eta_h, v_h) + a(\eta_h, v_h) = 0 \qquad \forall v_h\in V_h. $$
Setting $v_h = \eta_h$, we obtain 
    $$ \frac 1 2 \partial_t(\|\eta_h\|^2_0)+a(\eta_h,\eta_h) = -(\dot \xi_h, v_h). $$
Note that 
    $$ \frac 1 2 \partial_t(\|\eta_h\|_0^2) = \|\eta_h\|_0 \partial_t(\|\eta_h\|_0), $$
and thus using the ellipticity of $a$ we obtain the following: 
    $$ \|\eta_h\|\partial_t(\|\eta_h\|_0) \leq \partial_t(\|\eta\|_0^2) + a(\eta_h, \eta_h) \leq \|\dot \eta_h\|_0 \|v_h\|_0, $$
which after dividing by $\|v_h\|_0$ and integrating yields
    $$ \| \eta_h(t)\|_0 \leq \|\eta_h(0)\|_0 + \int_0^t \| \dot \eta_h(s) \|_0\,ds. $$
Bounding each of the terms appearing gives the remaining estimates: 
    \begin{align*}
        \| \eta_h(0) \|_0 &= \|\Pi_h u_0 - R_h u_0 \|_0 \leq \|u_0 - \Pi_h\|_0 + \| u_0 - R_h u_0\|\\ 
        \| \dot \xi_h \|_0 &= \| \dot u - \R_h \dot u \|.
    \end{align*}
The resulting estimate comes from the convergence rate obtained from the Ritz projector. Another common choice is the Scott-Zhang projector. Another common way for deriving this type of estimate is using the Gronwall inequality.
\end{proof}
\end{theorem}
We finally derive a convergence estimate for the fully-discrete problem. [TODO]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Continuum mechanics}\label{sec:continuum}
A common application of the numerical analysis of PDEs is continuum mechanics, which consists of a framework of modeling mechanics through collections of deformable bodies. In this section, we set the background space $\mathcal{E} = \R^3$ as the standard Euclidean space. 

\subsection{Tensor calculus, kinematics and strain}

\paragraph{Preliminary definitions} As usual, we introduce the Einstein notation, where repeated indexes are added as $\vec X = X_i\vec e_i = X_1\vec e_1 + X_2\vec e_2 + X_3\vec e_3$. In this sense, the components of a vector $\vec X$ are the scalar values $X_j$ for $j=1,2,3$ and the components of a tensor $\ten M$ are the scalar values $M_{ij}$ for $i,j=1,2,3$. We define an additional operation called the \textit{tensor product} $\otimes$ between two vectors (not necessarily of the same length), where 
\begin{align*}
    \otimes: \R^m\times\R^n &\to \R^{m\times n}\\
    (\vec a,\vec n) \mapsto \vec a\otimes\vec b := \vec a\vec b^\top.
\end{align*}
With this construction, the object $\vec a\otimes\vec b$ is a rank-2 tensor, which can be represented via its coefficient matrix. 
\begin{definition}[Configuration]
    We define the \textit{configuration} of a body $\Omega\subset \R^3$ as the physical space it occupies at time $t$. By convention, we set the reference (initial) time $t=0$, where the body occupies a known \textit{reference configuration} $\Omega_0$ with a known physical state, such as its velocity or physical boundary conditions. We assume $\Omega_0$ is a bounded, open and connected set.
\end{definition}
From the above definition, we see that the numerical analysis of any PDE we define in $\Omega_0$ will have to take into account the fact that the domains of integration themselves are subject to change, which will motivate several results below. Before continuing, it is essential to introduce the two frames of reference that will be important when analyzing our problems, which are the material (Lagrangian) frame and the spatial (Eulerian) frame. 
\begin{definition}[Material and spatial frames]
    The material, Lagrangian or reference frame corresponds to the reference configuration $\Omega_0$, with material coordinates $X_i$ that are written in uppercase symbols. The spatial, Eulerian or current frame corresponds to the configuration $\Omega_t$ at a time $t>0$, with spatial coordinates $x_i$ that are written in lowercase symbols. 

    A body is composed of \textit{material points}, which are assigned material coordinates $\vec X=X_i\vec e_i$ where $\{\vec e_i\}_i$ is an orthonormal basis of $\mathcal{E}$. After deformation and at time $t$, we have \textit{spatial points}, which are assigned spatial coordinates $\vec x = x_i\vec e_i$
\end{definition}
Since the two coordinates are related by a mapping without any constraints on its shape, we have to distinguish between differentiation in the material and spatial coordinates. To this end, we use uppercase and lowercase notation for each case. Explicitly, the gradient, divergence and curl operators in material coordinates are symbolized in nabla and text notation as 
\begin{equation*}
    \nabla_X \equiv \text{Grad} \qquad \nabla_X\cdot \equiv \text{Div} \qquad \nabla_X \times \equiv \text{Curl},
\end{equation*}
and in spatial coordinates, we denote them as 
\begin{equation*}
    \nabla_x \equiv \text{grad} \qquad \nabla_x\cdot \equiv \text{div} \qquad \nabla_x \times \equiv \text{curl}
\end{equation*}


With these definitions, we are ready to define the vector field that correspond to the physical movement of the material points. 
\begin{definition}[Deformation function]
    The deformation function $\vec\varphi:\Omega_0\times\R^+ \to \Omega_t$ is the differentiable, injective and orientation-preserving\footnote{We say that the mapping preserves orientations if $\det\nabla_X\vec\varphi)>0$.} vector-valued mapping that describes the movement of point $\vec X$ in the material frame to point $\vec x$ in the spatial frame at time $t$, as 
    \begin{equation*}
        \vec x = \vec\varphi(\vec X,t) = \vec\varphi_t(\vec X)\in\Omega_t.
    \end{equation*}
    We now omit the explicit dependence on $t$ unless strictly necessary. We also define the following maps: 
    \begin{itemize}
        \item The displacement field is $$\vec u(\vec X) = \vec\varphi(\vec X) - \vec X = \vec x - \vec X.$$ 
        \item The gradient deformation tensor is $$\ten F(\vec X) = \nabla_X \vec\varphi(\vec X) = \nabla_X \vec u(\vec X) + \ten I,$$ 
        whose components are $F_{ij}(\vec X) = \partial_j \varphi_i(\vec X) = \varphi_{i,j}(\vec X)$.
    \end{itemize}
\end{definition}
We now characterize some types of generic deformation maps. 
\begin{itemize}
    \item A deformation map $\vec\varphi$ is an affine deformation if $\vec\varphi(\vec X) = \vec a + \ten M \vec X$, where $\vec a$ represents the translation and $\ten M$ is a fixed tensor. 
    \item A deformation map $\vec\varphi$ corresponds to rigid motion if $\vec \varphi(\vec X) = \vec a + \ten Q \vec X$, where $\ten Q$ is an orthogonal tensor with $\det\ten Q = \pm 1$ that represents rotation.
\end{itemize}
To differentiate and derive useful properties that will simplify the calculations later on, we recall the cofactor matrix of $\ten A\in\R^{m\times n}$ is
\begin{equation*}
    \text{Cof }(\ten A)_{ij} := (-1)^{i+j}\det \ten A'_{ij},
\end{equation*}
where $\ten A'_{ij}\in\R^{(m-1)\times(n-1)}$ is the same tensor after removing row $i$ and column $j$. Expanding this calculation we can deduce that 
\begin{equation*}
    \text{Cof }(\ten A) := \det(\ten A)\ten A^{-\top}, 
\end{equation*}
and thus
\begin{equation*}
    \ten A\text{Cof}(\ten A)^\top = (\det\ten A)\ten I.
\end{equation*}

\paragraph{Deformation metrics} Given a deformation map $\vec\varphi$ and a material point $\vec X$, we would like to precisely calculate the local deformation of line, surface and volume elements around $\vec X$. To this end, introduce the small perturbation $d\vec X$ with $\|d\vec X\|<<1$. By Taylor's theorem we can expand $d\vec x$ as
\begin{equation*}
    d\vec x = \vec\varphi(\vec X + d\vec X) - \vec\varphi(\vec X) \approx \vec\varphi(\vec X) + \nabla_X\vec\varphi(\vec X)\cdot d\vec X - \vec\varphi(\vec X) = \ten F d\vec X,
\end{equation*}
and thus the gradient deformation tensor $\ten F$ contains information about line element deformation. Directly from this, we note that distances change as 
\begin{equation*}
    \|d\vec x\| = \sqrt{d\vec x^\top d\vec x} = \sqrt{d\vec X^\top \ten F^\top \ten F d\vec X} = \sqrt{d\vec X^\top \ten C d\vec X},
\end{equation*}
where we have defined the \textit{Cauchy-Green right deformation tensor} $\ten C = \ten F^\top \ten F$. This tensor is clearly symmetric, and due to the orientation preservation of $\vec\varphi$, is positive definite. The calculation above does not quite compare $\|d\vec x\|$ with $\|d\vec X\|$ because of the square root. Thus, we square the above expression and subtract:
\begin{equation*}
    \|d\vec x\|^2 - \|d\vec X\|^2 = d\vec X^\top \ten C d\vec X - d\vec X^\top d\vec X = d\vec X^\top \underbrace{(\ten C - \ten I)}_{2\ten E} d\vec X,
\end{equation*}
where we now define the Euler-Lagrange deformation tensor $\ten E = \frac{1}{2}(\ten C - \ten I)$. Since $\ten F = \ten I + \nabla_X \vec u$, we get
\begin{equation*}
    \ten E = \frac{1}{2}(\nabla_X \vec u + (\nabla_X \vec u)^\top + (\nabla_X \vec u)^\top (\nabla_X\vec u)),
\end{equation*}
and expanding the square root from the definition of the norm of $d\vec x$ above, we get
\begin{align*}
    \|d\vec x\| &= \sqrt{d\vec X^\top \ten C d\vec X}\\
    &= \sqrt{d\vec X^\top d\vec X + 2d\vec X^\top \ten E d\vec X} \tag{$\ten C = 2\ten E + \ten I$}\\
    &\approx \sqrt{d\vec X^\top d\vec X} + \frac{1}{2}\frac{1}{\sqrt{d\vec X^\top d\vec X}}2d\vec X^\top \ten E d\vec X^\top \tag{Taylor expansion of $\sqrt{\cdot}$ around $d\vec X^\top d\vec X$}\\
    &= \|d\vec X\| + \frac{1}{\|d\vec X\|} d\vec X^\top \ten E d\vec X,
\end{align*}
which implies 
\begin{equation*}
    \|d\vec x\| - \|d\vec X\| \approx \frac{d\vec X^\top \ten E d\vec X^\top}{\|d\vec X\|} \implies \frac{\|d\vec x\| - \|d\vec X\|}{\|d\vec X\|} \approx \frac{d\vec X^\top \ten E d\vec X}{d\vec X^\top d\vec X}.
\end{equation*}
Indeed, when $\|\nabla_X \vec u\|<<1$, $\ten F = \ten I + \nabla_X \vec u$ is a small perturbation from the identity tensor, implying $(\nabla_X \vec u)^\top \nabla_X \vec u \approx 0$, and thus
\begin{align*}
    \ten E &= \frac{1}{2}(\nabla_X \vec u + (\nabla_X \vec u)^\top + (\nabla_X \vec u)^\top (\nabla_X\vec u))\\
    &\approx \frac{1}{2}(\nabla_X \vec u + (\nabla_X \vec u)^\top) =: \ten \varepsilon,
\end{align*}
where $\ten\varepsilon$ is the \textit{infinitesimal deformation tensor}, which is used often in the mechanics of solids under small deformations. One can define many other deformation tensors, such as the left Cauchy-Green deformation tensor $\ten B = \ten F\ten F^\top$, which are useful in other contexts. 

Let us now check how a simple unidirectional deformation is treated under this framework. Assume $\vec e_1 = (1,0,0)$, and we seek to see how $\vec X + (dX_1,0,0)$ changes under $\vec\varphi$. Let us denote the relative norm change $(\|d\vec x\| - \|d\vec X\|)/\|d\vec X\|$ in the $\vec e_1$ direction as $\rho_1$. We have $\|d\vec X\|^2 = d\vec X^\top d\vec X = dX_1^2$, and 
\begin{equation*}
    \|d\vec x\|^2 = \begin{bmatrix}
        dX_1 & 0 & 0
    \end{bmatrix}\ten C \begin{bmatrix}
        dX_1\\ 0\\ 0 = C_{11}dX_1^2,
    \end{bmatrix}
\end{equation*}
which combined imply $\|d\vec x\| = \sqrt{C_{11}} dX_1 = \sqrt{2E_{11} + 1}dX_1$. Thus,
\begin{equation*}
    \rho_1 = \frac{\|d\vec x\| - \|d\vec X\|}{\|d\vec X\|} = \frac{(\sqrt{2E_{11} + 1} - 1)dX_1}{dX_1} = \sqrt{2E_{11}+1}-1,
\end{equation*}
and when deformations are small, $|E_{11}|<<1$, which results in 
\begin{equation*}
    \rho_1 = \sqrt{2E_{11} + 1} - 1 \approx E_{11} \approx \varepsilon_{11}.
\end{equation*}
Similarly, the shear deformation in the plane $X_1-X_2$ corresponds to the angle change $\gamma_{12}$ between $dX_1$ and $dX_2$ in that plane, and we can deduce that 
\begin{equation*}
    \sin(\gamma_{12}) = \frac{2E_{12}}{\sqrt{1+2E_{11}}\sqrt{1+2E_{22}}} \implies \gamma_{12}\approx 2E_{12}\approx 2\varepsilon_{12}.
\end{equation*}
With this information, we conclude that for small deformations, $\ten E \approx \ten \varepsilon$, where $\varepsilon_{11}, \varepsilon_{22},\varepsilon_{33}$ represent the relative extension of the body in the corresponding direction. The remaining off-diagonal components represent the shear deformations that can be reinterpreted as angle changes. 
\paragraph{Principal directions and deformation invariants}
Given a material line oriented in the (unit) direction $\vec m$ in the reference configuration, i.e. $d\vec X = \|d\vec X\|\vec m$, we seek to deduce how much has the actual configuration stretched from the original configuration. We define the quotient $\Delta$ as 
\begin{equation*}
    \Delta(\vec m) := \frac{\|d\vec x\|^2}{\|d\vec X\|^2} = \frac{\|d\vec X\| \vec m^\top \ten C\vec m \|d\vec X\|}{\|d\vec X\|^2} = \vec m^\top \ten C\vec m.
\end{equation*}
Thus, the direction of maximum (or analogously, minimum) stretch $\Delta(\vec m)$ follows from the optimization problem
\begin{equation*}
    \max_{\vec m\in\R^3, \vec m^\top\vec m = 1}\Delta(\vec m).
\end{equation*}
Since this restriction is easy to model, we introduce the Lagrange multiplier $\lambda$ and the Lagrangian 
\begin{equation*}
    L(\vec m,\lambda) = \Delta(\vec m) - \lambda(\vec m^\top \vec m - 1),
\end{equation*}
whose first-order condition for optimality is
\begin{equation*}
    0 = \frac{\partial L}{\partial\vec m} = 2\ten C\vec m - 2\lambda\vec m \implies \ten C\vec m = \lambda\vec m.
\end{equation*}
The pairs $(\lambda,\vec m)$ are called the \textit{principal stretch} and \textit{principal direction} of the deformation $\vec\varphi$, and correspond to eigenvalues and eigenvectors of $\ten C$. Since $\ten C$ is symmetric and positive definite, the three principal stretches $\lambda_i$, $i=1,2,3$ are all positive, and their corresponding principal directions $\vec m_i$ form an orthogonal basis, i.e. $\vec m_i\cdot\vec m_j = \delta_{ij}$. This diagonalization procedure induces the spectral decomposition of $\ten C$ as 
\begin{equation*}
    \ten C = \sum_{i=1}^3 \lambda_i \vec m_i\otimes\vec m_i,
\end{equation*}
with $\lambda_1\geq \lambda_2\geq \lambda_3$, and thus the maximum of $\Delta(\vec m)$ is $\lambda_1$ in the direction $\vec m_1$, while the minimum is $\lambda_3$ in the direction $\vec m_3$. 

The above description allows us to compute 1D deformation metrics. For 3D, let $d\vec X$, $d\vec Y$ and $d\vec Z$ be three infinitesimal vectors in the reference configuration, which span a parallelepiped of volume $dV = [d\vec X, d\vec Y, d\vec Z]$, where $[\vec a, \vec b, \vec c] := \vec a \cdot (\vec b \times \vec c)$ is the triple product. Let $d\vec x, d\vec y, d\vec z$ be the corresponding infinitesimal vectors in the spatial configuration, which span a volume $dv = [d\vec x, d\vec y, d\vec z]$. The volume rate of change $J$ is called \textit{Jacobian} and is given by
\begin{equation*}
    J := \frac{dv}{dV} = |\det\ten F|.
\end{equation*}
For 2D deformation metrics, we consider $d\vec X$ and $d\vec Y$, which are mapped to $d\vec x$ and $d\vec y$ in the spatial configuration, respectively. Let $dA = \|d\vec X\times d\vec Y\|$ and $da = \|d\vec x\times d\vec y\|$, and define the \textit{unit normal vectors}
\begin{equation*}
    \vec N = \frac{d\vec X\times d\vec Y}{\|d\vec X\times d\vec Y\|} = \frac{d\vec X\times d\vec Y}{dA}, \qquad \vec n = \frac{d\vec x\times d\vec y}{\|d\vec x\times d\vec y\|} = \frac{d\vec x\times d\vec y}{da}.
\end{equation*}
We can relate the area differentials $dA$ and $da$ via the Nanson-Piola formula, which states that 
\begin{equation*}
    \vec n da = J\ten F^{-\top} \vec N dA.
\end{equation*}
\begin{definition}[Invariants of a tensor]
    Let $\ten Q$ be an orthogonal tensor and $\ten A$ be an arbitrary tensor. We say that a function $I(\ten A)$ is an invariant of $\ten A$ if and only if $I(\ten Q\ten A\ten Q^\top) = I(\ten A)$. In our setting, we can reduce to the case $\ten A = \ten C$ symmetric and positive definite, and here we have the invariants
    \begin{align*}
        I_1(\ten C) &= \tr\ten C = \lambda_1 + \lambda_2 + \lambda_3\\
        I_2(\ten C) &= \tr \text{Cof}(\ten C) = \lambda_1\lambda_2 + \lambda_1\lambda_3 + \lambda_2\lambda_3\\
        I_3(\ten C) &= \det\ten C = \lambda_1\lambda_2\lambda_3 = J^2.
    \end{align*}
\end{definition}

\paragraph{Deformation rates} Let us recall that the movement described by $\vec\varphi$ changes in space, which we have reviewed earlier by studying $\vec\varphi(\vec X + d\vec X)$, but also changes in time, since $\vec x =\vec \varphi(\vec X, t)$. We seek to understand how $\vec x$ changes with time. 
\begin{definition}[Velocity and acceleration]
    In the material frame, the material velocity $\vec V$ and the material acceleration $\vec A$ are defined as the time derivative of $\vec\varphi$, that is, 
    \begin{align*}
        \vec V(\vec X, t) := \dot{\vec \varphi}(\vec X, t) &= \frac{\partial \vec\varphi(\vec X, t)}{\partial t}\\
        \vec A(\vec X, t) := \ddot{\vec \varphi}(\vec X, t) &= \frac{\partial^2 \vec\varphi(\vec X, t)}{\partial t^2} = \dot{\vec v}(\vec X, t).
    \end{align*}
    These vector fields are functions of $\vec X$, and thus they can be evaluated at material points. To evaluate in spatial points $\vec x$, we naturally define
    \begin{align*}
        \vec v(\vec x, t) := (\vec V \circ \varphi^{-1})(\vec x, t) &= V(\vec \varphi^{-1}(\vec x, t), t)\\
        \vec a(\vec x, t) := (\vec A \circ \varphi^{-1})(\vec x, t) &= A(\vec \varphi^{-1}(\vec x, t), t).
    \end{align*}
\end{definition}
These definitions are correct in practice, but the second derivative requires more caution. To see this, let $\psi=\psi(\vec X,t)$ be a scalar field defined in the Lagrangian frame. Its total time derivative is, by the chain rule, given by
\begin{align*}
    \frac{\mathrm{d}\psi(\vec X, t)}{\mathrm{d}t} &= \frac{\partial\psi(\vec X, t)}{\partial t } + \nabla_X \psi(\vec X, t)\cdot \underbrace{\frac{\partial\vec X}{\partial t}}_{=0} \tag{$\vec X$ does not depend on $t$}\\
    &= \frac{\partial\psi(\vec X, t)}{\partial t }.
\end{align*}
Thus, the material acceleration is simply 
\begin{equation*}
    \vec A(\vec X, t) = \frac{\mathrm{d}V(\vec X, t)}{\mathrm{d}t} = \frac{\partial V(\vec X, t)}{\partial t} = \frac{\partial^2 \vec\varphi(\vec X, t)}{\partial t^2}.
\end{equation*}
By contrast, if $\psi=\psi(\vec x, f)$ is a scalar field defined in the Eulerian frame, we now have to keep the $frac{\partial\vec x}{\partial t}$ term due to the definition $\vec x = \vec\varphi(\vec X, t)$, and thus
\begin{align*}
    \frac{\mathrm{d}\psi(\vec x, t)}{\mathrm{d}t} &= \frac{\partial\psi(\vec x, t)}{\partial t } + \nabla_x \psi(\vec x, t)\cdot \frac{\partial\vec x(\vec x, t)}{\partial t}\\&
    = \frac{\partial\psi(\vec x, t)}{\partial t } + \nabla_x \psi(\vec x, t)\cdot \vec v(\vec x, t),
\end{align*}
which is commonly written with the last term interchanged, that is, 
\begin{equation*}
    \frac{\mathrm{d}\psi(\vec x, t)}{\mathrm{d}t} = \frac{\partial\psi(\vec x, t)}{\partial t } + \vec v(\vec x, t) \cdot \nabla_x \psi(\vec x, t).
\end{equation*}
This total derivative is referred to as the \textit{material derivative}, which we define as the differential operator
\begin{equation*}
    \frac{D}{Dt} := \frac{\partial}{\partial t} + \vec v \cdot \nabla_x.
\end{equation*}
Consequently, the spatial acceleration has an additional term:
\begin{equation*}
    \vec a(\vec x, t) = \frac{Dv(\vec x, t)}{Dt} = \frac{\partial \vec v(\vec x, t)}{\partial t} + \vec v(\vec x, t) \cdot \nabla_x \vec v(\vec x, t),  
\end{equation*}
or more compactly, 
\begin{equation*}
    \vec a = \partial_t \vec v + \vec v\cdot\nabla_x\vec v.
\end{equation*}
This last term corresponds to the product of a vector and a rank-2 tensor, which we have to analyze more carefully. Explicitly, we have
\begin{equation*}
    \vec v \cdot \nabla_x \vec v = v_j \frac{\partial \vec v}{\partial x_j} = v_j \frac{\partial (v_i\vec e_i)}{\partial x_j} = v_j \frac{\partial v_i}{\partial x_j}\vec e_i,
\end{equation*}
whose $i$-th component is 
\begin{equation*}
    (\vec v \cdot \nabla_x \vec v)_i = v_j \frac{\partial \vec v}{\partial x_j} = v_j \frac{\partial (v_i\vec e_i)}{\partial x_j} = \frac{\partial v_i}{\partial x_j} v_j.
\end{equation*}
Note that this last term resembles matrix-vector $\ten A\vec b$ multiplication, since $(\ten A\vec b)_i = A_{ij}b_j$. Indeed, by this observation it is correct to write the coefficient matrix of the rank-2 tensor $\nabla_x \vec v$ as 
\begin{equation*}
    \nabla_x \vec v = \begin{bmatrix}
        \frac{\partial v_1}{\partial x_1} & \frac{\partial v_1}{\partial x_2} & \frac{\partial v_1}{\partial x_3}\\
        \frac{\partial v_2}{\partial x_1} & \frac{\partial v_2}{\partial x_2} & \frac{\partial v_2}{\partial x_3}\\
        \frac{\partial v_3}{\partial x_1} & \frac{\partial v_3}{\partial x_2} & \frac{\partial v_3}{\partial x_3}
    \end{bmatrix},
\end{equation*}
and thus $\vec v\cdot\nabla_x\vec v = (\nabla_x\vec v)\vec v$, and the spatial acceleration is finally written as 
\begin{equation*}
    \vec a = \partial_t \vec v + (\nabla_x \vec v)\vec v.
\end{equation*}
In the spatial frame, we can further define the velocity gradient $\ten L$ as 
\begin{equation*}
    \ten L(\vec x, t) = \nabla_x\vec v(\vec x, t),
\end{equation*}
and thus acceleration can be written even more compactly as $\vec a = \partial_t \vec v + \ten L \vec v$. We can check that the time derivative of the gradient deformation tensor is 
\begin{align*}
    \dot{\ten F}(\vec X, t) &= \partial_t \nabla_X \vec\varphi(\vec X, t)\\
    &= \nabla_X (\partial_t \vec\varphi(\vec X, t)) \tag{$\partial_t\partial_{X_i} = \partial_{X_i}\partial_t$}\\
    &= \nabla_X \dot{\vec x}(\vec X, t)\\
    &= \nabla_X \vec v(\vec\varphi(\vec X, t), t)\\
    &= \nabla_x \vec v(\vec\varphi(\vec X, t), t) \nabla_X \vec\varphi(\vec X, t)\tag{chain rule}\\
    &= \ten L(\vec\varphi(\vec X, t), t) \ten F,
\end{align*}
where we remark that $\ten L = \ten L(\vec x,t) \neq \ten L(\vec\varphi(\vec X, t), t)$. We now introduce a lemma to decompose $\ten L$ into a symmetric and a skew-symmetric part. 
\begin{lemma}
    Let $\ten A$ be a second-order tensor. Then, $\ten A$ can be decomposed uniquely as a sum of a symmetric tensor $\ten S$ and a skew-symmetric tensor $\ten W$, i.e. $\ten A = \ten S + \ten W$. 
    \begin{proof}
        It is direct to check that $\ten S + \ten W = \ten A$. For the uniqueness, let $\ten S'$ and $\ten W'$ be a different pair of symmetric and skew-symmetric tensors, respectively. Then, we have $\ten S + \ten W = \ten S' + \ten W'$, which implies $\ten W - \ten W' = \ten S' - \ten S$. We see that $\ten W - \ten W'$ is skew-symmetric, and $\ten S' - \ten S$ is symmetric. The only tensor that is simultaneously symmetric and skew-symmetric is the zero tensor $\ten{0}$, we would have $C_{ij} = C_{ji} = -C_{ij}$ implies $2C_{ij} = 0$, and thus $C_{ij} = 0$. Then, $\ten S = \ten S'$ and $\ten W = \ten W'$, and the decomposition is unique. 
    \end{proof}
\end{lemma}
The velocity gradient $\ten L$ is often separated using this lemma, as 
\begin{equation*}
    \ten L = \ten D + \ten W,
\end{equation*}
where $\ten D = \frac{1}{2}(\ten L + \ten L^\top )$ is called the \textit{deformation rate tensor}, and $\ten W = \frac{1}{2}(\ten L - \ten L^\top )$ is called the \textit{spin tensor}. The latter is associated to the rotation of the body, as we can show that if $\vec v$ is the velocity, then 
\begin{equation*}
    \ten W \vec v = \frac{1}{2}\vec \omega \times \vec v =: (\nabla_x\times \vec v)\times v,
\end{equation*}
where $\frac{1}{2}\vec\omega$ corresponds to the angular velocity and $\vec\omega$ is the \textit{vorticity} vector. 

Now we seek to calculate the derivative of the invariants in space and time. 
\begin{lemma}
    For any tensor $\ten A$, the tensor derivative of its determinant is 
    \begin{equation*}
        \frac{\partial(\det\ten A)}{\partial \ten A} = \text{Cof}(\ten A) = \det(\ten A) \ten A^{-\top}.
    \end{equation*}
\end{lemma}
\begin{lemma}
    The cofactor matrix of the deformation gradient tensor is solenoidal, i.e. 
    \begin{equation*}
        \nabla_X \cdot \text{Cof}(\ten F) = \vec 0.
    \end{equation*}
\end{lemma}
With these results, we now can compute the time derivative of $\det\ten F$.
\begin{lemma}
    The time derivative of $\det \ten F$ is 
    \begin{equation*}
        \dot{\overline{\det(\ten F)}} = \text{Cof}(\ten F) : \dot{\ten F} = (\det\ten F)\dive \vec v.
    \end{equation*}
\end{lemma}
\paragraph{Change of coordinates and the Reynolds transport theorem}
Now we address the domain changes along the deformation, in order to be able to integrate quantities over arbitrary configurations. We note that that $\vec\varphi:\Omega_0 \to \Omega_t$, and thus by the change of variables theorem, the volume integral of a spatial scalar field $\vec f(\vec x)$ is 
\begin{equation*}
    \int_{\Omega_t}f(\vec x)dv = \int_{\Omega_0} f(\vec\varphi(\vec X)) |\det(\nabla_X\vec\varphi)| dV = f(\vec\varphi(\vec X)) JdV.
\end{equation*}
The integral quantities may represent time-varying values, such as the mass of a body, which may be redistributed in time as mass varies its concentration (density) over the domain, which is also time-varying. To define the time derivatives of spatial integrals, recall the Leibniz rule for integration, which precisely gives an expression for this kind of expression in 1D:
\begin{equation*}
    \frac{\mathrm{d}}{\mathrm{d}t}\left(\int_{a(t)}^{b(t)}f(x,t) \mathrm{d}x\right) = f(b(t),t) \frac{\mathrm{d}b}{\mathrm{d}t} - f(a(t),t) \frac{\mathrm{d}a}{\mathrm{d}t} + \int_{a(t)}^{b(t)}\frac{\partial}{\partial t}f(x,t)\mathrm{d}x.
\end{equation*}
The extension of this rule to higher dimensions is known as the Reynolds transport theorem. 
\begin{theorem}[Reynolds transport theorem]
    Let $\Omega_0$ and $\Omega_t$ be the material and spatial configurations, and let $\psi(\vec x, t)$ be a tensor or vector field in the spatial frame. Then, 
    \begin{align*}
        \frac{\mathrm{d}}{\mathrm{d}t}\int_{\Omega_t} &= \int_{\Omega_t}\frac{\partial\psi(\vec x, t)}{\partial t}dv + \int_{\Omega_t}\nabla_x\cdot (\psi\vec v)dv\\
        &= \int_{\Omega_t}\frac{\partial\psi(\vec x, t)}{\partial t}dv + \int_{\partial\Omega_t}\psi\vec v\cdot\vec n ds,
    \end{align*}
    where $\vec n$ is the exterior unit normal vector to $\partial\Omega_t$. 
    \begin{proof}
        We prove directly from the left hand side: 
        \begin{align*}
            \frac{\mathrm{d}}{\mathrm{d}t}\int_{\Omega_t} \psi(\vec x, t)dv &= \frac{\mathrm{d}}{\mathrm{d}t} \int_{\Omega_0} \psi(\vec\varphi(\vec X, t), t) \det\ten F(\vec X, t) dV \tag{change of variable theorem}\\
            &= \int_{\Omega_0} \frac{\mathrm{d}}{\mathrm{d}t} (\psi(\vec x, t)\det\ten F) dV\tag{$\Omega_0$ fixed in time}\\
            &= \int_{\Omega_0} \left(\frac{\mathrm{d}\psi}{\mathrm{d}t}\det\ten F + \psi \frac{\partial \det\ten F}{\partial t}\right)dV \tag{product rule, $\ten F = \ten F(\vec X, t)$}\\
            &= \int_{\Omega_0} \left(\left(\frac{\partial \psi}{\partial t} + \vec v \cdot \nabla_x \psi\right)\det\ten F + \psi \frac{\partial \det\ten F}{\partial t}\right)dV \tag{Reynolds transport theorem}\\
            &= \int_{\Omega_0} \left(\left(\frac{\partial \psi}{\partial t} + \vec v \cdot \nabla_x \psi\right)\det\ten F + \psi \frac{\partial \det\ten F}{\partial t}\right)dV \tag{$\dot{\overline{\det(\ten F)}} = (\det\ten F)\dive \vec v.$}\\
            &= \int_{\Omega_0}\frac{\partial \psi}{\partial t} J dV + \int_{\Omega_0} \left(\vec v \cdot \nabla_x \psi + \psi \dive \vec v\right)J dV\\
            &= \int_{\Omega_t} \frac{\partial \psi}{\partial t} dv + \int_{\Omega_0} \dive(\psi \vec v) JdV \tag{product rule}\\
            &= \int_{\Omega_t} \frac{\partial \psi}{\partial t} dv + \int_{\Omega_t} \dive(\psi \vec v) dv \\
            &= \int_{\Omega_t} \frac{\partial \psi}{\partial t} dv + \int_{\partial\Omega_t} \psi \vec v \cdot \vec n ds. \tag{divergence theorem}
        \end{align*}
    \end{proof}
\end{theorem}

\subsection{Mechanics and stress}
So far, we have discussed how to describe finite strain via deformation metrics, which stem from the deformation map $\vec\varphi$. Here, we introduced the deformation gradient tensor $\ten F$, from which we defined the Cauchy-Green right deformation tensor $\ten C$ and the Euler-Lagrange deformation tensor $\ten E$. We now have to connect these deformations to the actual forces and tensions that exist in a body. To this end, we need to define traction vector fields and the stress tensors.
\begin{definition}[Traction vectors and stress tensors]
    Let $d\vec f$ be the internal force acting on a 2D differential element $da$ in $\vec x$ at time $t$. We define the \textit{spatial traction field} $\vec t=\vec t(\vec n, \vec x, t)$ as 
    \begin{equation*}
        d\vec f(\vec x, t) =: \vec t(\vec n, \vec x, t) da.
    \end{equation*}
    Since the spatial configuration $\Omega_t$ is initially unknown, it is convenient to define the \textit{material traction field} $\vec T(\vec N, \vec X, t)$ analogously as 
    \begin{equation*}
        d\vec f\circ\vec\varphi = \vec T(\vec N, \vec X, t) dA,
    \end{equation*}
    where we also recall the Nanson-Piola formula $\vec n da = J\ten F^{-\top} \vec N dA$. Cauchy's stress theorem states that there exists a unique second-order tensor $\ten \sigma$, called the \text{Cauchy stress tensor}, such that 
    \begin{equation*}
        \vec t(\vec x, \vec n, t) = \ten\sigma(\vec x, t)\vec n,
    \end{equation*}
    and also there exists a unique second-order tensor $\ten P$, called the \textit{first Piola-Kirchhoff stress tensor}, such that
    \begin{equation*}
        \vec T(\vec X, \vec N, t) = \ten P(\vec X, t)\vec N.
    \end{equation*}
    From the above relations, we can show that $\ten \sigma$ and $\ten P$ are related via 
    \begin{equation*}
        \ten P = J\ten\sigma\ten F^{-\top} \qquad (\ten\sigma =J^{-1}\ten P\ten F^{\top}),
    \end{equation*}
    and we further define the \textit{second Piola-Kirchhoff stress tensor} $\ten S$ as 
    \begin{equation*}
        \ten S := \ten F^{-1}\ten P = J\ten F^{-1}\ten\sigma\ten F^{-\top}.
    \end{equation*}
    The tensor $\ten S$ formally corresponds to the \textit{pullback} of $\ten \sigma$, or equivalently, $\ten\sigma$ is the \textit{push-forward} of $\ten S$. % This notion is further studied in \ref{holzapfel}.
\end{definition}
\subsection{Conservation laws}
With the above definitions and the precise technique to differentiate quantities defined as spatial integrals via the Reynolds transport theorem, we can now write down \textit{conservation laws}, which are relationships between the rates of change of quantities of interest (e.g. mass, momentum) and the factors that physically induce those rates of change. Conservation laws define the partial differential equations that will be at the core of our numerical analysis later on. These laws have spatial and material formulations, both of which can be written in integral or differential form. To pass from the integral to the differential form, we use the following theorem: 
\begin{theorem}[Localization theorem]
    Let $\Omega\subset\R^n$ be a bounded, open and connected set. Given a scalar field $f:\Omega\to \R$, if for every subset $B\subset \Omega$ it holds that $\int_B fdV = 0$, then necessarily $f\equiv 0$ in $\Omega$. 
\end{theorem}
We now study the conservation laws for mass, linear momentum, angular momentum and energy. Let $\Omega_t\subset\R^3$ and let $B_t\subset\Omega_t$ be an arbitrary subset. 
\paragraph{Conservation of mass} We can naturally write the mass $M(t)$ of $B_t$ at time $t$ as the integral of the density over the domain, that is, 
\begin{equation*}
    M(t) = \int_{B_t}\rho(\vec x, t)dv,
\end{equation*}
which depends on time since the domain and the density distribution may change over time. The mass conservation principle states that the total mass $M(t)$ of a body does not change under deformation, that is, 
\begin{equation*}
    \frac{\mathrm{d}M(t)}{\mathrm{d}t} = 0 \implies \frac{\mathrm{d}}{\mathrm{d}t} \int_{B_t} \rho(\vec x, t)dv = 0.
\end{equation*}
By the Reynolds transport theorem, this expression is equivalent to 
\begin{equation*}
    \int_{B_t}\left(\frac{\partial\rho}{\partial t} + \nabla_x\cdot(\rho \vec v)\right) dv = 0,
\end{equation*}
and since $B_t$ is arbitrary, the above equality implies by the localization theorem that the argument of the integral is pointwise zero in $\Omega_t$, that is, 
\begin{equation*}
    \frac{\partial\rho}{\partial t} + \nabla_x\cdot(\rho \vec v) = 0.
\end{equation*}
To write these equations in the material frame, we can rewrite the law using the change of variables theorem to get 
\begin{equation*}
    \int_{B_0} \frac{\mathrm{d}}{\mathrm{d}t}(\rho(\vec\varphi(\vec X, t), t) \det\ten F)dV = 0.
\end{equation*} 
The localization theorem then implies 
\begin{equation*}
    \frac{\mathrm{d}}{\mathrm{d}t}(\rho(\vec\varphi(\vec X, t), t) \det\ten F(\vec X,t)) = 0,
\end{equation*}
and thus $\rho(\vec\varphi(\vec X, t), t) \det\ten F(\vec X, t)$ is constant. In particular, it is equal to the value it has at $t=0$, which implies
\begin{equation*}
    \rho(\vec\varphi(\vec X, t), t) \det\ten F(\vec X, t) = \rho(\underbrace{\vec\varphi(\vec X, 0)}_{\vec X}, 0)\underbrace{\det\ten F(\vec X, 0)}_{\ten I} = \rho(\vec X, 0) =: \rho_0(\vec X) \quad \forall t,
\end{equation*} 
where we defined the initial density field $\rho_0$. This translates to
\begin{equation*}
    \rho_0(\vec X) = \rho(\vec x) \det\ten F(\vec X),
\end{equation*}
where the dependence in time is implicitly written in $\vec x$. This equation corresponds to conservation of mass in the material frame. 

There exist some particular cases where the conservation of mass can be further simplified. We say that a deformation map $\vec\varphi$ is associated with an \textit{isochoric} deformation if and only if the volume of the body $\mathcal{V}(t) = \int_{B_t}1dv$ does not change under deformation, that is, 
\begin{equation*}
    0 = \frac{D\mathcal{V}(t)}{Dt} = \frac{D}{Dt} \int_{B_t} 1dv \overset{(*)}{=}\int_{B_t} \left(\frac{\partial (1)}{\partial t} + \dive(1\vec v)\right)dv = \int_{B_t} (\dive\vec v)dv,
\end{equation*}
and since $B_t$ is arbitrary, we have $\dive \vec v = 0$. This is known as the \textit{incompressibility condition}, which applied to the  conservation of mass in the spatial frame, yields
\begin{align*}
    0 &= \frac{\partial\rho}{\partial t} + \dive(\rho\vec v) \\
    &= \frac{\partial\rho}{\partial t} + \rho\dive\vec v + \vec v \cdot\nabla_x \rho\\
    &= \frac{D\rho}{Dt},
\end{align*}
where we used the incompressibility condition and the definition of the material derivative. Thus, the conservation of mass for an isochoric deformation is reduced to 
\begin{equation*}
    \frac{D\rho}{Dt} = 0.
\end{equation*}
\paragraph{Conservation of linear momentum} In a 1D setting we know that Newton's second law holds, which relates a deformation metric (acceleration, $a$) to the forces $F_j$ via the mass $m$ as 
\begin{equation*}
    ma = \sum_j F_j,
\end{equation*}
and assuming mass is constant in time, we can write $ma = m\dot{v} = \dot{mv} = \dot{p}$, where $p=mv$ is defined as the \textit{linear momentum} of the mass $m$. This yields $\dot{p} = \sum_j F_j$, which states that the rate of change of linear momentum is equal to the sum of all forces acting on the body. In 3D, we define the linear momentum of the mass $B_t$ with spatial velocity $\vec v$ and spatial density $\rho$ as the vector quantity
\begin{equation*}
    \vec I(t) = \int_{B_t} \rho(\vec x, t) \vec v(\vec x, t) dv.
\end{equation*}
From the Reynolds transport theorem, we can check that if $\rho(\vec x, t)$ is the spatial density field and $w(\vec x, t)$ is any spatial field, then the time derivative of their product is 
\begin{equation*}
    \frac{D}{Dt} \int_{B_t} w\rho dv = \int_{B_t} \rho \frac{Dw}{Dt} dv,
\end{equation*}
and thus by taking $w=v_i$, where $v_i$ are the components of the spatial velocity field $\vec v$, we obtain
\begin{equation*}
    \frac{D}{Dt} \int_{B_t} v_i\rho dv = \int_{B_t} \rho \frac{Dv_i}{Dt} dv,
\end{equation*}
and gathering components we deduce
\begin{equation*}
    \frac{D\vec I(t)}{Dt} = \frac{D}{Dt} \int_{B_t} \vec v \rho dv = \int_{B_t} \rho \frac{D\vec v}{Dt} dv.
\end{equation*}
We assume that the forces applied to $B_t$ come from traction forces and body forces. The traction forces are defined as 
\begin{equation*}
    \vec F_{e} (t) := \int_{\partial B_t} \vec t(\vec x, t) ds,
\end{equation*}
and the body forces are defined as 
\begin{equation*}
    \vec F_b(t) := \int_{B_t} f(\vec x, t) dv = \int_{B_t} \rho(\vec x, t) \vec b(\vec x, t)dv,
\end{equation*}
where $\vec f$ corresponds to the body force vector per unit volume, and $\vec b$ corresponds to the body force vector per unit mass, which are related via $\vec f = \rho\vec b$. With this, we can now properly define the conservation of linear momentum, which states that the rate of change of linear momentum of $B_t$ is equal to the sum of all forces acting on $B_t$, that is,
\begin{equation*}
    \frac{D\vec I(t)}{Dt} = \vec F_e(t) + \vec F_b(t) = \int_{\partial B_t} \vec t(\vec x, t) ds + \int_{B_t} \vec f(\vec x, t)dv.
\end{equation*}
To drop the integral and get a differential form of this conservation law, we need to transform the integral over $\partial B_t$ to a volume integral over $B_t$. Fortunately, we have the Cauchy stress relation $\vec t = \ten \sigma \vec n$, which implies that we can write 
\begin{equation*}
    \vec F_e(t) =  \int_{\partial B_t} \ten\sigma(\vec x, t) \vec n ds = \int_{B_t} \dive\ten\sigma(\vec x, t) dv,
\end{equation*}
where the last step follows from the divergence theorem. Substituting this and dropping the integral by the localization theorem, we get 
\begin{equation*}
    \rho(\vec x, t) \frac{D\vec v(\vec x, t)}{Dt} = \vec f(\vec x, t) + \dive \ten\sigma(\vec x, t),
\end{equation*}
which is the spatial and differential form of this law. To go back to the Lagrangian description, we note that by the change of variable theorem
\begin{equation*}
    \int_{B_t} \rho(\vec x, t) \frac{D\vec v(\vec x, t)}{Dt} dv = \int_{B_0} \frac{D\vec v(\vec x, t)}{Dt} \rho(\vec x, t) \det\ten F(\vec X, t)dV = \int_{B_0} \rho_0(\vec X) \ddot{\vec\varphi}(\vec X, t) dV,
\end{equation*}
where we used the Lagrangian conservation of mass and the fact that we are now integrating over the reference configuration, which is fixed in time. Analogously, the force vectors are 
\begin{equation*}
    \vec F_b(t) = \int_{B_0} \vec f(\vec\varphi(\vec X, t), t) \det\ten F(\vec X, t) dV = \int_{B_0} \vec f_0(\vec X, t) dV,
\end{equation*}
where we defined $\vec f_0(\vec X, t) := \vec f(\vec\varphi(\vec X, t), t) \det\ten F(\vec X,t)$, i.e. $\vec f_0 = J\vec f$, and similarly, 
\begin{equation*}
    \int_{B_t} \dive\ten\sigma(\vec x, t) dv = \int_{B_0} \dive\ten\sigma(\vec x, t) \det\ten F(\vec X, t) dV = \int_{B_0} \text{Div }\ten P(\vec X, t) dV,
\end{equation*}
where we transformed the tensions by using the relationship $\ten P = J \ten \sigma\ten F^{-\top}$:
\begin{align*}
    \nabla_X\cdot \ten P(\vec X, t) &= \nabla_X \cdot(J\ten\sigma\ten F^{-\top})\\
    &= \nabla_X \ten\sigma : (J\ten F^{-\top}) + \ten\sigma \nabla_X \cdot (J\ten F^{-\top}) \tag{product rule}\\
    &= \nabla_X \ten\sigma : (J\ten F^{-\top}) \tag{$\text{Cof} \ten F = J\ten F^{-\top}$ has zero divergence}\\
    &= (\ten F \nabla_x\ten\sigma) : J\ten F^{-\top} \tag{chain rule}\\
    &= \nabla_x \ten\sigma : J\ten I\\
    &= J \nabla_x\cdot \ten\sigma\\
    &= (\det\ten F)\dive\ten\sigma.
\end{align*}
Thus, conservation of linear momentum in the Lagrangian frame and in integral form is written as 
\begin{equation*}
    \int_{B_0}\rho_0(\vec X) \ddot{\vec\varphi}(\vec X, t)dV = \int_{B_0}\vec f_0(\vec X, t) dV + \int_{B_0} \nabla_X \cdot \ten P(\vec X, t)dV,
\end{equation*}
and by using the same arguments as above, we can derive its differential form, which yields
\begin{equation*}
    \rho_0(\vec X)\ddot{\vec\varphi}(\vec X, t) = \vec f_0(\vec X, t) + \nabla_X \cdot\ten P(\vec X, t).
\end{equation*}
\paragraph{Conservation of angular momentum} In the same fashion as before, we define the angular momentum of $B(t)$ as the vector quantity $\vec H(t)$ given by
\begin{equation*}
    \vec H(t) := \int_{B_t} \vec x \times \rho(\vec x, t)\vec v(\vec x, t)dv.
\end{equation*}
Here, it is convenient to introduce the \textit{Levi-Civita symbol} $\varepsilon$ as the third-order tensor given by components as
\begin{equation*}
    \varepsilon_{ijk} = \begin{cases}
        +1&\text{ if}(i,j,k)\in\{(1,2,3),(2,3,1), (3,1,2)\}\\
        -1&\text{ if}(i,j,k)\in\{(3,2,1),(2,1,3), (1,3,2)\}\\
        0 &\text{ otherwise.}
    \end{cases}
\end{equation*}
This third-order tensor has $27$ components, but only $6$ of them are nonzero, which are $+1$ when the indexes are an even permutation of $1$, $2$ and $3$, and $-1$ when they are an odd permutation. This allows us to write the cross product between $\vec a, \vec b\in\R^3$ as
\begin{equation*}
    \vec a\times\vec b = \varepsilon_{ijk}a_jb_k\vec e_i.
\end{equation*}
We also can show that the material derivative of $\vec x\times\vec v$ is given by 
\begin{equation*}
    \frac{D(\vec x\times \vec v)}{Dt} = \vec x \times \frac{D\vec v}{Dt}.
\end{equation*}
and thus we can write the time derivative of the angular momentum as 
\begin{equation*}
    \frac{D\vec H(t)}{Dt} = \frac{D}{Dt}\int_{B_t} \vec x \times \frac{D(\rho\vec v)}{Dt} dv = \int_{B_t} \vec x \times \rho\frac{D\vec v}{Dt}dv,
\end{equation*}
which corresponds to the integral form of the conservation of angular momentum in the spatial frame. From this and invoking the Cauchy stress theorem, we can deduce that this law does not have a differential form, but rather is satisfied if and only if the Cauchy stress tensor is symmetric, that is, 
\begin{equation*}
    \ten\sigma(\vec x, t) = \ten\sigma^\top(\vec x, t).
\end{equation*}
In the Lagrangian frame, we can directly use the relationship between $\ten \sigma$ and $\ten P$. Note that $\ten \sigma = J^{-1}\ten P \ten F^{\top}$, and $\ten\sigma^\top = J^{-1}\ten F \ten P^\top$. Thus, we get the conservation of angular momentum in the material frame
\begin{equation*}
    \ten \sigma = \ten \sigma^\top \iff \ten P \ten F^\top = \ten F \ten P^\top.
\end{equation*}
It is important to remark that, in general, $\ten P$ is not symmetric. However, the second Piola-Kirchhoff tensor $\ten S = \ten F^{-1}\ten P$ could be: note that 
\begin{align*}
    \ten S &= \ten F^{-1}\ten P = J\ten F^{-1}\ten \sigma\ten F^{-\top}\\
    \ten S^\top &= J(\ten F^{-1}\ten \sigma\ten F^{-\top})^\top = J\ten F^{-1}\ten \sigma^\top \ten F^{-\top},
\end{align*}
where we conclude that $\ten S$ is symmetric if and only if $\ten \sigma$ is symmetric, i.e. conservation of angular momentum holds in the spatial frame. Thus, the conservation of angular momentum in the Lagrangian frame can be equivalently written as 
\begin{equation}
    \ten S(\ten X, t) = \ten S^\top(\ten X, t).
\end{equation}
\paragraph{Conservation of energy}
We begin by defining mechanical power as the scalar field
\begin{equation*}
    \mathcal{P}(t) = \int_{B_t}\vec f\cdot\vec v dv + \int_{\partial B_t}\vec t(\vec n)\cdot\vec v ds,
\end{equation*}
where the first term is associated to the body forces, and the second term to the surface traction. We start by noting that 
\begin{align*}
    \int_{\partial B_t}\vec t(\vec n)\cdot\vec v ds &= \int_{\partial B_t}\ten\sigma\vec n\cdot\vec v ds \tag{Cauchy stress theorem}\\
    &= \int_{\partial B_t}\vec v^\top \ten \sigma\vec n ds\\
    &= \int_{\partial B_t}(\ten\sigma^\top\vec v)\cdot\vec n ds\\
    &= \int_{\partial B_t}(\ten\sigma\vec v)\cdot\vec n ds \tag{conservation of angular momentum}\\
    &= \int_{B_t}\dive(\ten\sigma\vec v)dv\tag{divergence theorem}\\
    &= \int_{B_t}\dive\ten\sigma \cdot \vec v dv + \int_{B_t}\ten\sigma:\nabla_x\vec v dv.
\end{align*}
Now we split $\nabla_x\vec v = \ten L = \ten D + \ten W$ in its symmetric and skew-symmetric parts, and since the scalar product (tensor contraction, $:$) of a symmetric and a skew-symmetric tensor is zero, we obtain
\begin{align*}
    \mathcal{P}(t) &= \int_{B_t}(\vec f + \dive\ten\sigma) \cdot\vec v dv + \int_{B_t}\ten\sigma:\ten D dv\\
    &= \int_{B_t} \rho\frac{D\vec v}{Dt}\cdot \vec v dv + \int_{B_t}\ten\sigma:\ten D dv \tag{conservation of linear momentum},
\end{align*}
and using index notation we can prove that the first term is 
\begin{equation*}
    \int_{B_t} \rho\frac{D\vec v}{Dt}\cdot \vec v dv = \frac{D}{Dt}\left(\frac{1}{2}\int_{B_t}\rho\vec v\cdot\vec v dv\right) = \frac{DK(t)}{Dt},
\end{equation*}
where we define $K(t) = \frac{1}{2}\int_{B_t}\rho \vec v\cdot\vec v dv = \frac{1}{2}\int_{B_t}\rho \|\vec v\|^2 dv$. With this, mechanical power in the Eulerian frame can be written as 
\begin{equation*}
     \mathcal{P}(t) = \frac{DK(t)}{Dt} + \int_{B_t}\ten\sigma:\ten D dv.
\end{equation*}
In the Lagrangian frame, we check that the second term, which corresponds to the power associated to mechanical stress, can be rewritten in terms of $\ten F$ and $\ten P$ as
\begin{align*}
    \int_{B_t}\ten\sigma:\ten D dv &= \int_{B_0}\ten\sigma :\nabla_x \vec v JdV \tag{change of variables theorem}\\
    &= \int_{B_0} J\tr(\ten\sigma(\dot{\ten F}\ten F^{-1})^\top) dV \tag{$\ten A:\ten B = \tr(\ten A^\top \ten B)$}\\
    &= \int_{B_0} J\tr((\ten\sigma\ten F^{-\top})\dot{\ten F}^\top) dV\\
    &= \int_{B_0} J (\ten\sigma\ten F^{-\top}):\dot{\ten F} dV\\
    &= \int_{B_0}\ten P:\dot{\ten F} dV,
\end{align*}
and thus mechanical power in the Lagrangian frame is
\begin{equation*}
    \mathcal{P}(t) = \frac{D}{Dt}\left(\frac{1}{2}\int_{B_0}\rho_0 \dot{\vec\varphi}\cdot\dot{\vec\varphi} dV\right) + \int_{\partial B_0} \ten P : \dot{\ten F}dV.
\end{equation*}
As with the conservation of angular momentum, we can write this definition in terms of the second Piola-Kirchhoff tensor $\ten S$ by noting that $\ten P:\dot{\ten F} = \ten S:\dot{\ten E}$. With this definition, we recall the first law of thermodynamics, which states that the rate of change of the total energy of a body is given by the heat influx and the mechanical power applied to the body. More precisely, if we denote heat by $Q$, we have
\begin{equation*}
    \frac{D(K+U)}{Dt} = \dot{Q} + \mathcal{P},
\end{equation*}
where $U$ is the internal energy of the system. This internal energy can come from several sources within the body, such as its temperature, chemical reactions, atomic vibration and other physical phenomena. We define an \text{intensive} energy function $e(\vec x, t)$ as the internal energy per unit mass, and thus 
\begin{equation*}
    U(t) = \int_{B_t} \rho(\vec x, t) e(\vec x, t) dv = \int_{B_0}\rho_0(\vec X) e_m(\vec X, t)dV,
\end{equation*}
where we have written $e_m = e\circ\vec\varphi$. The heat exchange term is defined as 
\begin{equation*}
    \dot{Q}(t) = \int_{B_t}r(\vec x, t)dv - \int_{\partial B_t}\vec q(\vec x, t)\cdot\vec n(\vec x)ds,
\end{equation*}
where $r(\vec x, t)$ is the internal heating per unit volume, and $\vec q(\vec x, t)$ is the heat flux vector. Here, $r$ has units W$/$m$^3$, and $\vec q$ has units W$/$m$^2$. We note that $\vec q$ points in the direction of heat flow from a higher to a lower temperature. Thus, the integral form of the conservation of energy in the spatial frame is 
\begin{equation*}
    \frac{D}{Dt}\left(\frac{1}{2}\int_{B_t}\rho \vec v\cdot\vec v dv\right) + \frac{D}{Dt}\left(\int_{B_t}\rho e dv\right) = \frac{DK}{Dt} + \int_{B_t}\ten\sigma:\ten D dv + \int_{B_t}rdv - \int_{\partial B_t} \vec q \cdot \vec n ds.
\end{equation*}
By virtue of the localization theorem, we readily obtain the differential form
\begin{equation*}
    \rho\left(\frac{\partial e}{\partial t} + \vec v\cdot\nabla_x e\right) = \ten\sigma : \ten D + r - \dive \vec q.
\end{equation*}
To pull back this equation to the reference frame, we note that a vector field $\vec q$ in the spatial frame is transformed to $\vec q_0$ in the reference frame as 
\begin{equation*}
    \vec q_0 = J \ten F^{-1}\vec q.
\end{equation*}
We further define $r_0(\vec X,t) = Jr(\vec\varphi(\vec X, t), t)$ from the change of variables theorem. Substituting and droppping the integrals by the localization theorem, we obtain the differential form of the conservation of energy in the Lagrangian frame, that is,
\begin{equation*}
    \rho_0\dot{e}_m = \ten S:\dot{\ten E} + r_0 - \text{Div }\vec q_0.
\end{equation*}
\paragraph{Second law of thermodynamics} 
Although not a conservation law, it is necessary to also introduce the inequality that arises from the second law of thermodynamics, which can be recast in our continuum mechanics framework. We denote the absolute temperature as $\Theta$ as in the zeroth law of thermodynamics, and denote the entropy as $S$. Both are physical properties of a thermodynamical system, and for a deformable body that constitutes a closed thermodynamical system (i.e. which conserves mass, but allows heat exchange with its surroundings), the rate of change of entropy must exceed the rate of heat influx divided by the absolute temperature. More precisely, we have the inequality 
\begin{equation*}
    \frac{DS}{Dt}\geq \frac{\dot{Q}}{\Theta}.
\end{equation*}
Note that this inequality is written in terms of extensive (integrated) quantities $S$, $Q$ and $\Theta$, and if the system is isolated, $\dot{Q}=0$, and thus $\frac{DS}{Dt}\geq 0$. We can write this law only in terms of intensive quantities, which are the entropy per unit mass $\eta(\vec x, t)$ and the temperature $\theta(\vec x, t)$. By replacing we obtain
\begin{equation*}
    \frac{D}{Dt}\int_{B_t}\rho\eta dv \geq \int_{B_t}\frac{r}{\theta}dv - \int_{\partial B_t}\frac{1}{\theta}\vec q\cdot\vec n ds,
\end{equation*}
which can be reorganized and simplified to obtain the \textit{Clausius-Duhem inequality}
\begin{equation*}
    \rho\frac{D\eta}{Dt} + \nabla_x\cdot\left(\frac{1}{\theta}\vec q\right) - \frac{r}{\theta} \geq 0.
\end{equation*}
Here, we can interpret $r/\theta$ as an internal entropy source, whereas $\frac{1}{\theta}\vec q$ is an entropy flux. By pulling back to the Lagrangian frame, we obtain the inequality
\begin{equation*}
    \rho_0\dot{\eta}_m + \nabla_X \cdot\left(\frac{1}{\theta_m}\vec q_0\right) - \frac{r_0}{\theta_m}\geq 0.
\end{equation*}
We can also rewrite both inequalities by substituting the Helmholtz free energy $\psi = e - \theta\eta$, which yields in the spatial frame
\begin{equation*}
    -\rho\frac{D\psi}{Dt} - \rho\eta \frac{D\theta}{Dt} + \ten\sigma:\ten D - \frac{1}{\theta}\nabla_x \theta\cdot\vec q \geq 0
\end{equation*}
and in the material frame
\begin{equation*}
    -\rho_0 \dot{\psi}_m - \rho_0 \eta_m \dot{\theta}_m + \ten S:\dot{\ten E} - \frac{1}{\theta_m}\vec q_0\cdot \nabla_X \theta_m \geq 0.
\end{equation*}
We will further use this form to deduce \textit{physically consistent} constitutive models that actually satisfy physical laws. Note that in a stationary system, where all time derivatives are zero, the Clausius-Duhem inequality in the spatial frame reduces to
\begin{equation*}
    -\frac{1}{\theta}\vec q \cdot\nabla_x\theta \geq 0.
\end{equation*}
This inequality states that $\vec q$ is a vector whose direction indicates heat flux, and $\nabla_x\theta$ indicates the direction of fastest temperature growth. Thus, since the inner product must be positive, and due to the minus sign, the vector $\vec q$ points from high temperature regions to low temperature regions.
\subsection{Constitutive models}
Let us go back to the linear momentum conservation, which is the time-dependent, vector-valued, possibly nonlinear partial differential equation 
\begin{equation*}
    \rho_0 \ddot{\vec u} = \vec f_0 + \nabla_X \cdot(\ten F\ten S).
\end{equation*}
At $t=0$, it is reasonable to assume that one knows the initial density function $\rho_0(\vec X) = \rho(\vec x, 0)$, the initial stress $\ten S(\vec X, 0)$ and $\vec f(\vec x, t)$ for all $t>0$. Thus, we would have to solve for $\vec u$, i.e. 3 components, which will allow us to compute $\vec\varphi(\vec X, t)$, and thus $\ten F = \nabla_X\vec u$, $\vec f_0 = J\vec f$, but also we would have to solve for $\ten S$, which due to the symmetry $\ten S = \ten S^\top$ has 6 components. Thus, we need to solve for $9$ components with the $3$ equations that the PDE has, and therefore the system is not solvable. We will necessarily need a \textit{constitutive model}, i.e. a functional relationship between stress and deformation, to reduce the number of unknowns. Since the PDE has $3$ components, the most direct choice is to find a relationship $\ten S = \ten S(\vec u)$ or $\ten\sigma = \ten \sigma(\vec u)$. 

So far, the body $\Omega_t$ has been thought to be any substance with finite volume, such as a liquid, a solid or a gas. The constitutive model must consist of an expression that reflects how the deformation depends on the stresses (and possibly other variables) of each substance. Some examples: 
\begin{enumerate}
    \item Solids are able to resist and restore temporal and permanent deformations in any direction. Some solids exhibit distinct failure and fracture behavior, and have different levels of compressibility. Also, solid density and mechanical properties vary with temperature, which allows, for example, for metal to be forged (deformed) to a certain shape only at high temperatures, undergoing plastic deformation while increasing their elastic modulus.
    \item Fluids do not resist shear stress, only normal stresses through hydrostatic pressure. Their stress derives from the deformation rate tensor $\ten D$, often through the tensor itself (viscous part) or its trace $\frac{1}{2} \tr (\nabla_x\vec v + (\nabla_x \vec v)^\top) = \nabla_x\cdot \vec v$ (compressible part). Most fluids are thermofluids, whose density and viscosity varies with temperature.
\end{enumerate}
In general, there exist constitutive models for the stress, the heat flow, the internal energy and the entropy. These constitutive models must satisfy a series of physical hypotheses: 
\begin{enumerate}
    \item The constitutive model is deterministic as a function of the primitive variables (e.g. deformation, temperature) that are being considered. 
    \item The model must be invariant under changes in the frame of reference, i.e. independent of the observer. 
    \item The model must satisfy the principle of locality, i.e. all points are influenced only by its immediate surroundings.
    \item The model is physically consistent, i.e. it does not violate by default any of the conservation principles or laws of thermodynamics.
    \item The model must respect the material symmetries, if there are any.
\end{enumerate}
These hypotheses are extended with additional conditions when required. When choosing a constitutive model, it is natural to choose a model that fits the particular behavior of a material. In some cases, like wood with parallel grain or laminated materials, the strength of the material is different in one direction to the other ones. We call this an \textit{anisotropic} material, whereas materials that exhibit the same response in all directions are called \textit{isotropic}. Apart from anisotropy, we distinguish between \textit{homogeneous} materials, whose properties are the same at every point of the material, and \textit{heterogeneous} materials, whose properties change in space. From these features, it is possible to find general functional forms for these models. 

We now study some explicit cases.
\paragraph{Constitutive modeling of solids} 
Assuming that $\ten S$ deppends only on the deformation $\vec\varphi$, we first obtain that $\ten S$ depends on $\ten F$ due to the locality principlpe, and from the observer independence, we conclude that $\ten S$ is a function of $\ten C=\ten F^\top \ten F$ or $\ten E = \frac{1}{2}(\ten C - \ten I)$. Some examples are the linear elastic model $\ten S(\ten E) = \lambda\tr(\ten E)\ten I + 2\mu\ten E$, and the neohookean model $\ten S(\ten C) = \frac{\lambda}{2}(J^2-1)\ten C^{-1} + \mu (\ten I - \ten C^{-1})$, where $\lambda,\mu$ are called the Lam√© parameters. These parameters are obtained from experiments, and are related to the Young modulus $E$ and the Poisson ratio $\nu$. Since $\ten F=\ten I + \nabla_X \vec u$, $\ten C$ and $\ten E$ are also functions of $\vec u$, and thus $\ten S(\ten E)$ also is a function of $\vec u$, thus allowing us to rewrite the conservation of linear momentum as
\begin{equation*}
    \rho_0\ddot{\vec u} = \vec f_0 + \nabla_X\cdot(\ten F(\vec u) \ten S(\ten E(\vec u))),
\end{equation*}
which are called the \textit{elastodynamics equations}. These equations are nonlinear in $\vec u$, because at least $\ten E$ is not linear due to the term $(\nabla_X\vec u)^\top (\nabla_X\vec u)$. If we know the initial density $\rho_0(\vec X)$, the initial position $\vec u(\vec X, 0)$, the initial velocity $\dot{\vec u}(\vec X, 0)$, the forces $\vec f_0(\vec X, t)$ and boundary conditions on $\vec u$ or $\ten P\vec n$ for all times $t>0$, we can attempt to solve for $\vec u(\vec X, t)$. Moreover, if the object is stationary, the acceleration is $\ddot{\vec u}=0$, and thus the equations reduce to the \textit{elastostatics equations}
\begin{equation*}
    -\nabla_X (\ten F(\vec u)\ten S(\ten E(\vec u))) = \vec f_0.
\end{equation*}
In a small-deformation regime, we checked that $\ten E \approx \ten \varepsilon = \frac{1}{2}(\nabla_X\vec u + (\nabla_X\vec u)^\top)$, we know that $\ten F = \ten I + \nabla_X\vec u \approx\ten I$, and assuming that the solid follows a linear elastic model, we compute
\begin{equation*}
    \ten F(\vec u)\ten S(\ten E(\vec u)) \approx \ten S(\ten \varepsilon(\vec u)) = \lambda\tr(\ten\varepsilon(\vec u)) \ten I + 2\mu\ten\varepsilon(\vec u),
\end{equation*}
which we can differentiate easily to get the explicit equations. Another way of writing $\ten S$ as a tensor field that is linear on $\ten\varepsilon$ is through a fourth-order \text{elasticity tensor} $\mathbb{C}$, which in the linear case results $\ten S(\ten\varepsilon(\vec u)) = \mathbb{C}:\ten\varepsilon(\vec u)$, resulting in the linear elastodynamics and elastostatic equations 
\begin{align*}
    \rho_0 \ddot{\vec u} &= \vec f_0 + \nabla_X \cdot(\mathbb{C}:\ten\varepsilon(\vec u))\\
    \vec 0 &= \vec f_0 + \nabla_X \cdot(\mathbb{C}:\ten\varepsilon(\vec u)).
\end{align*}
In the stationary case, there is no dependence on time, and thus it is not necessary to specify any initial conditions, but we do requiere boundary conditions on the displacement or the stress. Moreover, the solutions for the nonlinear $\ten S$ case may not be unique, but they are indeed unique in the linear case, where we can rewrite our problem in terms of an elliptic operator. We also note that the linear elastic model is an isotropic material model, and only depends on the two Lam√© parameters $\lambda, \mu$. By contrast, linear materials with less symmetries on $\mathbb{C}$ requiere up to 21 parameters to fully describe their behavior. 

To check for thermodynamical consistency, we need to evaluate whether the constitutive model actually satisfies the Clausius-Duhem inequality. To this end, Coleman and Noll described a procedure that gives us an explicit condition for tensor $\ten S$ as a function of the Helmholtz free energy. Recall the Clausius-Duhem inequality written in terms of the Helmholtz free energy $\psi = e - \theta\eta$: in the spatial frame, we have
\begin{equation*}
    -\rho\frac{D\psi}{Dt} - \rho\eta \frac{D\theta}{Dt} + \ten\sigma:\ten D - \frac{1}{\theta}\nabla_x \theta\cdot\vec q \geq 0
\end{equation*}
and in the material frame, we have
\begin{equation*}
    -\rho_0 \dot{\psi}_m - \rho_0 \eta_m \dot{\theta}_m + \ten S:\dot{\ten E} - \frac{1}{\theta_m}\vec q_0\cdot \nabla_X \theta_m \geq 0.
\end{equation*}
Assuming that $\psi_m$ only depends on $\ten E$ and $\theta_m$, i.e. $\psi_m = \psi_m(\ten E, \theta_m)$, we have by the chain rule
\begin{equation*}
    \dot{\psi}_m = \frac{\partial\psi_m}{\partial\ten E} : \dot{\ten E} + \frac{\partial\psi_m}{\partial \theta_m}\dot{\theta}_m,
\end{equation*}
which replaced in the Clausius-Duhem inequality yields
\begin{equation*}
    \left(\ten S - \rho_0 \frac{\partial\psi_m}{\partial\ten E}\right):\dot{\ten E} - \rho_0 \left(\eta_m + \frac{\partial\psi_m}{\partial \theta_m}\right)\dot{\theta}_m - \frac{1}{\theta_m}\vec q_0\cdot\nabla_X\theta_m\geq 0.
\end{equation*}
The Coleman-Noll argument is that it is always possible to construct possible processes given $\ten E, \theta_m$, but $\dot{\ten E}$ and $\dot{\theta}_m$ can have arbitrary signs, and thus to always satisfy the inequality, we require
\begin{equation*}
    \ten S = \rho_0 \frac{\partial\psi_m}{\partial\ten E},\qquad \eta_m = -\frac{\partial\psi_m}{\partial\theta_m},
\end{equation*}
that is, from the constitutive model of the Helmholtz free energy $\psi_m$ we can derive constitutive models for $\ten S$ and $\eta_m$. 
\paragraph{Constitutive modeling of fluids} Unlike solids, fluid stress only depends on the movement velocity. More precisely, the stress only depends on the (symmetric) strain rate tensor
\begin{equation*}
    \ten D(\vec v) = \frac{1}{2}\left(\nabla_x\vec v + (\nabla_x\vec v)^\top\right),
\end{equation*}
which we recall is the symmetric part of $\ten L=\nabla_x\vec v$. We may first assume that the fluid is incompressible, that is, 
\begin{equation*}
    \nabla_x\cdot\vec v = 0.
\end{equation*}
Then, by conservation of mass, we obtain $\frac{D\rho}{Dt}=0$, and thus if $\rho(\vec X, 0)$ is constant, then $\rho(\vec X, t)$ will be constant as well for all $\vec X$ and time $t$, and moreover $\tr\ten D(\vec v) = \nabla_x\cdot\vec v = 0$. The stress tensor in incompressible fluids can be split into two parts: a volumetric (spherical) part and a deviatoric part. The volumetric part is associated to the hydrostatic pressure, which always points normal to the surface. An example of such a constitutive model is the Newtonian fluid model, which is given by
\begin{equation*}
    \ten \sigma(\ten D) = -p\ten I + 2\mu\ten D,
\end{equation*}
where $p$ is the hydrostatic pressure and $\mu$ is the dynamic viscosity of the fluid. Here, conservation of linear momentum (in the spatial frame) is 
\begin{equation*}
    \rho\left(\frac{\partial \vec v}{\partial t} + (\nabla_x\vec v)\vec v\right) = \dive\ten\sigma + \vec f.
\end{equation*} 
Using index notation, we can show that 
\begin{equation*}
    \dive(\ten D) = \frac{1}{2}\nabla_x^2\vec v + \frac{1}{2}\nabla_x(\nabla_x\cdot\vec v),
\end{equation*}
and for incompressible fluids the second term is zero, which implies
\begin{equation*}
    \dive\ten\sigma = -\nabla_x p + \mu\nabla_x^2 \vec v,
\end{equation*}
and replacing in the conservation of linear momentum we obtain the second-order PDE system known as the \textit{Navier-Stokes equations}:
\begin{align*}
    \rho\left(\frac{\partial \vec v}{\partial t} + (\nabla_x\vec v)\vec v\right) &= -\nabla_x p + \mu\nabla_x^2 \vec v + \vec f\\
    \nabla_x\cdot\vec v &= 0,
\end{align*} 
which is a system with unknowns $\vec v$ and $p$. Two particular cases of the Navier-Stokes equations are of physical relevance. First, when the fluid has no viscosity, we obtain a first-order PDE system known as the incompressible \textit{Euler equations}
\begin{align*}
    \rho\left(\frac{\partial \vec v}{\partial t} + (\nabla_x\vec v)\vec v\right) &= -\nabla_x p + \vec f\\
    \nabla_x\cdot\vec v &= 0.
\end{align*} 
Second, if the fluid has viscosity but is in stationary state, the left hand side is identically zero, and thus the resulting second-order linear system known as the incompressible \textit{Stokes equations}
\begin{align*}
    -\nabla_x p + \mu\nabla_x^2 \vec v &= - \vec f\\
    \nabla_x\cdot\vec v &= 0.
\end{align*} 
This system is linear in both $\vec v$ and $p$. 

There exist more complicated constitutive models for instance, for compressible fluids, where one has to simultaneously solve conservation of mass, momentum and energy, and not just conservation of momentum as we have been studying. From compressible flow one can deduce wave equations for (mechanical) sound waves, where 
\begin{equation*}
    \nabla^2 p - \frac{1}{c^2}\frac{\partial^2 p}{\partial t^2} = 0,
\end{equation*}
where $p$ is the fluid pressure and $c$ is the speed of sound. Another important case involves adding a thermal component to the fluid density, where we define 
\begin{equation*}
    \rho = \rho_0 (1-\alpha \theta),\qquad \theta = T - T_0,
\end{equation*}
where $\rho_0$ is a reference, constant density, $\alpha$ is a thermal expansion coefficient, $\theta$ is the temperature change from the reference $T_0$, and $T$ is the current temperature. Assuming that density fluctuations satisfy $\rho\frac{D\vec v}{Dt}\approx \rho_0\frac{D\vec v}{Dt}$, and that body forces are due to gravity, i.e.
\begin{equation*}
    \vec f = \rho\vec g = \rho_0\vec g - \rho_0\alpha\theta \vec g = \nabla_x(-\rho_0 g z) - \rho_0\alpha\theta\vec g,
\end{equation*}
where $\vec g = (0,0,-g)$ is the gravitational acceleration vector, we substitute in the conservation of linear momentum, which yields 
\begin{equation*}
    \rho_0\left(\frac{\partial\vec v}{\partial t}+\vec v \cdot\nabla_x\vec v\right) = -\nabla_x(p+\rho_0 g z) + \mu\nabla_x^2\vec v - \rho_0 \alpha  \theta\vec g.
\end{equation*} 
Simplifying the energy conservation equation in this settings, we obtain the time-dependent convection-diffusion equation 
\begin{equation*}
    \rho_0 c_P\left(\frac{\partial \theta}{\partial t} + \vec v \dot\nabla_x\theta\right) = k\nabla_x^2\theta + r_0.
\end{equation*}
We can equip these equations with the incompressibility condition $\nabla_x\cdot\vec v = 0$, and thus we have a 5-equation system for $\vec v$, $p$ and $\theta$, which can be solved from appropriate initial and boundary conditions. 

Other thermal and non-thermal fluid models are used in practice for different purposes, such as biofluid modeling, atmospherical physics and oceanographic simulations. An example of the latter case consists of integrating a non-inertial reference frame to the Navier-Stokes equations, resulting in an additional Coriolis acceleration term of the form $2\Omega \sin(\phi)$, where $\Omega$ is the (approximately) constant Earth angular velocity, and $\phi$ is the latitude, and an additional drag term $-k\vec v$ where $k$ is a drag coefficient. These equations are solved in 2D, where the third dimension is rewritten as $z = H+h$, where $H$ is the ocean floor average height, and $h$ is the distance to the ocean floor, resulting in the third equation
\begin{equation*}
    \frac{\partial h}{\partial t} + \nabla_x((H+h)\vec v) = 0.
\end{equation*}
These equations are known as the \textit{shallow water equations}, and are used to model floodings and tsunamis.  
\bibliography{main}
\bibliographystyle{alpha}
\end{document}

