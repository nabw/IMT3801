The most direct path to solving a differential equation numerically is to approximate the derivatives themselves. This is the central idea of the \emph{finite difference method} (FDM), which replaces the continuous differential operators with discrete analogues derived from Taylor's theorem. Instead of seeking a solution function over the entire domain, we seek its values on a discrete set of points, i.e. a \emph{grid}. This method stands in contrast to the variational methods we will study later, which are based on weak formulations, and its strength lies in its simple and intuitive nature, making it a good starting point tackling the analysis of approximation methods for PDEs. 

In this chapter, we will formalize the concepts of \emph{consistency}, \emph{stability} and \emph{convergence}, and establish their connection through the \emph{Lax equivalence theorem}, which states that for a consistent scheme, stability is equivalent to convergence. We will begin by deriving first and second-order difference operators in one dimension and analyzing their convergence properties. We will then extend these ideas to higher-dimensional problems on structured grids and introduce the essential techniques for handling time-dependent equations.

This section has been partly extracted from some notes from the WIAS in Berlin. At the time of writing these notes, the original document did not list an author, and we gratefully acknowledge the unknown contributor. The notes can be found in \href{https://www.wias-berlin.de/people/john/LEHRE/NUMERIK\_IV\_21\_22/num\_konv\_dom\_prob\_3.pdf}{this website}.

\section{First and second order discrete derivatives}\label{sec:first-second-fd}
Probably the simplest (and oldest) idea for approximating equations stems from the use of Taylor's theorem:
\begin{theorem}\label{thm:taylor}
    Consider a function $f$ in $C^n((a,b), \R)$, i.e. $n$ times differentiable whose derivatives are well-defined in $a$. Then, there is some $\xi$ in $(a,b)$ such that
    \begin{equation}
        f(b) = \sum_{k=0}^{n-1}\frac{f^{(k)}(a)}{k!}(b-a)^k + \frac{f^{(n)}(\xi)}{n!}(b-a)^n.
    \end{equation}
\end{theorem}
Naturally, this arises from the mean value theorem when $n=1$. From now on we consider our setting to be in $\R$. For any continuous function $f:(a,b)\to \R$, we can define its associated \emph{grid function} given by its evaluation in a finite set of points. We sometimes refer to this as \emph{collocation}. Set $a<b$, and define the points $a=x_1<\hdots<x_N=b$ with $h_i=x_{i}-x_{i-1}$, such that we have $N$ points, and define the grid operator $R_h$ and the grid function $R_h f$ as
\begin{equation}\label{eq:def-grid-function}
    \begin{aligned}
        R_h:C((a,b),\R)&\to [\R^N\to \R^N]\\
        f &\mapsto R_h f \coloneqq (f(x_1), \dots, f(x_N)).
    \end{aligned}
\end{equation}
We can then define the following difference operators: 
\begin{itemize}
    \item Forward difference: 
    \begin{equation*}
        D^+f(x_i) \coloneqq \frac{f_{i+1} - f_i}{h_{i+1}}
    \end{equation*}
    \item Backward difference: 
    \begin{equation*}
        D^-f(x_i) \coloneqq \frac{f_{i} - f_{i-1}}{h_{i}}
    \end{equation*}
    \item Centered difference:
    \begin{equation*}
        Df(x_i) \coloneqq \frac{f_{i+1} - f_{i-1}}{h_{i+1} + h_{i-1}}
    \end{equation*}
\end{itemize}
A nice simple exercise is to extend these ideas to second order derivatives, which all stem from Taylor's formula. The application we will be interested is in that of approximating the action of a differential operator using these formulas. For this, we will focus on the forward difference formula. At each grid point we will have
\begin{equation}
    D^+f(x_i) = \frac{f_{i+1} - f_i}{h_{i+1}} = \frac 1{h_{i+1}} [-1\quad 1]\begin{bmatrix}f_i \\ f_{i+1}\end{bmatrix},
\end{equation}
so that defining the matrix $\mat D_h^+$ in $\R^{N\times N}$ as
\begin{equation}
    [\mat D_h^+]_{ij} = \begin{cases} -1/h_{i+1} & j=i \\ 1/h_i  & j=i+1 \\ 0 & \text{elsewhere} \end{cases}
\end{equation}
we obtain an operator which, given a grid function, it yields its discrete derivative. Naturally, care has to be taken at the last point, where we can simply use instead a backwards difference. 

The standard way of approximating second order derivatives is that of using a finite difference approximation twice. It is given by the following: 
\begin{tightalign}
    f''(x) &\approx D^+(f')(x) \notag \\
           &=\frac{f'(x+h) - f'(x)}{h} \notag\\
           &\approx\frac{\frac{f(x+h) - f(x)}{h} - \frac{f(x) - f(x-h)}{h}}{h} \notag\\
           &= \frac{f(x+h) - 2f(x) + f(x-h)}{h^2}.
\end{tightalign}
We note that the intermediate steps show that these computations can be represented as the matrix product $\mat D^+ \mat D^-$, where the order of differentiation can be inverted and thus equivalently $\mat D^- \mat D^+$. 

The order of convergence of the previously computed schemes can be derived from Taylor's theorem. 
\begin{figure}[!h]
    \centering
    \begin{tikzpicture}
        \begin{loglogaxis}[ 
            xlabel={$h$}, 
            ylabel={$\|y' - Dy\|_{\infty}$}, 
            legend style={at={(1.05,0.5)}, anchor=west, column sep=1ex, legend columns=1, align=left}, 
            grid=major,
            width=10cm, height=6cm, % Adjust the size if needed
            ]
            \addplot[
                blue, 
                line width=1pt
            ] table [x=h, y=fd, col sep=comma] {convergence_rates_FD.csv};
            
            \addplot[
                orange, 
                dashed, 
                line width=1pt
            ] table [x=h, y=bd, col sep=comma] {convergence_rates_FD.csv};
            
            \addplot[
                green, 
                line width=1pt
            ] table [x=h, y=cd, col sep=comma] {convergence_rates_FD.csv};

            % Legend
            \legend{forward, backward, centered}
        \end{loglogaxis}
    \end{tikzpicture}
    \caption{Convergence of forward, centered and backward difference schemes for calculating the first derivative of $u(x)=\sin(2\pi x)$. Note that the centered difference error starts increasing for $h\lesssim 10^{-6}$.}
    \label{fig:fd_convergence}
\end{figure}

We can easily derive the order of convergence for sufficiently smooth functions. To see this, consider Taylor's theorem to obtain for $h>0$:
\begin{equation*}
    f(x+ h) = f(x) + f'(x)h + O(h^2),
\end{equation*}
from which we readily obtain that $| D^+f(x) - f'(x) | = O(h)$, so that forward differences and also backward differences (through a straightforward calculation) converge pointwise linearly. A better rate is obtained through centered differences, which converge pointwise quadratically. To justify this, we compute the third-order expansions
\begin{tightalign*}
    f(x+h) &= f(x) + hf'(x) + \frac {h^2} 2 f''(x) + \mathcal O(h^3) \\
    f(x-h) &= f(x) - hf'(x) + \frac {h^2} 2 f''(x) + \mathcal O(h^3),
\end{tightalign*}
and the conclusion follows from adding both equations. However, it is noteworthy that although the convergence is quadratic for large $h$, they become unstable for very small $h$, see Figure~\ref{fig:fd_convergence}.

\paragraph{The empirical convergence rate} One issue that is always overlooked is the computation of an \emph{empirical} convergence rate, so we will write it here once and for all so that it is never an issue again. Assume that some error $\vec e_h$ converges to zero in some $X$ norm with rate $k$:
\begin{equation*}
    \|\vec e_h\|_X \leq C h^k.
\end{equation*}
Then, given a sequence of mesh sizes $h$ and errors as $(h_j, [\vec e_h]_j)$, we would like to estimate $k$ to validate we really obtained the computed rate $k$. For this, we take the logarithm of the error expression:
\begin{equation*}
    \log \|[\vec e_h]_j\|_X \leq \log C + k \log h_j,
\end{equation*}
and note that we obtain roughly a line equation if we assume an equality. Because of this, we can estimate now estimate the slope of our line equation using two consecutive points as
\begin{equation*}
    k \approx \frac{\log \|[\vec e_h]_{j+1}\|_X - \log \|[\vec e_h]_j\|_X}{ \log h_{j+1} - \log h_j},
\end{equation*}
which yields the classical expression
\begin{equation}
    k \approx  \frac{\log \left(\|[\vec e_h]_{j+1}\|_X / \|[\vec e_h]_j\|_X\right)}{ \log \left(h_{j+1} / h_j\right)}.
\end{equation}
This is also referred to sometimes as the \emph{numerical convergence rate}. 

\section{Stability and convergence analysis}\label{sec:fd-convergence-analysis}
We can now propose a mechanism for discretizing differential operators. A central question regards the capability of approximation that such an objects has, which finds its answer in the Lax equivalence theorem. We will first require other definitions, and we will assume that our differential operators are \emph{densely-defined}, a property that is studied more carefully in Section~\ref{chapter:sturm-liouville}.

\begin{definition}[Consistency]
    Consider some differential operator $\mathcal A$ that is densely-defined. A discrete operator $\mat A_h$ in $\R^{N\times N}$ is said to be \emph{consistent} with $\mathcal A$ of order $k$ if for all sufficiently smooth functions it holds that
    \begin{equation}\label{def:consistency}
        \max_{i\in\{1,\hdots,N\}}|\mathcal Af(x_i) - [\mat A_h R_h f]_i| =: \|R_h \mathcal Af - \mat A_hR_h f\|_{\infty,h} \leq C h^k,
    \end{equation}
    where $\|\cdot \|_{\infty,h}$ is the induced infinity norm in $\R^N$ and $C$ is a positive constant independent of $h$. This is sometimes stated more weakly by saying that the norm is $\mathcal O(h^k)$. 
\end{definition}

A simple example is to verify that all finite difference operators are consistent with the first order derivative, which is a consequence of Taylor's theorem.
\begin{definition}[Stability]
    A finite difference operator $\mat A_h$ is said to be stable (in the discrete maximum norm) if there is a stability constant $C>0$ that does not depend on the grid size such that
    \begin{equation}\label{def:stability}
        \| \vec v_h \|_{\infty,h} \leq C \|\mat A_h \vec v_h \|_{\infty,h}
    \end{equation}
    for all grid functions $\vec v_h$. 
\end{definition}
We finally consider an infinite dimensional problem given by
\begin{equation}
    \mathcal A u = f
\end{equation}
and its finite difference approximation as
\begin{equation}\label{def:fd-approx}
    \mat A_h \vec u_h = \vec f_h,
\end{equation}
which will allow us to define convergence. 

\begin{definition}[Convergence]
    The finite difference scheme~\ref{def:fd-approx} is said to be convergent of order $k$ in the discrete maximum norm if the discrete solution and the continuous one satisfy that 
    \begin{equation}\label{def:fd-convergence}
        \| \vec u_h - R_h u \|_{\infty,h} \leq C h^k.
    \end{equation}
    This is sometimes stated more weakly by saying simply that $\| \vec u_h - R_h u \|_{\infty,h}$ is $\mathcal O(h^k)$. 
\end{definition}
\begin{theorem}[Lax equivalence theorem]\label{thm:lax-equivalence}
    Let $ \mat A_h \vec u_h = \vec f_h$ be the finite difference discretization for the problem $\mathcal A u = f$ as defined above. If $A_h$ is consistent, then stability and convergence are equivalent. 
    \begin{proof}
        The stability to convergence part is taken directly from the aforementioned notes. The convergence to stability is instead partly inspired by the notes by Long Chen on abstract convergence analysis~\cite{chenLFDM}.
        \begin{itemize}
            \item Stability $\implies$ convergence: we bound the convergence error as
            \begin{tightalign*}
                \| \vec u_h - R_h u\|_{\infty,h} &\leq C\|\mat A_h(\vec u_h - R_hu) \|_{\infty,h}  \tag{stability} \\
                &=C \| f_h - \mat A_hR_h u \|_{\infty,h} \\
                &=C \| R_h f - \mat A_hR_h u\|_{\infty,h}\\
                &=C \|R_h(\mathcal A u) - \mat A_hR_h u\|_{\infty,h} \\
                &\leq Ch^k  \tag{consistency}.
            \end{tightalign*}
            \item Convergence $\implies$ stability: by taking inverses, we can write stability as 
                \begin{equation*}
                    \| \mat A_h^{-1}\vec f_h \|_{\infty, h} \leq C \|\vec f_h\|_{\infty, h},
                \end{equation*}
                which is simply the continuity of the inverse map $\mat A_h^{-1}$. Consider an arbitrary grid vector $\vec f_h$, and we denote the associated problem solution as $\vec u_h$, where $\mat A_h \vec u_h = \vec f_h$. Using this, we have that
                \begin{tightalign*} 
                    \|\mat A_h^{-1}\vec f_h\|_{\infty,h} = \| \vec u_h \|_{\infty, h} &\leq \| \vec u_h - R_h u \|_{\infty, h} + \| R_h u \|_{\infty, h}    \tag{convergence}\\
                    &\leq C h^k + \| R_h \mathcal A^{-1} f \|_{\infty, h}. \tag{stability}
                \end{tightalign*}
                To bound this last term, we note that $R_h$ yields pointwise evaluations, so for any continuous function $\eta$ it will hold that
                \begin{equation*}
                    \| R_h \eta \|_{\infty,h} \leq \| \eta \|_0,
                \end{equation*}
                where $\| \eta \|_0$ is the supremum norm. Finally, we use that $\mathcal A$ is a continuous bijection, and thus from the open mapping theorem it holds that $\|\mathcal A^{-1}\|$ is bounded by some constant $C_2$. Using this fact, the previous estimate becomes 
                \begin{equation*}
                    \| \mat A_h^{-1} \vec f_h \|_{\infty,h} \leq Ch^k + \| \mathcal A f \|_0 \leq C h^k + C_2 \| \vec f_h \|_0<\infty,
                \end{equation*}
                which in particular shows that $ \| \mat A_h^{-1} \vec f_h \|$ is bounded for all $\vec f_h$. The uniform boundedness principle then yields that the operator $\mat A_h^{-1}$ is bounded, which concludes the proof.
        \end{itemize}
    \end{proof}
\end{theorem}

\paragraph{Method of manufactured solutions} A practical difficulty in computing the convergence error $\|u - u_h\|$ is that the exact solution $u$ is often unknown for non-trivial problems. The \emph{method of manufactured solutions} is a standard technique that circumvents this issue. The procedure is as follows:
\begin{enumerate}
    \item Choose, or ``manufacture'', a sufficiently smooth function $u^{\text{ex}}$ that will serve as the exact solution. This function should ideally exercise various features of the code and be simple enough to qualitatively analyze once the solution is computed. The most common choices are trigonometric and polynomial functions.
    \item Apply the continuous differential operator $\mathcal{A}$ to this manufactured solution to generate a source term $f^{\text{ex}} \coloneqq \mathcal{A}u^{\text{ex}}$.
    \item Solve the modified problem $\mathcal{A}u = f^{\text{ex}}$ with the corresponding boundary conditions derived from $u^{\text{ex}}$, to find a discrete solution $u^{\text{ex}}_h$. When uniqueness holds, the solution to this problem is precisely $u=u^{\text{ex}}$, and thus the convergence error is computed as $\|u^{\text{ex}} - u^{\text{ex}}_h\|$.
\end{enumerate}
This technique allows for a direct and rigorous computation of the convergence error and its rate. An important remark is that one must be careful with the choice of $u^{\text{ex}}$, since for a fixed discrete method, some $u^{\text{ex}}$ may be inherently easier or harder to approximate, leading to deceptively low or high numerical convergence errors, and incorrect rates. This may be a desirable effect when comparing methods, but it may not be desirable when one is analyzing a different error behavior, for instance with respect to the mesh size.

\paragraph{Stability of the discrete Laplacian in 1D}
Let us now study the stability of the discretized Laplacian operator in 1D, $-\Delta u = -u''$, over the interval $[a,b]$ with $a<b$ and homogeneous boundary conditions $u_h(x_1) = u_h(x_N) = 0$. Consider $\Omega_h = \{x_i\}_{i=1}^N$ a grid of $[a,b]$ with $N$ equispaced elements, where $x_1=a$ and $x_{N}=b$, and mesh size $h=(b-a)/N$. Recall that the numerical second derivative can be approximated by the three-point approximation derived above, which for a given interior point is given by
\begin{equation}
    -u_h''(x_i) =  \frac{1}{h^2}(-u_h(x_{i-1}) + 2u_h(x_i) - u_h(x_{i+1})).
\end{equation}
We note that in our discretized system we do not need to solve for $u_h(x_1)=u_h(x_N)=0$ because of the boundary condition. This implies that the derivative at $x_2$ and $x_{N-1}$ is
\begin{tightalign*}
    -u_h''(x_2) &= \frac{1}{h^2}(\cancel{-u_h(x_{1})} + 2u_h(x_2) - u_h(x_{3})) = \frac{1}{h^2}(2u_h(x_2) - u_h(x_{3}))\\
    -u_h''(x_{N-1}) &= \frac{1}{h^2}(-u_h(x_{N-2}) + 2u_h(x_{N-1}) - \cancel{u_h(x_{N})}) = \frac{1}{h^2}(-u_h(x_{N-2}) + 2u_h(x_{N-1})),
\end{tightalign*}
and thus the finite difference operator associated with $-\Delta$ is
\begin{equation}\label{def:fd-operator-laplacian}
    A_h = \frac{1}{h^2} \begin{bmatrix} 2 & -1 & 0 & \cdots & 0 \\ -1 & 2 & -1 & \cdots & 0 \\ 0 & -1 & 2 & \cdots & 0 \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & 0 & \cdots & 2\end{bmatrix}.
\end{equation}
where we have $-\Delta u \approx A_h u_h$. To show stability, we will use that it actually satisfies a discrete maximum principle. 
\begin{theorem}[Discrete maximum principle (DMP)]
    Let $A_h$ be the finite difference operator defined above. Then, for any grid function $u_h$ such that $A_h u_h \geq 0$, it holds that
    \begin{equation}
        \max_{x_i\in \Omega_h} u_h \leq \max_{x_i\in \Gamma_h} u_h,
    \end{equation}
    where $x_i$ is an interior node of $\Omega_h$ and the equality holds if and only if $u_h$ is constant.
    \begin{proof}
        We proceed by contradiction, so we assume that $u_h(x_i) \overset{(*)}{=} \max_{x\in \Omega_h} u_h > \max_{x\in \Gamma_h} u_h$, where this maximum is achieved at the internal node $x_i\in \Omega_h$. Then, from the centered difference scheme we get
        \begin{equation*}
            2u_h(x_i) = u_h(x_{i-1}) + u_h(x_{i+1}) - h^2 \underbrace{A_h u_h(x_i)}_{\geq 0} \leq u_h(x_{i-1}) + u_h(x_{i+1}) \overset{(*)}{\leq} 2u_h(x_i),
        \end{equation*}
        which implies that $u_h(x_i)$ is constant in the discrete neighborhood of $x_i$. Repeating this argument recursively for all interior points, we conclude that $u_h(x_i)$ is constant in $\Omega_h$, which contradicts the assumption. Hence, the discrete maximum principle holds.
    \end{proof}
\end{theorem}
We can apply this principle to show that $A_h$ is stable. 
\begin{theorem}
    Let $\Omega$ be a domain with boundary $\Gamma$, and $A_h$ the finite difference operator associated to the problem
    \begin{equation}
        \begin{aligned}
            -\Delta u &= f && \tin \Omega, \\
            u &= g && \ton \Gamma.
        \end{aligned}
    \end{equation}
    Then, if $A_h u_h = f_h$ in $\Omega_h$ and $u_h = g_h$ in $\Gamma_h$, it holds that
    \begin{equation}
        \|u\|_{\infty,h} \leq C\left(\|f\|_{\infty, h} + \|g\|_{\infty, h}\right),
    \end{equation}
    where we write $\|g\|_{\infty, h} = \sup_{x_i\in\Gamma_h}|g(x_i)|$. 
    \begin{proof}
        We will prove the theorem over $\R$. Using the comparison function $\phi(x) = \frac{c}{2}x^2$ where $c=\|f\|_{\infty, h}$ and consider a constant $M$ such that $\phi(x) \leq cM$ in $\Omega$. Using that $A_h \phi_h = c$ (inspired by $\phi''(x) = c$), we get
        \begin{equation*}
            A_h(u_h + \phi_h) = A_h u_h + c = f_h + c \geq 0,
        \end{equation*}
        and using the discrete maximum principle (DMP) we obtain
        \begin{equation*}
            \max_{\Omega_h} u_h \leq \max_{\Omega_h} (u_h + \phi_h) \overset{\text{DMP}}{\leq} \max_{\Gamma_h} (u_h + \phi_h) = \|g_h\|_{\infty, h} + cM = \|g_h\|_{\infty, h} + M \|f\|_{\infty, h},
        \end{equation*}
        and repeating the argument for $-u_h$ yields the lower bound for $\|u_h\|_{\infty,h}$. This shows that $A_h$ is stable.
    \end{proof}
\end{theorem}

\paragraph{Unavoidable nonlinearities}
Sometimes, specially in stationary models, we cannot avoid having to solve a nonlinear problem. Two common ways to obtain an approximate solution to a nonlinear problem are 1) fixed point methods (usually of first order), and 2) Newton-Raphson methods. To analyze them, consider the domain $\Omega=(a,b)$ and the example problem
\begin{equation}
    \begin{aligned}
        -u'' + \sin(u) &= 0\\
        u(a) &= u^a\\
        u(b) &= u^b.
    \end{aligned}
\end{equation}
As usual, we set a discrete grid of $N$ equispaced interior points $a=x_1<\cdots<x_N=b$. Thus, at each non-boundary node $x_i$ we have the equation
\begin{equation}
    -\frac{1}{h^2}(u^{i+1}-2u^i+u^{i-1}) + \sin(u^i) = 0.
\end{equation}
We write $g(\vec u_h) = \{g(u^i)\}$ for the vector of evaluations of $g$ at the non-boundary nodes. This allows us to write more compactly the root-finding problem
\begin{equation}
    F(\vec u_h) \coloneqq  \mat A_h \vec u_h + \sin(\vec u_h) = 0.
\end{equation}
Let us analyze both methods for solving this problem.
\begin{enumerate}
    \item \emph{Newton-Raphson}: we look for solutions of a linearized problems and iterate. Abstractly, we solve the equation $F(\vec x)=0$ by considering an initial point $\vec x^0$ and then using Taylor's theorem for linearizing $F(\vec x^{k+1})$ (unknown) around the previous iteration result $\vec x^k$ (known):
    \begin{equation*}
        F(\vec x^{k+1}) \approx F(\vec x^k) + \nabla F(\vec x^k)\cdot \underbrace{\vec{\delta x}^{k+1}}_{\vec x^{k+1} - \vec{x}^k}.
    \end{equation*}
    We can now solve a system for $\vec{\delta x}^{k+1}$ such that $F(\vec x^{k+1}) = 0$, that is,
    \begin{equation*}
        \underbrace{[\nabla F(\vec x^k)]}_{\text{matrix}} \underbrace{\vec{\delta x}^{k+1}}_{\text{vector}} = \underbrace{-F(\vec x^k)}_{\text{vector}},
    \end{equation*}
    along with the update step
    \begin{equation*}
        \vec x^{k+1} = \vec{x}^k + \vec{\delta x}^{k+1}.
    \end{equation*}
    We call such a system a \emph{Newton iteration} or \emph{tangent system}. Also, $\vec{\delta x}^{k}$ must have homogeneous (zero) boundary conditions, so that all $\{\vec x^{k}\}_k$ satisfy the original ones. 
    
    In our problem, the $i$-th component of $F(\vec u_h)$ is given by
    \begin{equation*}
        [F(\vec u_h)]^i = -\frac{1}{h^2}(u^{i-1}-2u^i+u^{i+1}) + \sin(u^i),
    \end{equation*}
    which we differentiate to get
    \begin{equation*}
        [\nabla F(\vec u_h)]_{ij} = \frac{\partial [F(\vec u_h)]^i}{\partial u^j} = \begin{cases}
            -\frac{1}{h^2} &j\in\{i-1,i+1\}\\
            \frac{2}{h^2} + \cos(u^i) &j=i\\
            0 &\text{elsewhere.}
        \end{cases}
    \end{equation*}
    We can write more compactly this gradient as
    \begin{equation*}
        \nabla F(\vec u_h) = \mat A_h + \bolddiag(\cos(\vec u_h)),
    \end{equation*}
    which allows us to write the tangent system as
    \begin{equation*}
        \left(\mat A_h + \bolddiag(\cos(\vec u^k))\right)\vec{\delta u}^{k+1} = -\left(\mat A_h \vec{u}^k + \bolddiag(\sin(\vec u^k))\right).
    \end{equation*}
    \item \emph{Fixed point}: similarly to the time-dependent case, we can \emph{delay} the evaluation of a nonlinearity. We do this by choosing a starting point $\vec u^0$ and evaluating the nonlinear part in the previous iteration $\vec u^k$. This yields
    \begin{equation*}
        -\Delta \vec u^{k+1} + \sin(\vec u^k) = 0.
    \end{equation*}
    Note that we can enforce this equality at the discrete level as well, i.e. 
    \begin{equation*}
        -A_h \vec u_h^{k+1} + \sin(\vec u_h^k) = 0,
    \end{equation*}
    where $\vec u_h^k$ is the discrete vector $\vec u_h$ at the $k$-th iteration. In such cases, we can sometimes prove the convergence of the scheme. For that, we consider the discrete iterates $\vec u_h^k$ given by
    \begin{tightalign*}
        \mat A_h \vec u_h^{k+1} + \sin(\vec u_h^k) &= 0 \\
        \mat A_h \vec u_h^{k} + \sin(\vec u_h^{k-1}) &= 0,
    \end{tightalign*}
    which subtracted give the equation
    \begin{equation*}
        \mat A_h(\vec u_h^{k+1} - \vec u_h^{k}) = -(\sin(\vec u_h^k) - \sin(\vec u_h^{k-1})).
    \end{equation*}
    We typically call this equation an \emph{error equation}. Finally, we can use the fact that $\mat A_h$ is stable, so we can bound the error norm as
    \begin{tightalign*}
        \|\vec u_h^{k+1} - \vec u_h^{k}\|_{\infty, h} &\leq C\|\mat A_h (\vec u_h^{k+1} - \vec u_h^{k})\|_{\infty, h} \tag{Stability}\\
        &= C\|-(\sin(\vec u_h^k) - \sin(\vec u_h^{k-1}))\|_{\infty, h}\tag{Error equation}\\
        &\leq C\|\vec u_h^{k+1} - \vec u_h^{k}\|_{\infty, h}.\tag{$\sin$ is $1$-Lipschitz}
    \end{tightalign*}
    Thus, as justified by the Banach fixed point theorem, which we will study in more detail in Section~\ref{sec:fixed-point-theorems}, the scheme is convergent if $C<1$, i.e. the iterative map is a \emph{contraction}. Otherwise, the scheme \emph{can converge}, but we have no theoretical guarantee from this analysis. Still, each fixed point iteration
    \begin{equation*}
        \mat A_h \vec u_h^{k+1} = - \sin(\vec u_h^k)
    \end{equation*}
    is much simpler than a Newton iteration
    \begin{tightalign*}
        \left(\mat A_h + \bolddiag(\cos(\vec u^k))\right)\vec{\delta u}^{k+1} &= -\left(\mat A_h \vec{u}^k + \sin(\vec u^k)\right)\\
        \vec u_h^{k+1} &= \vec{u}^k_h + \vec{\delta u}_h^{k+1}.
    \end{tightalign*}
\end{enumerate}
Both fixed point and Newton-Raphson methods are commonly used strategies, and in general there is no universally better choice. We study these nonlinear methods more thoroughly in Chapter~\ref{chapter:nonlinear}.

\section{Finite differences in 2D and 3D}\label{sec:fd-high-dimensions}
In this section we hint on how to address numerically higher dimensional problems, and also how to include time. The stability of such schemes will be studied further ahead with the von Neumann stability analysis. In typical 1D examples, one has a domain $(a,b)$ and an associated grid given by some points $\{x_i\}_i^N$ such that each index $i$ corresponds to an order of the domain, and thus naturally a finite difference operator is going to involve something like $i$ and its neighbors. This scenario would be different if, for example, one had a random ordering of the grid points such as
\begin{equation*}
    x_0 = a,\; x_7 = a + h,\; x_{12} = a + 2h,\; \hdots, x_N = b,
\end{equation*}
so that in such a grid, a forward difference with respect to the point $x_0$ would be given by
\begin{equation*}
    D^+f(x_0) = \frac 1 h \left( f^7 - f^0 \right).
\end{equation*}
Although this is an unnatural way to do things in 1D, it is the natural scenario arising in higher dimensions. For this, and for the sake of simplicity, we will restrict the presentation to meshes given by tensor products of intervals, i.e. $\Omega = (a_x, b_x) \times (a_y, b_y)$ in 2D and $\Omega = (a_x, b_x) \times (a_y, b_y) \times (a_z, b_z)$ in 3D. The main difficulty will be that of creating an ordering of the degrees of freedom.

Let us now consider the 2D case. Consider $\Omega = (a_x, b_x) \times (a_y, b_y)$ and an equispaced discrete grid $\Omega_h$ with $ a_x = x_0, \hdots, x_{N_x-1} = b_x$ and $a_y = y_0, \hdots, b_y = y_{N_y-1}$, where $N_x$ and $N_y$ are number of points in each subdomain. Then, each grid point will be given by     
\begin{equation}
    \vec X_{ij} \coloneqq (x_i, y_j) \qquad \forall i,j \in \{0,\hdots, N_x-1\}\times \{0,\hdots, N_y-1\},
\end{equation}
and analogously we can define a (matrix) grid function associated to some continuous function $f:\Omega\to \R$ as 
\begin{equation*}
    \vec f^{ij} \coloneqq f(\vec X_{ij}) = f(x_i, y_j) \in \R^{N_x}\times \R^{N_y}
\end{equation*}
We can flatten 2D matrices to obtain 1D vectors through the definition of an \emph{index map} $I: \{0,\hdots, N_x-1\} \times \{0, \hdots, N_y-1\}\to \{0,\hdots, N_x N_y-1\}$ given by 
\begin{equation*}
    I \coloneqq I(i,j) = i + j N_x.
\end{equation*}
Fortunately, this map can be inverted as
\begin{equation*}
    I \mapsto (i(I), j(I)) \coloneqq (I\mod N_x, \lfloor I/N_x \rfloor),
\end{equation*}
and so we can uniquely define the 1D vector $\vec f^I$ as the vector of all grid points in $\Omega$ as
\begin{equation*}
    \vec f^I \coloneqq \vec f^{i(I)j(I)} = f(\vec X_{i(I)j(I)}) \in \R^{N_x N_y}.
\end{equation*}
For example, if one has a grid with $N_x=3$ and $N_y=2$, then the forward and inverse index maps are given as in Table 2.1.
\begin{table}[ht!]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \centering
    \begin{tabular}{c c | c}
        \toprule $i$ & $j$ & $I(i,j)$ \\ \midrule
        0 & 0 & 0 \\
        1 & 0 & 1 \\
        2 & 0 & 2 \\
        0 & 1 & 3 \\
        1 & 1 & 4 \\
        2 & 1 & 5 \\ \bottomrule
    \end{tabular}
    \caption{Forward map}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \centering
    \begin{tabular}{c | c c}
        \toprule $I$ & $i(I)$ & $j(I)$ \\ \midrule
        0 & 0 & 0 \\
        1 & 1 & 0 \\
        2 & 2 & 0 \\
        3 & 0 & 1 \\
        4 & 1 & 1 \\
        5 & 2 & 1 \\ \bottomrule
    \end{tabular}
    \caption{Backward map}
    \end{subfigure}
    \caption{Forward and inverse index maps for a 2D example.}
\end{table}

Using these maps, we define a forward difference in $x$ and $y$ directions as $D^+_x f(\vec X)$ and $D^+_y f(\vec X)$ and note that they are given by
\begin{tightalign*}
    D^+_x f(\vec X_{ij}) = \frac 1{h_x} \left(\vec f^{(i+1)j} - \vec f^{ij}\right)= \frac 1 {h_x} \left( \vec f^{I(i+1,j)} - \vec f^{I(i,j)}\right), \\
    D^+_y f(\vec X_{ij}) = \frac 1{h_y} \left(\vec f^{i(j+1)} - \vec f^{ij}\right) = \frac 1{h_y} \left(\vec f^{I(i,j+1)} - \vec f^{I(i,j)}\right),
\end{tightalign*}
where we have used a slight abuse of notation by denoting both the 2D vector and 1D vectors as $\vec f^{ij}$ and $\vec f^I$ respectively. Further, the second derivative operators can be defined as 
\begin{tightalign*}
    D_{xx} f(\vec X_{ij}) &= \frac{1}{h_x^2} \left( -\vec f^{(i+1)j} + 2\vec f^{ij} - \vec f^{(i-1)j}\right) = \frac{-1}{h_x^2} \left(\vec f^{I(i+1,j)} - 2\vec f^{I(i,j)} + \vec f^{I(i-1,j)}\right) = A_h f(\vec X_{ij}), \\
    D_{yy} f(\vec X_{ij}) &= \frac{1}{h_y^2} \left( -\vec f^{i(j+1)} + 2\vec f^{ij} - \vec f^{i(j-1)}\right) = \frac{1}{h_y^2} \left( \vec f^{I(i,j+1)} - 2\vec f^{I(i,j)} + \vec f^{I(i,j-1)}\right) = f(\vec X_{ij})A_h^\top,
\end{tightalign*}
where $A_h$ is the discrete second derivative matrix. Let us now apply this for the Laplace equation with zero boundary conditions, that is,
\begin{equation}
    \begin{aligned}
        -\Delta u &= f && \tin \Omega, \\
        u &= 0 && \ton \partial\Omega.
    \end{aligned}
\end{equation}
Here, the corresponding discrete equation is
\begin{equation}
    -\Delta u(\vec X_{ij}) = - \frac{\partial^2}{\partial x^2}\vec u(X_{ij}) - \frac{\partial^2}{\partial y^2} u(\vec X_{ij}) \approx A_h u(\vec X_{ij}) + u(\vec X_{ij})A_h^\top  = f(\vec X_{ij}),
\end{equation}
which is a matrix equation since $u(\vec X_{ij})$ is a grid function. We define a suitable index map $I$, which allows us to write the 1D vectors $u^I$ and $f^I$, and now it remains to define the operator matrix for these 1D vectors. For this, we use the Kronecker product $\otimes$ defined by
\begin{equation*}
    \begin{aligned}
        \otimes : \R^{m\times n} \times \R^{p\times q} &\to \R^{mp\times nq}, \\
        (A,B) &\mapsto A\otimes B \coloneqq \begin{bmatrix}
            A_{11}B & A_{12}B & \cdots & A_{1n}B \\
            A_{21}B & A_{22}B & \cdots & A_{2n}B \\
            \vdots & \vdots & \ddots & \vdots \\
            A_{m1}B & A_{m2}B & \cdots & A_{mn}B
        \end{bmatrix}. \\
    \end{aligned}
\end{equation*}
Note that the meaning of $\otimes$ in this context is different to the tensor product defined in~\ref{eq:tensor-product-definition}. This operation is needed to correctly define the operator matrix. For this, we need a brief lemma to calculate this matrix. 
\begin{lemma}
    Let $A\in R^{m\times n}$, $B\in \R^{p\times q}$ and $X\in \R^{q\times n}$. Denote the vector form of a matrix (in our case, a grid function) $A$ as $\vec A^I$, $A_i\in \R^n$ the rows of $A$ and $X_i\in \R^q$ the columns of $X$. Then, 
    \begin{equation*}
        (BXA^\top)^I = (A\otimes B)\vec X^I.
    \end{equation*}
    \begin{proof}
        We expand from the right hand side:
        \begin{tightalign*}
            (A\otimes B)\vec x^I &= \begin{bmatrix}
                A_{11}B & A_{12}B & \cdots & A_{1n}B \\
                A_{21}B & A_{22}B & \cdots & A_{2n}B \\
                \vdots & \vdots & \ddots & \vdots \\
                A_{m1}B & A_{m2}B & \cdots & A_{mn}B
            \end{bmatrix} \begin{bmatrix}
                X_1^\top \\
                X_2^\top \\
                \vdots \\
                X_n^\top
            \end{bmatrix} \\
            &= \begin{bmatrix}
                \sum_{i=1}^n A_{1i} B X_i\\
                \sum_{i=1}^n A_{2i} B X_i\\
                \vdots \\
                \sum_{i=1}^n A_{mi} B X_i
            \end{bmatrix}\\
            &= \begin{bmatrix}
                BXA_1^\top\\
                BXA_2^\top\\
                \vdots \\
                BXA_m^\top\\
            \end{bmatrix}\\
            &= (BXA^\top)^I.
        \end{tightalign*}
    \end{proof}
\end{lemma}
Now, we take the 1D vector form of the discrete Laplace equation and invoke the above lemma with $A=I$ the identity matrix:
\begin{tightalign*}
    \vec f^I &= (A_h u(\vec X_{ij}) + u(\vec X_{ij})A_h^\top)^I\\
    &= (A_h u(\vec X_{ij}) I)^I + (I u(\vec X_{ij})A_h^\top)^I\\
    &= (I\otimes A_h)\vec u^I + (A_h^\top \otimes I)\vec u^I\\
    &= (A_h\otimes I + I\otimes A_h)\vec u^I \tag{$A_h$ symmetric}.
\end{tightalign*}
Thus, our new operator is now $D^2_h = A_h\otimes I + I\otimes A_h$, and then the discrete Laplace equation is reduced to the linear system $D^2_h \vec u^I = \vec f^I$, where $D^2_h$ is sparse by construction. We can use the same methods as before to prove the stability of the scheme.  

\section{Time discretization}\label{sec:fd-time-discretization}
Time discretization is an enormous topic, so here we provide simple strategies and ways to think about them. We will dedicate an entire section to their analysis using the von Neumann stability analysis. We consider a more general setting with Bochner spaces in~\ref{chapter:dependent}. 

Consider a differential operator $A$ and its associated discrete matrix $\mat A_h$ which includes the problems boundary conditions. The continuous differential equation, for some initial condition $x(0) = x_0$, is given by 
\begin{equation}
    \dot x + A x = f,
\end{equation}
which we can discretize in space to obtain the system of differential equations
\begin{equation}
    \dot x_h + \mat A_h \vec x_h = \vec f_h.
\end{equation}
Instead of considering a Taylor approximation of the time derivative, we will use a quadrature rule to obtain different strategies. For this, integrating in the interval $(t^n, t^{n+1})$ and using the notation $\vec x(t^n) \approx \vec x^{n}$, yields
\begin{equation}
    \vec x^{n+1} - \vec x^n + \int_{t^n}^{t^{n+1}} \mat A_h \vec x_h(s) \,ds = \int_{t^n}^{t^{n+1}} \vec f_h(s)\,ds.
\end{equation}
We present three different schemes depending on three different quadrature rules: 
\begin{itemize}
    \item Left-sided rule (Explicit):
    \begin{equation}
        \int_{t^n}^{t^{n+1}} \mat A_h \vec x_h(s)\,ds \approx \Delta t \mat A_h\vec x_h^n.
    \end{equation}
    \item Right-sided rule (Implicit): 
    \begin{equation}
        \int_{t^n}^{t^{n+1}} \mat A_h \vec x_h(s)\,ds \approx \Delta t\mat A_h\vec x_h^{n+1}.
    \end{equation}
    \item Trapezoidal rule (Mid-point): 
    \begin{equation}
        \int_{t^n}^{t^{n+1}} \mat A_h \vec x_h(s)\,ds \approx \frac{\Delta t}{2}\left(\mat A_h\vec x_h^{n+1}+ \mat A_h \vec x_h^n\right).
    \end{equation}
    \end{itemize}
    The commonly acknowledged relevant points here are: 
    \begin{itemize}
        \item The explicit scheme yields a system that is easy to solve but unstable.
        \item The implicit scheme is harder to solve but more (usually unconditionally) stable.
        \item The mid-point or \emph{Crank-Nicholson} scheme is also stable, with better accuracy but involves more computations and possible numerical saturation at small timesteps.
    \end{itemize}
A natural generalization of these three schemes is the $\theta$-method, which for a fixed parameter $\theta\in[0,1]$, results in the quadrature rule
\begin{equation}
    \int \mat A_h \vec x_h(s)\,ds \approx \theta\Delta t \mat A_h\vec x_h^{n+1} + (1-\theta)\Delta t \mat A_h \vec x_h^n.
\end{equation}
We note that $\theta=0$ corresponds to the explicit scheme, $\theta=1$ to the implicit scheme, and $\theta=1/2$ to the mid-point scheme. 
    
\paragraph{Von Neumann stability analysis}
Energy methods for stability analysis can quickly become untractable, so in order to get easier bounds, we can perform this analysis after moving the equation to the frequency domain. This is the so-called \emph{von Neumann stability analysis}, which is a very powerful tool for analyzing the stability of finite difference schemes. The idea is to assume that the solution can be expressed as a Fourier series, and then analyze the growth of the Fourier modes over time.

For this analysis, we will use the Fourier transform, which is comfortable for this task as it yields simple eigenfunctions of the derivatives. 

\begin{definition}[Fourier transform]\label{def:fourier-transform}
    Let $v\in L^2(\Omega)$. Then, its Fourier transform $\mathcal{F}(v)  \coloneqq  \hat{v}$ is defined as
    \begin{equation}\label{eq:fourier-transform}
        \mathcal{F}(v)(\xi)  \coloneqq  \hat{v}(\xi) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty} v(x)e^{-i\xi x}dx,
    \end{equation}
    where $\xi$ is the frequency variable and $i=\sqrt{-1}$. This transform satisfies the Parseval identity:
    \begin{equation}\label{eq:parseval}
    \| v\|_{L^2} = \| \hat{v}\|_{L^2},
    \end{equation}
    and the inverse transform is defined as
    \begin{equation}\label{eq:inverse-fourier-transform}
        v(x) = \mathcal{F}^{-1}(\hat{v})(x) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty} \hat{v}(\xi)e^{i\xi x}d\xi.
    \end{equation}
    The most important identity we will use is the Fourier transform of the derivative, which is given by
    \begin{equation}\label{eq:fourier-transform-derivative}
        \mathcal{F}(v')(x) = i\xi \hat{v}(\xi).
    \end{equation}
    This definition is mostly relevant for differential operators over $\R$, so it will only give guidelines for actual problems.
\end{definition}
Let us consider a basis function $e^{i\xi x}$ for a given wave number $\xi$, and for some finite difference scheme, its grid function
\begin{equation}\label{eq:grid-function-fourier}
    W_j = e^{i\xi (jh)}.
\end{equation}
This function will be an eigenfunction of all translation-invariant finite difference operators. Set $x_j = jh$, then by~\ref{eq:grid-function-fourier} we get
\begin{equation*}
D^+ W(x_j) = \frac{1}{h}\left(e^{i\xi (j+1)h} - e^{i\xi j h}\right) = \frac{1}{h}\left(e^{i\xi h} - 1\right)e^{i\xi j h} = \frac{e^{i\xi h} - 1}{h}W_j.
\end{equation*}
This helps in decoupling the degrees of freedom, and by the Parseval identity, we can again try to get a bound of the form
\begin{equation*}
\|u^{n+1}\| = \|\hat{u}^{n+1}\|\leq |g(\xi)| \|\hat{u}^{n}\| = |g(\xi)| \|u^n\|,
\end{equation*}
where $g(\xi)$ is defined as an amplification factor. This scheme will be stable when $|g(\xi)|\leq 1$.

Let us now illustrate the application on the heat equation. Consider the boundary value problem
\begin{equation*}
    \begin{aligned}
        \dot{u}(x,t) - u_{xx}(x,t) &= 0, &&\quad (x,t) \tin \Omega \times (0,T), \\
        u(x,0)                  &= u^0(x), &&\quad x \tin  \Omega, \\
        u(x,t)                  &= f(x,t), &&\quad (x,t) \tin  \partial\Omega \times (0,T).
    \end{aligned}
\end{equation*}
Introducing a finite difference scheme, we get
\begin{equation}
    \frac{u^{n+1}_j - u^n_j}{\Delta t} + \frac{1}{h^2} (-u^n_{j+1} +2u^n_j - u^n_{j-1}) = 0.
\end{equation}
Here we set $u_j^n = e^{i\xi (jh)}$, and introduce the ansatz $u_j^{n+1} = g(\xi) u_j^n = g(\xi) e^{i\xi (jh)}$. Inserting this into the scheme we get
\begin{tightalign*}
    g(\xi) e^{i\xi j h} &= e^{i\xi (jh)} - \frac{\Delta t}{h^2}\left(-e^{i\xi (j-1) h} + 2e^{i\xi (jh)} - e^{i\xi (j+1) h}\right)\\
    &= \left(1 + \frac{\Delta t}{h^2}\left(e^{-i\xi h} - 2 + e^{i\xi h}\right)\right)e^{i\xi(jh)}.
\end{tightalign*}
Dividing by $e^{i\xi j h}$, we get
\begin{tightalign*}
    g(\xi) &= 1 + \frac{\Delta t}{h^2}\left(e^{-i\xi h} + e^{i\xi h}- 2 \right) \\
    &= 1 + \frac{\Delta t}{h^2}\left(2\cos(\xi h) - 2\right) \tag{$e^{-i\xi h} + e^{i\xi h} = 2\cos(\xi h)$} \\
    &= 1 - \frac{2\Delta t}{h^2}\left(1 - \cos(\xi h)\right).
\end{tightalign*}
We can now use the fact that $\cos(\xi h) \in (-1,1)$, which yield the following two bounds:
\begin{tightalign}
    g(\xi) &\leq 1 + \frac{2\Delta t}{h^2}(1-1) = 1 \\
    g(\xi) &\geq 1 + 2\frac{\Delta t}{h^2}(-1-1)  = 1 - \frac{4\Delta t}{h^2}.
\end{tightalign}
We note that the first inequality yields $g(\xi)\leq 1$, and from the second inequality we seek $-1\leq g(\xi)$. Bounding by below, we get
\begin{tightalign}
    1-4\frac{\Delta t}{h^2} \geq -1 &\iff 2\geq 4\frac{\Delta t}{h^2} \notag\\
    &\iff \Delta t \leq \frac{h^2}{2},
\end{tightalign}
which yields an upper bound for the time step $\Delta t$ to ensure stability of the finite difference scheme. Inequalities of this kind are studied more thoroughly in~\cite{LeVeque2007}.

\paragraph{Time stability}
We now look for stability estimates regarding also time. We first study a forward scheme 
\begin{equation}
    \begin{aligned}
        \displaystyle\frac{u^{n+1}-u^n}{\Delta t} &= cu^n\\
        u^0&= u_{h,0},
    \end{aligned}
\end{equation}
with $c\in \R$. We note that a result such as $\|u^{n+1}\|\leq c\|u^n\|$ for $c\leq 1$ results in $\|u_h\|_{\infty, h} \leq \|u^0\|_{\infty, h}$. This is indeed a stability result, so it will be common tu study equations in the form $u^{n+1} = g(u^n)$. Given that our theory works for linear operators, we will start by studying a simple first-order equation:
\begin{equation}
    \begin{aligned}
        u'+cu &= f\\
        u(0) &= u_0.
    \end{aligned}
\end{equation}
Now we look at the stability of forward and backward Euler and the $\theta$-method. 
\begin{enumerate}
    \item \emph{Explicit}: we have $u^{n+1}-u^n + c\Delta t u^n = \Delta t f^n$. Stability is a property of the differential operator, and thus we analyze it with $f\equiv 0$. We note that
    \begin{equation*}
        u^{n+1} = (1-c\Delta t)u^n \implies |u^{n+1}|\leq |1-c\Delta t||u^n|,
    \end{equation*}
    and thus this scheme is stable if $|1-c\Delta t|< 1$. We note two cases:
    \begin{itemize}
        \item If $c > 0$, then we have $|1-c\Delta t|<1$ if $\Delta t < \frac{2}{c}$. This means that this method is \emph{conditionally stable}. 
        \item If $c< 0$, then $|1-c\Delta t|\geq 1$, which means that the scheme is \emph{unconditionally unstable}.
    \end{itemize} 
    \item \emph{Implicit}: we have $u^{n+1}-u^n + c\Delta tu^{n+1} = \Delta t f^n$. This yields
    \begin{equation*}
        (1+c\Delta t) u^{n+1} = u^n \implies |u^{n+1}| \leq \left|\frac{1}{1+c\Delta t}\right| |u^{n}|.
    \end{equation*}
    \begin{itemize}
        \item If $c > 0$,  we have $1+c\Delta t >1$, which means that the method is \emph{unconditionally stable}. 
        \item If $c < 0$, then $1+c\Delta t <1$, so the scheme is \emph{unconditionally unstable}.
    \end{itemize}     
    It is important to note that if $c<0$, then the solution is just $u(x) = Ce^x$, so a condition such as $|u^{n+1}|<|u^n|$ does not make sense. Thankfully, our analysis reveals that, and we now just assume $c>0$. 
    \item \emph{$\theta$-method}: we have
    \begin{equation*}
        u^{n+1}-u^n + \theta c\Delta t u^{n+1} + (1-\theta)c\Delta t u^n = 0.
    \end{equation*}
    Rearranging this we get
    \begin{equation*}
        (1+\theta c\Delta t)u^{n+1} = (1- (1-\theta)c\Delta t) u^n \implies |u^{n+1}| \leq \left|\frac{1-(1-\theta)c\Delta t}{1+\theta c\Delta t}\right| |u^n|.
    \end{equation*}
    We now recognize two cases. 
    \begin{itemize}
        \item If $\Delta t$ is big and such that $1-(1-\theta)c\Delta t < 0$, we have
        \begin{tightalign*}
            -\frac{1-(1-\theta)c\Delta t}{1+\theta c\Delta t} \leq 1 &\iff -1+(1-\theta)c\Delta t \leq 1+\theta c\Delta t\\
            &\iff (1-2\theta) c\Delta t \leq 2\\
            &\iff \left(\frac{1}{2}-\theta\right)c\Delta t\leq 1.
        \end{tightalign*}
        So, if $\theta\geq 1/2$, the method is unconditionally stable, and if $\theta <1/2$, the method is \emph{conditionally stable}. 
        \item If $\Delta t$ is small, such that $1-(1-\theta)c\Delta t < 1 + \theta c \Delta t$, then we get $-c\Delta t < 1$, which is \emph{unconditionally stable}.
    \end{itemize}
\end{enumerate}